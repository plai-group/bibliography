@misc{wood-constrained-generative-modeling-2025,
	abstract = {In this paper we describe a novel framework for diffusion-based generative modeling on constrained spaces. In particular, we introduce manual bridges, a framework that expands the kinds of constraints that can be practically used to form so-called diffusion bridges. We develop a mechanism for combining multiple such constraints so that the resulting multiply-constrained model remains a manual bridge that respects all constraints. We also develop a mechanism for training a diffusion model that respects such multiple constraints while also adapting it to match a data distribution. We develop and extend theory demonstrating the mathematical validity of our mechanisms. Additionally, we demonstrate our mechanism in constrained generative modeling tasks, highlighting a particular high-value application in modeling trajectory initializations for path planning and control in autonomous vehicles.},
	author = {Saeid Naderiparizi, Xiaoxuan Liang, Berend Zwartsenberg, Frank Wood},
	title = {Constrained Generative Modeling with Manually Bridged Diffusion Models},
	year = {2025}
}

@misc{wood-controlling-the-2025,
	abstract = {Simulating realistic driving behavior is crucial for developing and testing autonomous systems in complex traffic environments. Equally important is the ability to control the behavior of simulated agents to tailor scenarios to specific research needs and safety considerations. This paper extends the general-purpose multi-agent driving behavior model ITRA (Scibior et al., 2021), by introducing a method called Control-ITRA to influence agent behavior through waypoint assignment and target speed modulation. By conditioning agents on these two aspects, we provide a mechanism for them to adhere to specific trajectories and indirectly adjust their aggressiveness. We compare different approaches for integrating these conditions during training and demonstrate that our method can generate controllable, infraction-free trajectories while preserving realism in both seen and unseen locations.},
	author = {Vasileios Lioutas, Adam Scibior, Matthew Niedoba, Berend Zwartsenberg, Frank Wood},
	title = {Control-ITRA: Controlling the Behavior of a Driving Model},
	year = {2025}
}

@misc{wood-method-and-system-2025,
	abstract = {Methods, systems, and techniques for generating one or more conditionally dependent data entries using a probabilistic generative model, and for training that model. The probabilistic generative model is trained using a plurality of data generation tasks respectively corresponding to a plurality of vectors each having a sequence of differently indexed data entries that are conditionally dependent on each other. Each of the data generation tasks involves generating at least one latent data entry selected from the sequence of data entries in response to being provided at least one index for each of the at least one latent data entry. Training can be performed by minimizing an expected value of a denoising loss over all training stages.},
	author = {William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Dietrich Weilbach, Frank Wood},
	title = {Method and system for generating one or more conditionally dependent data entries},
	year = {2025}
}

@misc{wood--2025,
	abstract = {Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.},
	author = {Yingchen He, Christian D Weilbach, Martyna E Wojciechowska, Yuxuan Zhang, Frank Wood},
	title = {PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI},
	year = {2025}
}

@misc{wood-rolling-ahead-diffusion-2025,
	abstract = {Realistic driving simulation requires that NPCs not only mimic natural driving behaviors but also react to the behavior of other simulated agents. Recent developments in diffusion-based scenario generation focus on creating diverse and realistic traffic scenarios by jointly modelling the motion of all the agents in the scene. However, these traffic scenarios do not react when the motion of agents deviates from their modelled trajectories. For example, the ego-agent can be controlled by a stand along motion planner. To produce reactive scenarios with joint scenario models, the model must regenerate the scenario at each timestep based on new observations in a Model Predictive Control (MPC) fashion. Although reactive, this method is time-consuming, as one complete possible future for all NPCs is generated per simulation step. Alternatively, one can utilize an autoregressive model (AR) to predict only the immediate next-step future for all NPCs. Although faster, this method lacks the capability for advanced planning. We present a rolling diffusion based traffic scene generation model which mixes the benefits of both methods by predicting the next step future and simultaneously predicting partially noised further future steps at the same time. We show that such model is efficient compared to diffusion model based AR, achieving a beneficial compromise between reactivity and computational efficiency.},
	author = {Yunpeng Liu, Matthew Niedoba, William Harvey, Adam Scibior, Berend Zwartsenberg, Frank Wood},
	title = {Rolling ahead diffusion for traffic scene simulation},
	year = {2025}
}

@misc{macke-inference-2024,
	abstract = {Amortized Bayesian inference trains neural networks to solve stochastic inference problems using model simulations, thereby making it possible to rapidly perform Bayesian inference for any newly observed data. However, current simulation-based amortized inference methods are simulation-hungry and inflexible: They require the specification of a fixed parametric prior, simulator, and inference tasks ahead of time. Here, we present a new amortized inference method -- the Simformer -- which overcomes these limitations. By training a probabilistic diffusion model with transformer architectures, the Simformer outperforms current state-of-the-art amortized inference approaches on benchmark tasks and is substantially more flexible: It can be applied to models with function-valued parameters, it can handle inference scenarios with missing or unstructured data, and it can sample arbitrary conditionals of the joint distribution of parameters and data, including both posterior and likelihood. We showcase the performance and flexibility of the Simformer on simulators from ecology, epidemiology, and neuroscience, and demonstrate that it opens up new possibilities and application domains for amortized Bayesian inference on simulation-based models.},
	author = {Manuel Gloeckler, Michael Deistler, Christian Weilbach, Frank Wood, Jakob H Macke},
	title = {All-in-one simulation-based inference},
	year = {2024}
}

@misc{pleiss-layerwise-proximal-2024,
	abstract = {In online continual learning, a neural network incrementally learns from a non-i.i.d. data stream. Nearly all online continual learning methods employ experience replay to simultaneously prevent catastrophic forgetting and underfitting on past data. Our work demonstrates a limitation of this approach: neural networks trained with experience replay tend to have unstable optimization trajectories, impeding their overall accuracy. Surprisingly, these instabilities persist even when the replay buffer stores all previous training examples, suggesting that this issue is orthogonal to catastrophic forgetting. We minimize these instabilities through a simple modification of the optimization geometry. Our solution, Layerwise Proximal Replay (LPR), balances learning from new and replay data while only allowing for gradual changes in the hidden activation of past data. We demonstrate that LPR consistently improves replay-based online continual learning methods across multiple problem settings, regardless of the amount of available replay memory.},
	author = {Jason Yoo, Yunpeng Liu, Frank Wood, Geoff Pleiss},
	title = {Layerwise proximal replay: A proximal point method for online continual learning},
	year = {2024}
}

@misc{wood-lifelong-learning-of-2024,
	abstract = {This work demonstrates that training autoregressive video diffusion models from a single, continuous video stream is not only possible but remarkably can also be competitive with standard offline training approaches given the same number of gradient steps. Our demonstration further reveals that this main result can be achieved using experience replay that only retains a subset of the preceding video stream. We also contribute three new single video generative modeling datasets suitable for evaluating lifelong video model learning: Lifelong Bouncing Balls, Lifelong 3D Maze, and Lifelong PLAICraft. Each dataset contains over a million consecutive frames from a synthetic environment of increasing complexity.},
	author = {Jason Yoo, Yingchen He, Saeid Naderiparizi, Dylan Green, Gido M van de Ven, Geoff Pleiss, Frank Wood},
	title = {Lifelong Learning of Video Diffusion Models From a Single Video Stream},
	year = {2024}
}

@misc{wood-nearest-neighbour-score-2024,
	abstract = {Score function estimation is the cornerstone of both training and sampling from diffusion generative models. Despite this fact, the most commonly used estimators are either biased neural network approximations or high variance Monte Carlo estimators based on the conditional score. We introduce a novel nearest neighbour score function estimator which utilizes multiple samples from the training set to dramatically decrease estimator variance. We leverage our low variance estimator in two compelling applications. Training consistency models with our estimator, we report a significant increase in both convergence speed and sample quality. In diffusion models, we show that our estimator can replace a learned network for probability-flow ODE integration, opening promising new avenues of future research.},
	author = {Matthew Niedoba, Dylan Green, Saeid Naderiparizi, Vasileios Lioutas, Jonathan Wilder Lavington, Xiaoxuan Liang, Yunpeng Liu, Ke Zhang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood},
	title = {Nearest neighbour score estimators for diffusion generative models},
	year = {2024}
}

@misc{wood-on-neural-process-2024,
	abstract = {We highlight an apparent deficiency of amortized posterior predictive inference models, particularly Neural Processes, with respect to diversity and realism of generated samples in the low data regime. While a recent trend in the literature emphasizes achieving high-quality samples with large conditioning datasets, we shift our focus to scenarios where the amount of conditioning data is small. We investigate the effect of architecture choices on neural processes, and investigate how these choices impact the model’s ability to represent uncertainty. Furthermore, we investigate how uncertainty is represented in such models. We demonstrate that max pooling in neural processes results in posterior samples that are better calibrated in the sense of being both sharper and more diverse, highlighting this via a difficult image completion task.},
	author = {Saeid Naderiparizi, Kenny Chiu, Benjamin Bloem-Reddy, Frank Wood},
	title = {On Neural Process Pooling Operator Choice and Posterior Calibration.},
	year = {2024}
}

@misc{fortuin-on-the-challenges-2024,
	abstract = {The field of deep generative modeling has grown rapidly in the last few years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models exhibit several fundamental shortcomings that hinder their widespread adoption across domains. In this work, our objective is to identify these issues and highlight key unresolved challenges in modern generative AI paradigms that should be addressed to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with insights for exploring fruitful research directions, thus fostering the development of more robust and accessible generative AI solutions.},
	author = {Laura Manduchi, Clara Meister, Kushagra Pandey, Robert Bamler, Ryan Cotterell, Sina Däubener, Sophie Fellenz, Asja Fischer, Thomas Gärtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric Nalisnick, Björn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt, Vincent Fortuin},
	title = {On the challenges and opportunities in generative ai},
	year = {2024}
}

@misc{wood-online-continual-learning-2024,
	abstract = {Diffusion models have shown exceptional capabilities in generating realistic videos. Yet, their training has been predominantly confined to offline environments where models can repeatedly train on iid data to convergence. This work explores the feasibility of training diffusion models from a semantically continuous video stream, where correlated video frames sequentially arrive one at a time. To investigate this, we introduce two novel continual video generative modeling benchmarks, Lifelong Bouncing Balls and Windows 95 Maze Screensaver, each containing over a million video frames generated from navigating stationary environments. Surprisingly, our experiments show that diffusion models can be effectively trained online using experience replay, achieving performance comparable to models trained with iid samples given the same number of gradient steps.},
	author = {Jason Yoo, Dylan Green, Geoff Pleiss, Frank Wood},
	title = {Online Continual Learning of Video Diffusion Models From a Single Video Stream},
	year = {2024}
}

@misc{wood-prospective-learning-2024,
	abstract = {Inter-neuron communication delays are ubiquitous in physically realized neural networks such as biological neural circuits and neuromorphic hardware. These delays have significant and often disruptive consequences on network dynamics during training and inference. It is therefore essential that communication delays be accounted for, both in computational models of biological neural networks and in large-scale neuromorphic systems. Nonetheless, communication delays have yet to be comprehensively addressed in either domain. In this paper, we first show that delays prevent state-of-the-art continuous-time neural networks called Latent Equilibrium (LE) networks from learning even simple tasks despite significant overparameterization. We then propose to compensate for communication delays by predicting future signals based on currently available ones. This conceptually straightforward approach, which we call prospective messaging (PM), uses only neuron-local information, and is flexible in terms of memory and computation requirements. We demonstrate that incorporating PM into delayed LE networks prevents reaction lags, and facilitates successful learning on Fourier synthesis and autoregressive video prediction tasks.},
	author = {Ryan Fayyazi, Christian Weilbach, Frank Wood},
	title = {Prospective Messaging: Learning in Networks with Communication Delays},
	year = {2024}
}

@misc{wood-semantically-consistent-video-2024,
	abstract = {Current state-of-the-art methods for video inpainting typically rely on optical flow or attention-based approaches to inpaint masked regions by propagating visual information across frames. While such approaches have led to significant progress on standard benchmarks, they struggle with tasks that require the synthesis of novel content that is not present in other frames. In this paper, we reframe video inpainting as a conditional generative modeling problem and present a framework for solving such problems with conditional video diffusion models. We introduce inpainting-specific sampling schemes which capture crucial long-range dependencies in the context, and devise a novel method for conditioning on the known pixels in incomplete frames. We highlight the advantages of using a generative approach for this task, showing that our method is capable of generating diverse, high-quality inpaintings and synthesizing new content that is spatially, temporally, and semantically consistent with the provided context.},
	author = {Dylan Green, William Harvey, Saeid Naderiparizi, Matthew Niedoba, Yunpeng Liu, Xiaoxuan Liang, Jonathan Lavington, Ke Zhang, Vasileios Lioutas, Setareh Dabiri, Adam Scibior, Berend Zwartsenberg, Frank Wood},
	title = {Semantically consistent video inpainting with conditional diffusion models},
	year = {2024}
}

@misc{wood-a-reinforcement-2024,
	abstract = {The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators. Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify. To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv. TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns. Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API. This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose …},
	author = {Jonathan Wilder Lavington, Ke Zhang, Vasileios Lioutas, Matthew Niedoba, Yunpeng Liu, Dylan Green, Saeid Naderiparizi, Xiaoxuan Liang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood},
	title = {TorchDriveEnv: A Reinforcement Learning Benchmark for Autonomous Driving with Reactive, Realistic, and Diverse Non-Playable Characters},
	year = {2024}
}

@misc{wood-a-reinforcement-2024,
	abstract = {The training, testing, and deployment, of autonomous vehicles requires realistic and efficient simulators. Moreover, because of the high variability between different problems presented in different autonomous systems, these simulators need to be easy to use, and easy to modify. To address these problems we introduce TorchDriveSim and its benchmark extension TorchDriveEnv. TorchDriveEnv is a lightweight reinforcement learning benchmark programmed entirely in Python, which can be modified to test a number of different factors in learned vehicle behavior, including the effect of varying kinematic models, agent types, and traffic control patterns. Most importantly unlike many replay based simulation approaches, TorchDriveEnv is fully integrated with a state of the art behavioral simulation API. This allows users to train and evaluate driving models alongside data driven Non-Playable Characters (NPC) whose initializations and driving behavior are reactive, realistic, and diverse. We illustrate the efficiency and simplicity of TorchDriveEnv by evaluating common reinforcement learning baselines in both training and validation environments. Our experiments show that TorchDriveEnv is easy to use, but difficult to solve.},
	author = {Jonathan Wilder Lavington, Ke Zhang, Vasileios Lioutas, Matthew Niedoba, Yunpeng Liu, Dylan Green, Saeid Naderiparizi, Xiaoxuan Liang, Setareh Dabiri, Adam Ścibior, Berend Zwartsenberg, Frank Wood},
	title = {Torchdriveenv: A reinforcement learning benchmark for autonomous driving with reactive, realistic, and diverse non-playable characters},
	year = {2024}
}

@misc{wood-towards-a-mechanistic-2024,
	abstract = {We propose a simple, training-free mechanism which explains the generalization behaviour of diffusion models. By comparing pre-trained diffusion models to their theoretically optimal empirical counterparts, we identify a shared local inductive bias across a variety of network architectures. From this observation, we hypothesize that network denoisers generalize through localized denoising operations, as these operations approximate the training objective well over much of the training distribution. To validate our hypothesis, we introduce novel denoising algorithms which aggregate local empirical denoisers to replicate network behaviour. Comparing these algorithms to network denoisers across forward and reverse diffusion processes, our approach exhibits consistent visual similarity to neural network outputs, with lower mean squared error than previously proposed methods.},
	author = {Matthew Niedoba, Berend Zwartsenberg, Kevin Murphy, Frank Wood},
	title = {Towards a mechanistic explanation of diffusion model generalization},
	year = {2024}
}

@misc{wood-a-of-2023,
	abstract = {Simulation of autonomous vehicle systems requires that simulated traffic participants exhibit diverse and realistic behaviors. The use of prerecorded real-world traffic scenarios in simulation ensures realism but the rarity of safety critical events makes large scale collection of driving scenarios expensive. In this paper, we present DJINN--a diffusion based method of generating traffic scenarios. Our approach jointly diffuses the trajectories of all agents, conditioned on a flexible set of state observations from the past, present, or future. On popular trajectory forecasting datasets, we report state of the art performance on joint trajectory metrics. In addition, we demonstrate how DJINN flexibly enables direct test-time sampling from a variety of valuable conditional distributions including goal-based sampling, behavior-class sampling, and scenario editing.},
	author = {Matthew Niedoba, Jonathan Lavington, Yunpeng Liu, Vasileios Lioutas, Justice Sefas, Xiaoxuan Liang, Dylan Green, Setareh Dabiri, Berend Zwartsenberg, Adam Scibior, Frank Wood},
	title = {A diffusion-model of joint interactive navigation},
	year = {2023}
}

@misc{wood-be-so-2023,
	abstract = {The maximum likelihood principle advocates parameter estimation via optimization of the data likelihood function. Models estimated in this way can exhibit a variety of generalization characteristics dictated by, e.g. architecture, parameterization, and optimization bias. This work addresses model learning in a setting where there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling (DDPM) methodology, Gen-neG, that leverages this additional side-information. Our approach builds on generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.},
	author = {Saeid Naderiparizi, Xiaoxuan Liang, Berend Zwartsenberg, Frank Wood},
	title = {Don't be so negative! Score-based Generative Modeling with Oracle-assisted Guidance},
	year = {2023}
}

@misc{wood-graphically-structured-diffusion-2023,
	abstract = {We introduce a framework for automatically defining and learning deep generative models with problem-specific structure. We tackle problem domains that are more traditionally solved by algorithms such as sorting, constraint satisfaction for Sudoku, and matrix factorization. Concretely, we train diffusion models with an architecture tailored to the problem specification. This problem specification should contain a graphical model describing relationships between variables, and often benefits from explicit representation of subcomputations. Permutation invariances can also be exploited. Across a diverse set of experiments we improve the scaling relationship between problem dimension and our model’s performance, in terms of both training time and final accuracy. Our code can be found at https://github. com/plai-group/gsdm.},
	author = {Christian Dietrich Weilbach, William Harvey, Frank Wood},
	title = {Graphically structured diffusion models},
	year = {2023}
}

@misc{scibior-realistically-distributing-object-2023,
	abstract = {When training object detection models on synthetic data, it is important to make the distribution of synthetic data as close as possible to the distribution of real data. We investigate specifically the impact of object placement distribution, keeping all other aspects of synthetic data fixed. Our experiment, training a 3D vehicle detection model in CARLA and testing on KITTI, demonstrates a substantial improvement resulting from improving the object placement distribution.},
	author = {Setareh Dabiri, Vasileios Lioutas, Berend Zwartsenberg, Yunpeng Liu, Matthew Niedoba, Xiaoxuan Liang, Dylan Green, Justice Sefas, Jonathan Wilder Lavington, Frank Wood, Adam Scibior},
	title = {Realistically distributing object placements in synthetic training data improves the performance of vision-based object detection models},
	year = {2023}
}

@misc{wood-scaling-graphically-structured-2023,
	abstract = {Applications of the recently introduced graphically structured diffusion model (GSDM) family show that sparsifying the transformer attention mechanism within a diffusion model and meta-training on a variety of conditioning tasks can yield an efficiently learnable diffusion model artifact that is capable of flexible, in the sense of observing different subsets of variables at test-time, amortized conditioning in probabilistic graphical models.  While extremely promising in terms of applicability and utility, implementations of GSDMs prior to this work were not scalable beyond toy graphical model sizes. We overcome this limitation by describing and and solving two scaling issues related to GSDMs; one engineering and one methodological. We additionally propose a new benchmark problem of weight inference for a convolutional neural network applied to  MNIST.},
	author = {Christian Dietrich Weilbach, William Harvey, Hamed Shirzad, Frank Wood},
	title = {Scaling Graphically Structured Diffusion Models},
	year = {2023}
}

@misc{wood-uncertain-evidence-in-2023,
	abstract = {We consider the problem of performing Bayesian inference in probabilistic models where observations are accompanied by uncertainty, referred to as" uncertain evidence.” We explore how to interpret uncertain evidence, and by extension the importance of proper interpretation as it pertains to inference about latent variables. We consider a recently-proposed method" distributional evidence” as well as revisit two older methods: Jeffrey’s rule and virtual evidence. We devise guidelines on how to account for uncertain evidence and we provide new insights, particularly regarding consistency. To showcase the impact of different interpretations of the same uncertain evidence, we carry out experiments in which one interpretation is defined as" correct.” We then compare inference results from each different interpretation illustrating the importance of careful consideration of uncertain evidence.},
	author = {Andreas Munk, Alexander Mead, Frank Wood},
	title = {Uncertain evidence in probabilistic models and stochastic simulators},
	year = {2023}
}

@misc{wood-video-killed-the-2023,
	abstract = {The development of algorithms that learn multi-agent behavioral models using human demonstrations has led to increasingly realistic simulations in the field of autonomous driving. In general, such models learn to jointly predict trajectories for all controlled agents by exploiting road context information such as drivable lanes obtained from manually annotated high-definition (HD) maps. Recent studies show that these models can greatly benefit from increasing the amount of human data available for training. However, the manual annotation of HD maps which is necessary for every new location puts a bottleneck on efficiently scaling up human traffic datasets. We propose an aerial image-based map (AIM) representation that requires minimal annotation and provides rich road context information for traffic agents like pedestrians and vehicles. We evaluate multi-agent trajectory prediction using the AIM by …},
	author = {Yunpeng Liu, Vasileios Lioutas, Jonathan Wilder Lavington, Matthew Niedoba, Justice Sefas, Setareh Dabiri, Dylan Green, Xiaoxuan Liang, Berend Zwartsenberg, Adam Ścibior, Frank Wood},
	title = {Video Killed the HD-Map: Predicting Multi-Agent Behavior Directly From Aerial Images},
	year = {2023}
}

@misc{wood-visual-diffusion-2023,
	abstract = {Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We propose to close the gap between conditional and unconditional models using a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we sample the image conditioned on this embedding and then discard the embedding. Doing so lets us leverage the power of conditional diffusion models on the unconditional generation task, which we show improves FID by 25-50% compared to standard unconditional generation.},
	author = {William Harvey, Frank Wood},
	title = {Visual chain-of-thought diffusion models},
	year = {2023}
}

@misc{wood-amortized-rejection-sampling-2022,
	abstract = {Naive approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. This is particularly true of importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method’s correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.},
	author = {Saeid Naderiparizi, Adam Scibior, Andreas Munk, Mehrdad Ghadiri, Atilim Gunes Baydin, Bradley J Gram-Hansen, Christian A Schroeder De Witt, Robert Zinkov, Philip Torr, Tom Rainforth, Yee Whye Teh, Frank Wood},
	title = {Amortized rejection sampling in universal probabilistic programming},
	year = {2022}
}

@misc{wood-a-continually-2022,
	abstract = {Associative memory plays an important role in human intelligence and its mechanisms have been linked to attention in machine learning. While the machine learning community's interest in associative memories has recently been rekindled, most work has focused on memory recall ($ read $) over memory learning ($ write $). In this paper, we present BayesPCN, a hierarchical associative memory capable of performing continual one-shot memory writes without meta-learning. Moreover, BayesPCN is able to gradually forget past observations ($ forget $) to free its memory. Experiments show that BayesPCN can recall corrupted iid high-dimensional data observed hundreds to a thousand``timesteps''ago without a large drop in recall ability compared to the state-of-the-art offline-learned parametric memory models.},
	author = {Jinsoo Yoo, Frank Wood},
	title = {Bayespcn: A continually learnable predictive coding associative memory},
	year = {2022}
}

@misc{wood-beyond-simple-2022,
	abstract = {Modern deep learning requires large-scale extensively labelled datasets for training. Few-shot learning aims to alleviate this issue by learning effectively from few labelled examples. In previously proposed few-shot visual classifiers, it is assumed that the feature manifold, where classifier decisions are made, has uncorrelated feature dimensions and uniform feature variance. In this work, we focus on addressing the limitations arising from this assumption by proposing a variance-sensitive class of models that operates in a low-label regime. The first method, Simple CNAPS, employs a hierarchically regularized Mahalanobis-distance based classifier combined with a state of the art neural adaptive feature extractor to achieve strong performance on Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. We further extend this approach to a transductive learning setting, proposing Transductive CNAPS. This transductive method combines a soft k-means parameter refinement procedure with a two-step task encoder to achieve improved test-time classification accuracy using unlabelled data. Transductive CNAPS achieves state of the art performance on Meta-Dataset. Finally, we explore the use of our methods (Simple and Transductive) for "out of the box" continual and active learning. Extensive experiments on large scale benchmarks illustrate robustness and versatility of this, relatively speaking, simple class of models. All trained model checkpoints and corresponding source codes have been made publicly available.},
	author = {Peyman Bateni, Jarred Barber, Raghav Goyal, Vaden Masrani, Jan-Willem van de Meent, Leonid Sigal, Frank Wood},
	title = {Beyond simple meta-learning: Multi-purpose models for multi-domain, active and continual few-shot learning},
	year = {2022}
}

@misc{wood-conditional-permutation-invariant-2022,
	abstract = {We present a novel, conditional generative probabilistic model of set-valued data with a tractable log density. This model is a continuous normalizing flow governed by permutation equivariant dynamics. These dynamics are driven by a learnable per-set-element term and pairwise interactions, both parametrized by deep neural networks. We illustrate the utility of this model via applications including (1) complex traffic scene generation conditioned on visually specified map information, and (2) object bounding box generation conditioned directly on images. We train our model by maximizing the expected likelihood of labeled conditional data under our flow, with the aid of a penalty that ensures the dynamics are smooth and hence efficiently solvable. Our method significantly outperforms non-permutation invariant baselines in terms of log likelihood and domain-specific metrics (offroad, collision, and combined infractions), yielding realistic samples that are difficult to distinguish from real data.},
	author = {Berend Zwartsenberg, Adam Ścibior, Matthew Niedoba, Vasileios Lioutas, Yunpeng Liu, Justice Sefas, Setareh Dabiri, Jonathan Wilder Lavington, Trevor Campbell, Frank Wood},
	title = {Conditional permutation invariant flows},
	year = {2022}
}

@misc{scibior-critic-sequential-monte-2022,
	abstract = {We introduce CriticSMC, a new algorithm for planning as inference built from a composition of sequential Monte Carlo with learned Soft-Q function heuristic factors. These heuristic factors, obtained from parametric approximations of the marginal likelihood ahead, more effectively guide SMC towards the desired target distribution, which is particularly helpful for planning in environments with hard constraints placed sparsely in time. Compared with previous work, we modify the placement of such heuristic factors, which allows us to cheaply propose and evaluate large numbers of putative action particles, greatly increasing inference and planning efficiency. CriticSMC is compatible with informative priors, whose density function need not be known, and can be used as a model-free control algorithm. Our experiments on collision avoidance in a high-dimensional simulated driving task show that CriticSMC significantly reduces collision rates at a low computational cost while maintaining realism and diversity of driving behaviors across vehicles and environment scenarios.},
	author = {Vasileios Lioutas, Jonathan Wilder Lavington, Justice Sefas, Matthew Niedoba, Yunpeng Liu, Berend Zwartsenberg, Setareh Dabiri, Frank Wood, Adam Scibior},
	title = {Critic sequential monte carlo},
	year = {2022}
}

@misc{wood-enhancing-image-2022,
	abstract = {We develop a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve state of the art performance on the Meta-Dataset, mini-ImageNet and tiered-ImageNet benchmarks. All trained models and code have been made publicly available at github. com/plai-group/simple-cnaps.},
	author = {Peyman Bateni, Jarred Barber, Jan-Willem Van de Meent, Frank Wood},
	title = {Enhancing few-shot image classification with unlabelled examples},
	year = {2022}
}

@misc{wood-exploration-with-2022,
	abstract = {Distributional reinforcement learning (RL) aims to learn a value-network that predicts the full distribution of the returns for a given state, often modeled via a quantile-based critic. This approach has been successfully integrated into common RL methods for continuous control, giving rise to algorithms such as Distributional Soft Actor-Critic (DSAC). In this paper, we introduce multi-sample target values (MTV) for distributional RL, as a principled replacement for single-sample target value estimation, as commonly employed in current practice. The improved distributional estimates further lend themselves to UCB-based exploration. These two ideas are combined to yield our distributional RL algorithm, E2DC (Extra Exploration with Distributional Critics). We evaluate our approach on a range of continuous control tasks and demonstrate state-of-the-art model-free performance on difficult tasks such as Humanoid control. We provide further insight into the method via visualization and analysis of the learned distributions and their evolution during training.},
	author = {Michael Teng, Michiel van de Panne, Frank Wood},
	title = {Exploration with multi-sample target values for distributional reinforcement learning},
	year = {2022}
}

@misc{wood-flexible-diffusion-modeling-2022,
	abstract = {We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames. We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length. We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.},
	author = {William Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, Frank Wood},
	title = {Flexible diffusion modeling of long videos},
	year = {2022}
}

@misc{torr-gradients-without-backpropagation-2022,
	abstract = {Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differentiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one can compute exactly and efficiently via the forward mode. We call this formulation the forward gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for backpropagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training up to twice as fast in some cases.},
	author = {Atılım Güneş Baydin, Barak A Pearlmutter, Don Syme, Frank Wood, Philip Torr},
	title = {Gradients without Backpropagation},
	year = {2022}
}

@misc{torr-gradients-without-backpropagation-2022,
	abstract = {Using backpropagation to compute gradients of objective functions for optimization has remained a mainstay of machine learning. Backpropagation, or reverse-mode differentiation, is a special case within the general family of automatic differentiation algorithms that also includes the forward mode. We present a method to compute gradients based solely on the directional derivative that one can compute exactly and efficiently via the forward mode. We call this formulation the forward gradient, an unbiased estimate of the gradient that can be evaluated in a single forward run of the function, entirely eliminating the need for backpropagation in gradient descent. We demonstrate forward gradient descent in a range of problems, showing substantial savings in computation and enabling training up to twice as fast in some cases.},
	author = {Atılım Güneş Baydin, Barak A Pearlmutter, Don Syme, Frank Wood, Philip Torr},
	title = {Gradients without backpropagation},
	year = {2022}
}

@misc{wood-glimpse-sequences-2022,
	abstract = {Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies. However, hard attention mechanisms can be difficult and slow to train, which is especially costly for applications like neural architecture search where multiple networks must be trained. We introduce a method to amortise the cost of training by generating an extra supervision signal for a subset of the training data. This supervision is in the form of sequences of ‘good’ locations to attend to for each image. We find that the best method to generate supervision sequences comes from framing hard attention for image classification as a Bayesian optimal experimental design (BOED) problem. From this perspective, the optimal locations to attend to are those which provide the greatest expected reduction in the entropy of the classification distribution. We introduce methodology from the BOED …},
	author = {William Harvey, Michael Teng, Frank Wood},
	title = {Near-optimal glimpse sequences for improved hard attention neural network training},
	year = {2022}
}

@misc{duc-physics-aware-inference-2022,
	abstract = {We propose a parametric forward model for single particle cryo-electron microscopy (cryo-EM), and employ stochastic variational inference to infer posterior distributions of the physically interpretable latent variables. Our cryo-EM forward model accounts for the biomolecular configuration (via spatial coordinates of pseudo-atoms, in contrast with traditional voxelized representations) the global pose, the effect of the microscope (contrast transfer function’s defocus parameter). To account for conformational heterogeneity, we use the anisotropic network model (ANM). We perform experiments on synthetic data and show that the posterior of the scalar component along the lowest ANM mode and the angle of 2D in-plane pose can be jointly inferred with deep neural networks. We also perform Fourier frequency marching in the simulation and likelihood during training of the neural networks, as an annealing step.},
	author = {Geoffrey Woollard, Shayan Shekarforoush, Frank Wood, Marcus A Brubaker, Khanh Dao Duc},
	title = {Physics aware inference for the cryo-EM inverse problem: anisotropic network model heterogeneity, global pose and microscope defocus},
	year = {2022}
}

@misc{nasseri-planning-as-inference-2022,
	abstract = {In this work we demonstrate how to automate parts of the infectious disease-control policy-making process via performing inference in existing epidemiological models. The kind of inference tasks undertaken include computing the posterior distribution over controllable, via direct policy-making choices, simulation model parameters that give rise to acceptable disease progression outcomes. Among other things, we illustrate the use of a probabilistic programming language that automates inference in existing simulators. Neither the full capabilities of this tool for automating inference nor its utility for planning is widely disseminated at the current time. Timely gains in understanding about how such simulation-based models and inference automation tools applied in support of policy-making could lead to less economically damaging policy prescriptions, particularly during the current COVID-19 pandemic.},
	author = {Frank Wood, Andrew Warrington, Saeid Naderiparizi, Christian Weilbach, Vaden Masrani, William Harvey, Adam Ścibior, Boyan Beronov, John Grefenstette, Duncan Campbell, S Ali Nasseri},
	title = {Planning as inference in epidemiological dynamics models},
	year = {2022}
}

@misc{wood-probabilistic-programming-2022,
	abstract = {Pearl [2000, p. 26] attributes to Laplace [1814] the idea of a probabilistic model as a deterministic system with stochastic inputs. Pearl defines causal models in terms of deterministic systems with stochastic inputs. In this chapter, we show how deterministic systems with (independent) probabilistic inputs are also the basis of modern probabilistic programming languages [van de Meent et al. 2018]. Proba bilistic programs can be seen as consisting of independent choices (over which there are probability distributions) and deterministic programs that give the con sequences of these choices. The work on developing such languages has gone in parallel with the development of causal models, and many of the foundations are remarkably similar. Most of the work in probabilistic programming languages has been in the context of specific languages. This chapter abstracts the work on proba bilistic programming languages …},
	author = {David Poole, Frank Wood},
	title = {Probabilistic programming languages: Independent choices and deterministic systems},
	year = {2022}
}

@misc{wood-probabilistic-surrogate-networks-2022,
	abstract = {We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure and control flow of the reference simulator. Our surrogates target stochastic simulators where the number of random variables itself can be stochastic and potentially unbounded. Our framework further enables an automatic replacement of the reference simulator with the surrogate when undertaking amortized inference. The fidelity and speed of our surrogates allow for both faster stochastic simulation and accurate and substantially faster posterior inference. Using an illustrative yet non-trivial example we show our surrogates’ ability to accurately model a probabilistic program with an unbounded number of random variables. We then proceed with an example that shows our surrogates are able to accurately model a complex structure like an unbounded stack in a program synthesis example. We further demonstrate how our surrogate modeling technique makes amortized inference in complex black-box simulators an order of magnitude faster. Specifically, we do simulator-based materials quality testing, inferring safety-critical latent internal temperature profiles of composite materials undergoing curing.},
	author = {Andreas Munk, Berend Zwartsenberg, Adam Ścibior, Atılım Güneş G Baydin, Andrew Stewart, Goran Fernlund, Anoush Poursartip, Frank Wood},
	title = {Probabilistic surrogate networks for simulators with unbounded randomness},
	year = {2022}
}

@misc{wood-learned-human-2022,
	abstract = {Models of human driving behavior have long been used for prediction in autonomous vehicles, but recently have also started being used to create non-playable characters for driving simulations. While such models are in many respects realistic, they tend to suffer from unacceptably high rates of driving infractions, such as collisions or off-road driving, particularly when deployed in map locations with road geometries dissimilar to the training dataset. In this paper we present a novel method for fine-tuning a foundation model of human driving behavior to novel locations where human demonstrations are not available which reduces the incidence of such infractions. The method relies on inference in the foundation model to generate infraction-free trajectories as well as additional penalties applied when fine-tuning the amortized inference behavioral model. We demonstrate this "titration" technique using the ITRA foundation behavior model trained on the INTERACTION dataset when transferring to CARLA map locations. We demonstrate a 76-86% reduction in infraction rate and provide evidence that further gains are possible with more computation or better inference algorithms.},
	author = {Vasileios Lioutas, Adam Scibior, Frank Wood},
	title = {TITRATED: Learned Human Driving Behavior without Infractions via Amortized Inference},
	year = {2022}
}

@misc{wood-vehicle-type-specific-2022,
	abstract = {We develop a generic mechanism for generating vehicle-type specific sequences of waypoints from a probabilistic foundation model of driving behavior. Many foundation behavior models are trained on data that does not include vehicle information, which limits their utility in downstream applications such as planning. Our novel methodology conditionally specializes such a behavior predictive model to a vehicle-type by utilizing byproducts of the reinforcement learning algorithms used to produce vehicle specific controllers. We show how to compose a vehicle specific value function estimate with a generic probabilistic behavior model to generate vehicle-type specific waypoint sequences that are more likely to be physically plausible then their vehicle-agnostic counterparts.},
	author = {Yunpeng Liu, Jonathan Wilder Lavington, Adam Scibior, Frank Wood},
	title = {Vehicle type specific waypoint generation},
	year = {2022}
}

@misc{wood-a-closer-look-2021,
	abstract = {The concept of reinforcement learning as inference (RLAI) has led to the creation of a variety of popular algorithms in deep reinforcement learning. Unfortunately, most research in this area relies on wider algorithmic innovations not necessarily relevant to such frameworks. Additionally, many seemingly unimportant modifications made to these algorithms, actually produce inconsistencies with the original inference problem posed by RLAI. Taking a divergence minimization perspective, this work considers some of the practical merits and theoretical issues created by the choice of loss function minimized in the policy update for off-policy reinforcement learning. Our results show that while the choice of divergence rarely has a major affect on the sample efficiency of the algorithm, it can have important practical repercussions on ease of implementation, computational efficiency, and restrictions to the distribution over actions.},
	author = {Jonathan Wilder Lavington, Michael Teng, Mark Schmidt, Frank Wood},
	title = {A closer look at gradient estimators with reinforcement learning as inference},
	year = {2021}
}

@misc{wood-assisting-the-adversary-2021,
	abstract = {Some of the most popular methods for improving the stability and performance of GANs involve constraining or regularizing the discriminator. In this paper we consider a largely overlooked regularization technique which we refer to as the Adversary's Assistant (AdvAs). We motivate this using a different perspective to that of prior work. Specifically, we consider a common mismatch between theoretical analysis and practice: analysis often assumes that the discriminator reaches its optimum on each iteration. In practice, this is essentially never true, often leading to poor gradient estimates for the generator. To address this, AdvAs is a penalty imposed on the generator based on the norm of the gradients used to train the discriminator. This encourages the generator to move towards points where the discriminator is optimal. We demonstrate the effect of applying AdvAs to several GAN objectives, datasets and network …},
	author = {Andreas Munk, William Harvey, Frank Wood},
	title = {Assisting the Adversary to Improve GAN Training},
	year = {2021}
}

@misc{wood-conditional-image-generation-2021,
	abstract = {We present a conditional variational auto-encoder (VAE) which, to avoid the substantial cost of training from scratch, uses an architecture and training objective capable of leveraging a foundation model in the form of a pretrained unconditional VAE. To train the conditional VAE, we only need to train an artifact to perform amortized inference over the unconditional VAE's latent variables given a conditioning input. We demonstrate our approach on tasks including image inpainting, for which it outperforms state-of-the-art GAN-based approaches at faithfully representing the inherent uncertainty. We conclude by describing a possible application of our inpainting model, in which it is used to perform Bayesian experimental design for the purpose of guiding a sensor.},
	author = {William Harvey, Saeid Naderiparizi, Frank Wood},
	title = {Conditional image generation by conditioning variational auto-encoders},
	year = {2021}
}

@misc{wood-differentiable-particle-filtering-2021,
	abstract = {Particle filters are not compatible with automatic differentiation due to the presence of discrete resampling steps. While known estimators for the score function, based on Fisher's identity, can be computed using particle filters, up to this point they required manual implementation. In this paper we show that such estimators can be computed using automatic differentiation, after introducing a simple correction to the particle weights. This correction utilizes the stop-gradient operator and does not modify the particle filter operation on the forward pass, while also being cheap and easy to compute. Surprisingly, with the same correction automatic differentiation also produces good estimators for gradients of expectations under the posterior. We can therefore regard our method as a general recipe for making particle filters differentiable. We additionally show that it produces desired estimators for second-order derivatives and how to extend it to further reduce variance at the expense of additional computation.},
	author = {Adam Ścibior, Frank Wood},
	title = {Differentiable particle filtering without modifying the forward pass},
	year = {2021}
}

@misc{wood-essentials-of-bayesian-2021,
	abstract = {These notes are heavily based on a chapter on unsupervised learning by Ghahramani (2004) and the “Pattern recognition and machine learning” book by Bishop (2006). The parts on sampling and Markov Chain Monte Carlo are based on a review by Neal (1993).Machine learning is usually divided into supervised, unsupervised and reinforcement learning. In supervised learning, our data consists of (x, y) pairs, where x is some input and y is the corresponding label. For example, x can be an image of a cat and y can be the label “cat.” These are used to train our model (eg regression) so that during test time, when we can accurately predict a label for a new input. This approach works very well but requires a lot of labelled inputs. Unsupervised learning is all about getting insights from data when we don’t have labels.},
	author = {Frank Wood},
	title = {Essentials of Bayesian Inference},
	year = {2021}
}

@misc{wood-a-generative-2021,
	abstract = {DARPAs Data Driven Discovery of Models (D3M) program aimed to automate machine learning so as to allow non-expert government users to pose queries, make predictions, and otherwise model data without necessarily having a background in data science or machine learning. Our research was to develop an extensive set of model primitives and contribute them to a library of discoverable component models that would be assembled by other teams pipeline search and tuning software tools. Our part of this work was developing a software toolchain that made it possible to vastly extend a set of such model primitives. Our contributions included significantly influencing the design of the D3M primitive interface, implementing the original core primitives that demonstrated this interface, implementing an additional number of specialized primitives, and explaining the opportunity, untapped, for program participants to construct their own primitives based on our developed and contributed generative model compiler technology.},
	author = {Frank Wood},
	title = {Hasty: A Generative Model Compiler},
	year = {2021}
}

@misc{wood-image-completion-via-2021,
	author = {William Harvey, Saeid Naderiparizi, Frank Wood},
	title = {Image completion via inference in deep generative models},
	year = {2021}
}

@misc{wood-imagining-the-road-2021,
	abstract = {We develop a deep generative model built on a fully differentiable simulator for multi-agent trajectory prediction. Agents are modeled with conditional recurrent variational neural networks (CVRNNs), which take as input an ego-centric birdview image representing the current state of the world and output an action, consisting of steering and acceleration, which is used to derive the subsequent agent state using a kinematic bicycle model. The full simulation state is then differentiably rendered for each agent, initiating the next time step. We achieve state-of-the-art results on the INTERACTION dataset, using standard neural architectures and a standard variational training objective, producing realistic multi-modal predictions without any ad-hoc diversity-inducing losses. We conduct ablation studies to examine individual components of the simulator, finding that both the kinematic bicycle model and the continuous …},
	author = {Adam Ścibior, Vasileios Lioutas, Daniele Reda, Peyman Bateni, Frank Wood},
	title = {Imagining the road ahead: Multi-agent trajectory prediction via differentiable simulation},
	year = {2021}
}

@misc{meent-probabilistic-deep-2021,
	abstract = {The enormous commercial success of machine learning has failed to translate into high performance military applications. Though deep learning is beginning to show impressive results in several specific military tasks, current capabilities fail to perform sufficiently due to their requirement for extremely large, labeled training sets. Under the Probabilistic Label-Efficient Deep Generative Structures (PLEDGES) project, part of DARPA's Learning with Less Labels (LwLL) program, we pursued pioneering research at the interface of probabilistic modeling and deep learning. We investigated three main directions. First, we developed Structured Deep Probabilistic Models (SDPMs), which define structured and disentangled joint probability distributions over unlabeled data observations. Second, we pursued efficient and accurate algorithms for high-capacity probabilistic models. Third, we developed probabilistic variants of deep learning models for semi-supervised and weakly supervised learning.},
	author = {Avi Pfeffer, Catherine Call, Frank Wood, Brad Rosenberg, Kirstin Bibbiani, Leonid Sigal, Ishaan Shah, Deniz Erdogmus, Sameer Singh, Jan W Van De Meent},
	title = {Probabilistic Label-Efficient Deep Generative Structures (PLEDGES)},
	year = {2021}
}

@misc{wood-robust-asymmetric-learning-2021,
	abstract = {Policies for partially observed Markov decision processes can be efficiently learned by imitating expert policies generated using asymmetric information. Unfortunately, existing approaches for this kind of imitation learning have a serious flaw: the expert does not know what the trainee cannot see, and as a result may encourage actions that are sub-optimal or unsafe under partial information. To address this issue, we derive an update which, when applied iteratively to an expert, maximizes the expected reward of the trainee’s policy. Using this update, we construct a computationally efficient algorithm, adaptive asymmetric DAgger (A2D), that jointly trains the expert and trainee policies. We then show that A2D allows the trainee to safely imitate the modified expert, and outperforms policies learned either by imitating a fixed expert or through direct reinforcement learning.},
	author = {Andrew Warrington, Jonathan W Lavington, Adam Scibior, Mark Schmidt, Frank Wood},
	title = {Robust asymmetric learning in pomdps},
	year = {2021}
}

@misc{campbell-sequential-monte-2021,
	abstract = {Sequential Monte Carlo (SMC) is a general-purpose methodology for recursive Bayesian inference, and is widely used in state space modeling and probabilistic programming. Its resample-move variant reduces the variance of posterior estimates by interleaving Markov chain Monte Carlo (MCMC) steps for particle “rejuvenation”; but this requires accessing all past observations and leads to linearly growing memory size and quadratic computation cost. Under the assumption of exchangeability, we introduce sequential core-set Monte Carlo (SCMC), which achieves constant space and linear time by rejuvenating based on sparse, weighted subsets of past data. In contrast to earlier approaches, which uniformly subsample or throw away observations, SCMC uses a novel online version of a state-of-the-art Bayesian core-set algorithm to incrementally construct a nonparametric, data-and model-dependent variational representation of the unnormalized target density. Experiments demonstrate significantly reduced approximation errors at negligible additional cost.},
	author = {Boyan Beronov, Christian Weilbach, Frank Wood, Trevor Campbell},
	title = {Sequential Core-Set Monte Carlo},
	year = {2021}
}

@misc{wood-generalizing-the-2021,
	abstract = {Many common machine learning methods involve the geometric annealing path, a sequence of intermediate densities between two distributions of interest constructed using the geometric average. While alternatives such as the moment-averaging path have demonstrated performance gains in some settings, their practical applicability remains limited by exponential family endpoint assumptions and a lack of closed form energy function. In this work, we introduce -paths, a family of paths which is derived from a generalized notion of the mean, includes the geometric and arithmetic mixtures as special cases, and admits a simple closed form involving the deformed logarithm function from nonextensive thermodynamics. Following previous analysis of the geometric path, we interpret our -paths as corresponding to a -exponential family of distributions, and provide a variational representation of intermediate densities as minimizing a mixture of -divergences to the endpoints. We show that small deviations away from the geometric path yield empirical gains for Bayesian inference using Sequential Monte Carlo and generative model evaluation using Annealed Importance Sampling.},
	author = {Vaden Masrani, Rob Brekelmans, Thang Bui, Frank Nielsen, Aram Galstyan, Greg Ver Steeg, Frank Wood},
	title = {q-paths: Generalizing the geometric annealing path using power means},
	year = {2021}
}

@misc{galstyan-all-in-the-2020,
	abstract = {The recently proposed Thermodynamic Variational Objective (TVO) leverages thermodynamic integration to provide a family of variational inference objectives, which both tighten and generalize the ubiquitous Evidence Lower Bound (ELBO). However, the tightness of TVO bounds was not previously known, an expensive grid search was used to choose a "schedule" of intermediate distributions, and model learning suffered with ostensibly tighter bounds. In this work, we propose an exponential family interpretation of the geometric mixture curve underlying the TVO and various path sampling methods, which allows us to characterize the gap in TVO likelihood bounds as a sum of KL divergences. We propose to choose intermediate distributions using equal spacing in the moment parameters of our exponential family, which matches grid search performance and allows the schedule to adaptively update over the course of training. Finally, we derive a doubly reparameterized gradient estimator which improves model learning and allows the TVO to benefit from more refined bounds. To further contextualize our contributions, we provide a unified framework for understanding thermodynamic integration and the TVO using Taylor series remainders.},
	author = {Rob Brekelmans, Vaden Masrani, Frank Wood, Greg Ver Steeg, Aram Galstyan},
	title = {All in the exponential family: Bregman duality in thermodynamic variational inference},
	year = {2020}
}

@misc{nielsen-annealed-importance-sampling-2020,
	abstract = {Annealed importance sampling (AIS) is the gold standard for estimating partition functions or marginal likelihoods, corresponding to importance sampling over a path of distributions between a tractable base and an unnormalized target. While AIS yields an unbiased estimator for any path, existing literature has been primarily limited to the geometric mixture or moment-averaged paths associated with the exponential family and KL divergence. We explore AIS using -paths, which include the geometric path as a special case and are related to the homogeneous power mean, deformed exponential family, and -divergence.},
	author = {Rob Brekelmans, Vaden Masrani, Thang Bui, Frank Wood, Aram Galstyan, Greg Ver Steeg, Frank Nielsen},
	title = {Annealed importance sampling with q-paths},
	year = {2020}
}

@misc{wood-coping-with-simulators-2020,
	abstract = {Deterministic models are approximations of reality that are easy to interpret and often easier to build than stochastic alternatives. Unfortunately, as nature is capricious, observational data can never be fully explained by deterministic models in practice. Observation and process noise need to be added to adapt deterministic models to behave stochastically, such that they are capable of explaining and extrapolating from noisy data. We investigate and address computational inefficiencies that arise from adding process noise to deterministic simulators that fail to return for certain inputs; a property we describe as’ brittle’. We show how to train a conditional normalizing flow to propose perturbations such that the simulator succeeds with high probability, increasing computational efficiency.},
	author = {Andrew Warrington, Saeid Naderiparizi, Frank Wood},
	title = {Coping with simulators that don’t always return},
	year = {2020}
}

@misc{wood-ensemble-a-2020,
	abstract = {There are currently many barriers that prevent non-experts from exploiting machine learning solutions ranging from the lack of intuition on statistical learning techniques to the trickiness of hyperparameter tuning. Such barriers have led to an explosion of interest in automated machine learning (AutoML), whereby an off-the-shelf system can take care of many of the steps for end-users without the need for expertise in machine learning. This paper presents Ensemble Squared (Ensemble), an AutoML system that ensembles the results of state-of-the-art open-source AutoML systems. Ensemble exploits the diversity of existing AutoML systems by leveraging the differences in their model search space and heuristics. Empirically, we show that diversity of each AutoML system is sufficient to justify ensembling at the AutoML system level. In demonstrating this, we also establish new state-of-the-art AutoML results on the OpenML tabular classification benchmark.},
	author = {Jason Yoo, Tony Joseph, Dylan Yung, S Ali Nasseri, Frank Wood},
	title = {Ensemble squared: A meta automl system},
	year = {2020}
}

@misc{wood-gaussian-process-bandit-2020,
	abstract = {Achieving the full promise of the Thermodynamic Variational Objective (TVO), a recently proposed variational inference objective that lower-bounds the log evidence via one-dimensional Riemann integration, requires choosing a``schedule''of sorted discretization points. This paper introduces a bespoke Gaussian process bandit optimization method for automatically choosing these points. Our approach not only automates their one-time selection, but also dynamically adapts their positions over the course of optimization, leading to improved model learning and inference. We provide theoretical guarantees that our bandit optimization converges to the regret-minimizing choice of integration points. Empirical validation of our algorithm is provided in terms of improved learning and inference in Variational Autoencoders and sigmoid belief networks.},
	author = {Vu Nguyen, Vaden Masrani, Rob Brekelmans, Michael Osborne, Frank Wood},
	title = {Gaussian process bandit optimization of the thermodynamic variational objective},
	year = {2020}
}

@misc{sigal-improved-visual-2020,
	abstract = {Few-shot learning is a fundamental task in computer vision that carries the promise of alleviating the need for exhaustively labeled data. Most few-shot learning approaches to date have focused on progressively more complex neural feature extractors and classifier adaptation strategies, and the refinement of the task definition itself. In this paper, we explore the hypothesis that a simple class-covariance-based distance metric, namely the Mahalanobis distance, adopted into a state of the art few-shot learning approach (CNAPS) can, in and of itself, lead to a significant performance improvement. We also discover that it is possible to learn adaptive feature extractors that allow useful estimation of the high dimensional feature covariances required by this metric from surprisingly few samples. The result of our work is a new" Simple CNAPS" architecture which has up to 9.2% fewer trainable parameters than CNAPS and performs up to 6.1% better than state of the art on the standard few-shot image classification benchmark dataset.},
	author = {Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, Leonid Sigal},
	title = {Improved few-shot visual classification},
	year = {2020}
}

@misc{wood-revisiting-reweighted-2020,
	abstract = {Stochastic control-flow models (SCFMs) are a class of generative models that involve branching on choices from discrete random variables. Amortized gradient-based learning of SCFMs is challenging as most approaches targeting discrete variables rely on their continuous relaxations—which can be intractable in SCFMs, as branching on relaxations requires evaluating all (exponentially many) branching paths. Tractable alternatives mainly combine REINFORCE with complex control-variate schemes to improve the variance of naive estimators. Here, we revisit the reweighted wake-sleep (RWS)[5] algorithm, and through extensive evaluations, show that it outperforms current state-of-the-art methods in learning SCFMs. Further, in contrast to the importance weighted autoencoder, we observe that RWS learns better models and inference networks with increasing numbers of particles. Our results suggest that RWS is a competitive, often preferable, alternative for learning SCFMs.},
	author = {Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, Frank Wood},
	title = {Revisiting reweighted wake-sleep for models with stochastic control flow},
	year = {2020}
}

@misc{wood-sequential-generative-2020,
	abstract = {We introduce a novel objective for training deep generative time-series models with discrete latent variables for which supervision is only sparsely available. This instance of semi-supervised learning is challenging for existing methods, because the exponential number of possible discrete latent configurations results in high variance gradient estimators. We first overcome this problem by extending the standard semi-supervised generative modeling objective with reweighted wake-sleep. However, we find that this approach still suffers when the frequency of available labels varies between training sequences. Finally, we introduce a unified objective inspired by teacher-forcing and show that this approach is robust to variable length supervision. We call the resulting method caffeinated wake-sleep (CWS) to emphasize its additional dependence on real data. We demonstrate its effectiveness with experiments on MNIST, handwriting, and fruit fly trajectory data.},
	author = {Michael Teng, Tuan Anh Le, Adam Scibior, Frank Wood},
	title = {Semi-supervised sequential generative models},
	year = {2020}
}

@misc{harvey-structured-conditional-continuous-2020,
	abstract = {We exploit minimally faithful inversion of graphical model structures to specify sparse continuous normalizing flows (CNFs) for amortized inference. We find that the sparsity of this factorization can be exploited to reduce the numbers of parameters in the neural network, adaptive integration steps of the flow, and consequently FLOPs at both training and inference time without decreasing performance in comparison to unconstrained flows. By expressing the structure inversion as a compilation pass in a probabilistic programming language, we are able to apply it in a novel way to models as complex as convolutional neural networks. Furthermore, we extend the training objective for CNFs in the context of inference amortization to the symmetric Kullback-Leibler divergence, and demonstrate its theoretical and practical advantages.},
	author = {Christian Weilbach, Boyan Beronov, Frank Wood, William Harvey},
	title = {Structured conditional continuous normalizing flows for efficient amortized inference in graphical models},
	year = {2020}
}

@misc{zaidi-bayesian-2020,
	abstract = {Standard approaches for Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions--a computational pipeline that is inefficient when the target function(s) are known up-front. We address this inefficiency by introducing a framework for target-aware Bayesian inference (TABI) that estimates these expectations directly. While conventional Monte Carlo estimators have a fundamental limit on the error they can achieve for a given sample size, our TABI framework is able to breach this limit; it can theoretically produce arbitrarily accurate estimators using only three samples, while we show empirically that it can also breach this limit in practice. We utilize our TABI framework by combining it with adaptive importance sampling approaches and show both theoretically and empirically that the resulting estimators are capable of converging faster than the standard O(1/N) Monte Carlo rate, potentially producing rates as fast as O(1/N2). We further combine our TABI framework with amortized inference methods, to produce a method for amortizing the cost of calculating expectations. Finally, we show how TABI can be used to convert any marginal likelihood estimator into a target aware inference scheme and demonstrate the substantial benefits this can yield.},
	author = {Tom Rainforth, Adam Golinski, Frank Wood, Sheheryar Zaidi},
	title = {Target–aware bayesian inference: how to beat optimal conventional estimators},
	year = {2020}
}

@misc{wood-uncertainty-in-neural-2020,
	abstract = {We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models. We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large. We instead focus our attention on the case where the amount of conditioning data is small. We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime. Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes. Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.},
	author = {Saeid Naderiparizi, Kenny Chiu, Benjamin Bloem-Reddy, Frank Wood},
	title = {Uncertainty in neural processes},
	year = {2020}
}

@misc{rainforth-amortized-monte-carlo-2019,
	abstract = {Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions {—} a computational pipeline which is inefficient when the target function (s) are known upfront. In this paper, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI operates similarly to amortized inference but produces three distinct amortized proposals, each tailored to a different component of the overall expectation calculation. At runtime, samples are produced separately from each amortized proposal, before being combined to an overall estimate of the expectation. We show that while existing approaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically produce arbitrarily small errors for any integrable target function using only a single sample from each proposal at runtime. We further show that it is able to empirically outperform the theoretically optimal selfnormalized importance sampler on a number of example problems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.},
	author = {Adam Golinski, Frank Wood, Tom Rainforth},
	title = {Amortized monte carlo integration},
	year = {2019}
}

@misc{wood-attention-for-inference-2019,
	abstract = {We present a new approach to automatic amortized inference in universal probabilistic programs which improves performance compared to current methods. Our approach is a variation of inference compilation (IC) which leverages deep neural networks to approximate a posterior distribution over latent variables in a probabilistic program. A challenge with existing IC network architectures is that they can fail to model long-range dependencies between latent variables. To address this, we introduce an attention mechanism that attends to the most salient variables previously sampled in the execution of a probabilistic program. We demonstrate that the addition of attention allows the proposal distributions to better match the true posterior, enhancing inference about latent variables in simulators.},
	author = {William Harvey, Andreas Munk, Atılım Güneş Baydin, Alexander Bergholm, Frank Wood},
	title = {Attention for inference compilation},
	year = {2019}
}

@misc{wood-deep-probabilistic-surrogate-2019,
	abstract = {We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of existing stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure of the reference simulators. The particular way we achieve this allows us to replace the reference simulator with the surrogate when undertaking amortized inference in the probabilistic programming sense. The fidelity and speed of our surrogates allow for not only faster “forward” stochastic simulation but also for accurate and substantially faster inference. We support these claims via experiments that in-},
	author = {Andreas Munk, Adam Scibior, Atılım Güneş Baydin, Andrew Stewart, Goran Fernlund, Anoush Poursartip, Frank Wood},
	title = {Deep probabilistic surrogate networks for universal simulator approximation},
	year = {2019}
}

@misc{rainforth-efficient-bayesian-inference-2019,
	abstract = {We introduce two approaches for conducting efficient Bayesian inference in stochastic simulators containing nested stochastic sub-procedures, i.e., internal procedures for which the density cannot be calculated directly such as rejection sampling loops. The resulting class of simulators are used extensively throughout the sciences and can be interpreted as probabilistic generative models. However, drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density, preventing the use of many standard inference procedures like Markov Chain Monte Carlo (MCMC). To address this, we introduce inference algorithms based on a two-step approach that first approximates the conditional densities of the individual sub-procedures, before using these approximations to run MCMC methods on the full program. Because the sub-procedures can be dealt with separately and are lower-dimensional than that of the overall problem, this two-step process allows them to be isolated and thus be tractably dealt with, without placing restrictions on the overall dimensionality of the problem. We demonstrate the utility of our approach on a simple, artificially constructed simulator.},
	author = {Bradley Gram-Hansen, Christian Schroeder de Witt, Robert Zinkov, Saeid Naderiparizi, Adam Scibior, Andreas Munk, Frank Wood, Mehrdad Ghadiri, Philip Torr, Yee Whye Teh, Atilim Gunes Baydin, Tom Rainforth},
	title = {Efficient Bayesian inference for nested simulators},
	year = {2019}
}

@misc{wood-efficient-inference-amortization-2019,
	abstract = {We introduce a more efficient neural architecture for amortized inference, which combines continuous and conditional normalizing flows using a principled choice of structure. Our gradient flow derives its sparsity pattern from the minimally faithful inverse of its underlying graphical model. We find that this factorization reduces the necessary numbers both of parameters in the neural network and of adaptive integration steps in the ODE solver. Consequently, the throughput at training time and inference time is increased, without decreasing performance in comparison to unconstrained flows. By expressing the structural inversion and the flow construction as compilation passes of a probabilistic programming language, we demonstrate their applicability to the stochastic inversion of realistic models such as convolutional neural networks (CNN).},
	author = {Christian Weilbach, Boyan Beronov, William Harvey, Frank Wood},
	title = {Efficient inference amortization in graphical models using structured continuous conditional normalizing flows},
	year = {2019}
}

@misc{wood-efficient-probabilistic-inference-2019,
	abstract = {We present a novel probabilistic programming framework that couples directly to existing large-scale simulators through a cross-platform probabilistic execution protocol, which allows general-purpose inference engines to record and control random number draws within simulators in a language-agnostic way. The execution of existing simulators as probabilistic programs enables highly interpretable posterior inference in the structured model defined by the simulator code base. We demonstrate the technique in particle physics, on a scientifically accurate simulation of the tau lepton decay, which is a key ingredient in establishing the properties of the Higgs boson. Inference efficiency is achieved via inference compilation where a deep recurrent neural network is trained to parameterize proposal distributions and control the stochastic simulator in a sequential importance sampling scheme, at a fraction of the computational cost of a Markov chain Monte Carlo baseline.},
	author = {Atilim Gunes Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Saeid Naderiparizi, Andreas Munk, Jialin Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, Philip Torr, Victor Lee, Kyle Cranmer, Mr Prabhat, Frank Wood},
	title = {Efficient probabilistic inference in the quest for physics beyond the standard model},
	year = {2019}
}

@misc{wood-bringing-probabilistic-2019,
	abstract = {Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN--LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global minibatch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.},
	author = {Baydin Atılım Günes, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr, Victor Lee, Kyle Cranmer, Frank Wood},
	title = {Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale},
	year = {2019}
}

@misc{wood-bringing-probabilistic-2019,
	abstract = {Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN-LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global mini-batch size of 128k: achieving a performance of 450 Tflop/s …},
	author = {Atilim Güneş Baydin, Lei Shao, Wahid Bhimji, Lukas Heinrich, Lawrence Meadows, Jialin Liu, Andreas Munk, Saeid Naderiparizi, Bradley Gram-Hansen, Gilles Louppe, Mingfei Ma, Xiaohui Zhao, Philip Torr, Victor Lee, Kyle Cranmer, Prabhat, Frank Wood},
	title = {Etalumis: Bringing probabilistic programming to scientific simulators at scale},
	year = {2019}
}

@misc{wood-imitation-learning-of-2019,
	abstract = {We apply recent advances in deep generative modeling to the task of imitation learning from biological agents. Specifically, we apply variations of the variational recurrent neural network model to a multi-agent setting where we learn policies of individual uncoordinated agents acting based on their perceptual inputs and their hidden belief state. We learn stochastic policies for these agents directly from observational data, without constructing a reward function. An inference network learned jointly with the policy allows for efficient inference over the agent’s belief state given a sequence of its current perceptual inputs and the prior actions it performed, which lets us extrapolate observed sequences of behavior into the future while maintaining uncertainty estimates over future trajectories. We test our approach on a dataset of flies interacting in a 2D environment, where we demonstrate better predictive performance than existing approaches which learn deterministic policies with recurrent neural networks. We further show that the uncertainty estimates over future trajectories we obtain are well calibrated, which makes them useful for a variety of downstream processing tasks.},
	author = {Michael Teng, Tuan Anh Le, Adam Scibior, Frank Wood},
	title = {Imitation learning of factored multi-agent reactive models},
	year = {2019}
}

@misc{wood-a-2019,
	abstract = {We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine.},
	author = {Yuan Zhou, Bradley J Gram-Hansen, Tobias Kohn, Tom Rainforth, Hongseok Yang, Frank Wood},
	title = {LF-PPL: A low-level first order probabilistic programming language for non-differentiable models},
	year = {2019}
}

@misc{wood-the-thermodynamic-variational-2019,
	abstract = {We introduce the thermodynamic variational objective (TVO) for learning in both continuous and discrete deep generative models. The TVO arises from a key connection between variational inference and thermodynamic integration that results in a tighter lower bound to the log marginal likelihood than the standard variational evidence lower bound (ELBO) while remaining as broadly applicable. We provide a computationally efficient gradient estimator for the TVO that applies to continuous, discrete, and non-reparameterizable distributions and show that the objective functions used in variational inference, variational autoencoders, wake sleep, and inference compilation are all special cases of the TVO. We use the TVO to learn both discrete and continuous deep generative models and empirically demonstrate state of the art model and inference network learning.},
	author = {Vaden Masrani, Tuan Anh Le, Frank Wood},
	title = {The thermodynamic variational objective},
	year = {2019}
}

@misc{wood-the-virtual-patch-2019,
	abstract = {We develop a stochastic whole-brain and body simulator of the nematode roundworm Caenorhabditis elegans (C. elegans) and show that it is sufficiently regularizing to allow imputation of latent membrane potentials from partial calcium fluorescence imaging observations. This is the first attempt we know of to "complete the circle," where an anatomically grounded whole-connectome simulator is used to impute a time-varying "brain" state at single-cell fidelity from covariates that are measurable in practice. The sequential Monte Carlo (SMC) method we employ not only enables imputation of said latent states but also presents a strategy for learning simulator parameters via variational optimization of the noisy model evidence approximation provided by SMC. Our imputation and parameter estimation experiments were conducted on distributed systems using novel implementations of the aforementioned techniques applied to synthetic data of dimension and type representative of that which are measured in laboratories currently.},
	author = {Andrew Warrington, Arthur Spencer, Frank Wood},
	title = {The virtual patch clamp: Imputing c. elegans membrane potentials from calcium imaging},
	year = {2019}
}

@misc{wood-an-introduction-to-2018,
	abstract = {This book is a graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning is a foundational computation central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a first-order probabilistic programming language (PPL) whose programs correspond to graphical models with a known, finite, set of random variables. In the context of this PPL we introduce fundamental inference algorithms and describe how they can be implemented. We then turn to higher-order probabilistic programming languages. Programs in such languages can define models with dynamic computation graphs, which may not instantiate the same set of random variables in each execution. Inference requires methods that generate samples by repeatedly evaluating the program. Foundational algorithms for this kind of language are discussed in the context of an interface between program executions and an inference controller. Finally we consider the intersection of probabilistic and differentiable programming. We begin with a discussion of automatic differentiation, and how it can be used to implement efficient inference methods based on Hamiltonian Monte Carlo. We then discuss gradient-based maximum likelihood …},
	author = {Jan-Willem Van De Meent, Brooks Paige, Hongseok Yang, Frank Wood},
	title = {An introduction to probabilistic programming},
	year = {2018}
}

@misc{wood-bayesian-distributed-stochastic-2018,
	abstract = {We introduce Bayesian distributed stochastic gradient descent (BDSGD), a high-throughput algorithm for training deep neural networks on parallel clusters. This algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini-batch gradient computation times in a compute cluster specific manner. Specifically, our algorithm mitigates the straggler effect in synchronous, gradient-based optimization by choosing an optimal cutoff beyond which mini-batch gradient messages from slow workers are ignored. In our experiments, we show that eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness. The principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance improves substantially over the static-cutoff prior art, leading to reduced deep neural net training times on large computer clusters.},
	author = {Michael Teng, Frank Wood},
	title = {Bayesian distributed stochastic gradient descent},
	year = {2018}
}

@misc{whiteson-deep-variational-reinforcement-2018,
	abstract = {Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of rewards and incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.},
	author = {Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon Whiteson},
	title = {Deep variational reinforcement learning for POMDPs},
	year = {2018}
}

@misc{wood-discontinuous-hamiltonian-monte-2018,
	abstract = {Hamiltonian Monte Carlo (HMC) is the dominant statistical inference algorithm used in most popular “first-order differentiable” probabilisticprogramming languages. HMC requires that the joint density be differentiable with respect to all latent variables. This complicates expressing some models in such languages and prohibits others. A recently proposed new integrator for HMC yielded a new Discontinuous HMC (DHMC) algorithm that can be used for inference in models with joint densities that have discontinuities. In this paper we show how to use DHMC for inference in probabilistic programs. To do this we introduce a sufficient set of language restrictions, a corresponding mathematical formalism that ensures that any joint density denoted in such a language has a suitably low measure of discontinuous points, and a recipe for how to apply DHMC in the more general probabilistic-programming context. Our experimental findings demonstrate the correctness of this approach.},
	author = {Bradley Gram-Hansen, Yuan Zhou, Tobias Kohn, Hongseok Yang, Frank Wood},
	title = {Discontinuous hamiltonian monte carlo for probabilistic programs},
	year = {2018}
}

@misc{wood-efficient-probabilistic-inference-2018,
	abstract = {We present a novel probabilistic programming framework that couples directly to existing large-scale simulators through a cross-platform probabilistic execution protocol, which allows general-purpose inference engines to record and control random number draws within simulators in a language-agnostic way. The execution of existing simulators as probabilistic programs enables highly interpretable posterior inference in the structured model defined by the simulator code base. We demonstrate the technique in particle physics, on a scientifically accurate simulation of the tau lepton decay, which is a key ingredient in establishing the properties of the Higgs boson. Inference efficiency is achieved via inference compilation where a deep recurrent neural network is trained to parameterize proposal distributions and control the stochastic simulator in a sequential importance sampling scheme, at a fraction of the …},
	author = {Atılım Güneş Baydin, Lukas Heinrich, Wahid Bhimji, Lei Shao, Saeid Naderiparizi, Andreas Munk, Jialin Liu, Bradley Gram-Hansen, Gilles Louppe, Lawrence Meadows, Philip Torr, Victor Lee, Kyle Cranmer, Frank Wood},
	title = {Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model},
	year = {2018}
}

@misc{wood-faithful-inversion-of-2018,
	abstract = {Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties:(a) they do not encode any independence assertions that are absent from the model and;(b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.},
	author = {Stefan Webb, Adam Golinski, Rob Zinkov, Tom Rainforth, Yee Whye Teh, Frank Wood},
	title = {Faithful inversion of generative models for effective amortized inference},
	year = {2018}
}

@misc{wood-hamiltonian-monte-carlo-2018,
	abstract = {Hamiltonian Monte Carlo (HMC) is arguably the dominant statistical inference algorithm used in most popular "first-order differentiable" Probabilistic Programming Languages (PPLs). However, the fact that HMC uses derivative information causes complications when the target distribution is non-differentiable with respect to one or more of the latent variables. In this paper, we show how to use extensions to HMC to perform inference in probabilistic programs that contain discontinuities. To do this, we design a Simple first-order Probabilistic Programming Language (SPPL) that contains a sufficient set of language restrictions together with a compilation scheme. This enables us to preserve both the statistical and syntactic interpretation of if-else statements in the probabilistic program, within the scope of first-order PPLs. We also provide a corresponding mathematical formalism that ensures any joint density denoted in such a language has a suitably low measure of discontinuities.},
	author = {Bradley Gram-Hansen, Yuan Zhou, Tobias Kohn, Tom Rainforth, Hongseok Yang, Frank Wood},
	title = {Hamiltonian monte carlo for probabilistic programs with discontinuities},
	year = {2018}
}

@misc{wood-high-throughput-synchronous-2018,
	abstract = {We introduce a new, high-throughput, synchronous, distributed, data-parallel, stochastic-gradient-descent learning algorithm. This algorithm uses amortized inference in a compute-cluster-specific, deep, generative, dynamical model to perform joint posterior predictive inference of the mini-batch gradient computation times of all worker-nodes in a parallel computing cluster. We show that a synchronous parameter server can, by utilizing such a model, choose an optimal cutoff time beyond which mini-batch gradient messages from slow workers are ignored that maximizes overall mini-batch gradient computations per second. In keeping with earlier findings we observe that, under realistic conditions, eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness. The principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance to dynamically adjust the cutoff improves substantially over the static-cutoff prior art, leading to, among other things, significantly reduced deep neural net training times on large computer clusters.},
	author = {Michael Teng, Frank Wood},
	title = {High throughput synchronous distributed stochastic gradient descent},
	year = {2018}
}

@misc{meent-inference-adaptive-2018,
	abstract = {We introduce inference trees (ITs), a new class of inference methods that build on ideas from Monte Carlo tree search to perform adaptive sampling in a manner that balances exploration with exploitation, ensures consistency, and alleviates pathologies in existing adaptive methods. ITs adaptively sample from hierarchical partitions of the parameter space, while simultaneously learning these partitions in an online manner. This enables ITs to not only identify regions of high posterior mass, but also maintain uncertainty estimates to track regions where significant posterior mass may have been missed. ITs can be based on any inference method that provides a consistent estimate of the marginal likelihood. They are particularly effective when combined with sequential Monte Carlo, where they capture long-range dependencies and yield improvements beyond proposal adaptation alone.},
	author = {Tom Rainforth, Yuan Zhou, Xiaoyu Lu, Yee Whye Teh, Frank Wood, Hongseok Yang, Jan-Willem van de Meent},
	title = {Inference trees: Adaptive inference with exploration},
	year = {2018}
}

@misc{wood-on-nesting-monte-2018,
	abstract = {Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.},
	author = {Tom Rainforth, Rob Cornish, Hongseok Yang, Andrew Warrington, Frank Wood},
	title = {On nesting monte carlo estimators},
	year = {2018}
}

@misc{wood-revisiting-reweighted-2018,
	abstract = {Discrete latent-variable models, while applicable in a variety of settings, can often be difficult to learn. Sampling discrete latent variables can result in high-variance gradient estimators for two primary reasons: 1) branching on the samples within the model, and 2) the lack of a pathwise derivative for the samples. While current state-of-the-art methods employ control-variate schemes for the former and continuous-relaxation methods for the latter, their utility is limited by the complexities of implementing and training effective control-variate schemes and the necessity of evaluating (potentially exponentially) many branch paths in the model. Here, we revisit the Reweighted Wake Sleep (RWS; Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it circumvents both these issues, outperforming current state-of-the-art methods in learning discrete latent-variable models. Moreover, we observe that, unlike the Importance-weighted Autoencoder, RWS learns better models and inference networks with increasing numbers of particles, and that its benefits extend to continuous latent-variable models as well. Our results suggest that RWS is a competitive, often preferable, alternative for learning deep generative models.},
	author = {Tuan Anh Le, Adam R Kosiorek, N Siddharth, Yee Whye Teh, Frank Wood},
	title = {Revisiting reweighted wake-sleep},
	year = {2018}
}

@misc{teh-tighter-variational-bounds-2018,
	abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
	author = {Tom Rainforth, Adam Kosiorek, Tuan Anh Le, Chris Maddison, Maximilian Igl, Frank Wood, Yee Whye Teh},
	title = {Tighter variational bounds are not necessarily better},
	year = {2018}
}

@misc{wood-towards-a-testable-2018,
	abstract = {We consider the question of how to assess generative adversarial networks, in particular with respect to whether or not they generalise beyond memorising the training data. We propose a simple procedure for assessing generative adversarial network performance based on a principled consideration of what the actual goal of generalisation is. Our approach involves using a test set to estimate the Wasserstein distance between the generative distribution produced by our procedure, and the underlying data distribution. We use this procedure to assess the performance of several modern generative adversarial network architectures. We find that this procedure is sensitive to the choice of ground metric on the underlying data space, and suggest a choice of ground metric that substantially improves performance.  We finally suggest that attending to the ground metric used in Wasserstein generative adversarial network training may be fruitful, and outline a concrete pathway towards doing so.},
	author = {Robert Cornish, Hongseok Yang, Frank Wood},
	title = {Towards a Testable Notion of Generalization for Generative Adversarial Networks},
	year = {2018}
}

@misc{wood-sequential-monte-2017,
	abstract = {We build on auto-encoding sequential Monte Carlo (AESMC): a method for model and proposal learning based on maximizing the lower bound to the log marginal likelihood in a broad family of structured probabilistic models. Our approach relies on the efficiency of sequential Monte Carlo (SMC) for performing inference in structured probabilistic models and the flexibility of deep neural networks to model complex conditional probability distributions. We develop additional theoretical insights and introduce a new training procedure which improves both model and proposal learning. We demonstrate that our approach provides a fast, easy-to-implement and scalable means for simultaneous model learning and proposal adaptation in deep generative models.},
	author = {Tuan Anh Le, Maximilian Igl, Tom Rainforth, Tom Jin, Frank Wood},
	title = {Auto-encoding sequential monte carlo},
	year = {2017}
}

@misc{wood-unsupervised-machine-2017,
	abstract = {These notes are heavily based on a chapter on unsupervised learning by Ghahramani (2004) and the “Pattern recognition and machine learning” book by Bishop (2006). The parts on sampling and Markov Chain Monte Carlo are based on a review by Neal (1993).Machine learning is usually divided into supervised, unsupervised and reinforcement learning. In supervised learning, our data consists of (x, y) pairs, where x is some input and y is the corresponding label. For example, x can be an image of a cat and y can be the label “cat.” These are used to train our model (eg regression) so that during test time, when we can accurately predict a label for a new input. This approach works very well but requires a lot of labelled inputs. Unsupervised learning is all about getting insights from data when we don’t have labels.},
	author = {Frank Wood},
	title = {C19: Unsupervised Machine Learning},
	year = {2017}
}

@misc{yang-computing-2017,
	abstract = {There is provided a computing engine (10) for use in simulating a complex system (20), controlling the complex system (20), or a combination of simulating and controlling the complex system (20), wherein the computing engine (10) includes a data processing arrangement (40) that is operable to execute one or more program instructions. The},
	author = {Frank Wood, Mike Wu, Yura Perov, Hongseok Yang},
	title = {Computing engine, software, system and method},
	year = {2017}
}

@misc{yang-efficient-exact-inference-2017,
	abstract = {The design of a general probabilistic programming system involves a balancing act between two deeply conflicting concerns. On the one hand, the system should provide as uniform and flexible an interface for specifying models as possible; on the other, it should be capable of doing efficient inference for any particular model specified. Current systems lie somewhere on a spectrum that ranges from highly expressive languages such as Church [2], Anglican [10], and Venture [3], to highly performant languages like Figaro [7], FACTORIE [4], and Infer .NET [5]. It has not yet been shown possible to optimise both these concerns simultaneously. To improve on this predicament we consider the class of discrete graphical models, for which various efficient exact inference algorithms exist. We present a technique for determining when an Anglican program expresses such a model, which allows us to achieve a substantial increase in inference performance in this case. Our approach can handle complicated language features including higher-order functions, bounded recursion, and data structures, which means that, for the discrete subset of Anglican, we do not incur any loss in expressiveness. Moreover, the resulting inference is exact, which can be useful in contexts where very high accuracy is required, or for doing nested inference inside larger models. Details of Anglican can be found in [10], but for our purposes its semantics may be understood as the Clojure programming language augmented with a sample statement and an observe statement. sample takes as argument a distribution object∆(which may be obtained by calling the built-in flip or dirac …},
	author = {Robert Cornish, Frank Wood, Hongseok Yang},
	title = {Efficient exact inference in discrete Anglican programs},
	year = {2017}
}

@misc{shen-training-of-2017,
	abstract = {In this work we present a unified interface and methodology for performing end-to-end gradient-based refinement of pipelines of differentiable machine-learning primitives.  This is distinguished from recent interoperability efforts such as the Open Neural Network Exchange (ONNX) format and other language-centric cross-compilation approaches in that the final pipeline does not need to be implemented nor trained in the same language nor cross-compiled into any single language; in other words, primitives may be written and pre-trained in PyTorch, TensorFlow, Caffe, scikit-learn or any of the other popular machine learning frameworks and fine-tuned end-to-end while being executed directly in their host frameworks.  Provided primitives expose our proposed interface, it is possible to automatically compose all such primitives and refine them based on an end-to-end loss.},
	author = {Mitar Milutinovic, Atılım Güneş Baydin, Robert Zinkov, William Harvey, Dawn Song, Frank Wood, Wade Shen},
	title = {End-to-end training of differentiable pipelines across machine learning frameworks},
	year = {2017}
}

@misc{davy-generalized-p-2017,
	abstract = {This article introduces a class of first-order stationary time-varying Pitman-Yor processes. Subsuming our construction of time-varying Dirichlet processes presented in (Caron et al., 2007), these models can be used for time-dynamic density estimation and clustering. Our intuitive and simple construction relies on a generalized Pólya urn scheme. Significantly, this construction yields marginal distributions at each time point that can be explicitly characterized and easily controlled. Inference is performed using Markov chain Monte Carlo and sequential Monte Carlo methods. We demonstrate our models and algorithms on epidemiological and video tracking data.},
	author = {François Caron, Willie Neiswanger, Frank Wood, Arnaud Doucet, Manuel Davy},
	title = {Generalized P {\'o} lya Urn for Time-Varying Pitman-Yor Processes},
	year = {2017}
}

@misc{bhimji-improvements-to-inference-2017,
	abstract = {We consider the problem of Bayesian inference in the family of probabilistic models implicitly defined by stochastic generative models of data. In scientific fields ranging from population biology to cosmology, low-level mechanistic components are composed to create complex generative models. These models lead to intractable likelihoods and are typically non-differentiable, which poses challenges for traditional approaches to inference. We extend previous work in "inference compilation", which combines universal probabilistic programming and deep learning methods, to large-scale scientific simulators, and introduce a C++ based probabilistic programming library called CPProb. We successfully use CPProb to interface with SHERPA, a large code-base used in particle physics. Here we describe the technical innovations realized and planned for this library.},
	author = {Mario Lezcano Casado, Atilim Gunes Baydin, David Martinez Rubio, Tuan Anh Le, Frank Wood, Lukas Heinrich, Gilles Louppe, Kyle Cranmer, Karen Ng, Wahid Bhimji},
	title = {Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators},
	year = {2017}
}

@misc{bhimji-improvements-to-inference-2017,
	abstract = {We consider the problem of Bayesian inference in the family of probabilistic models implicitly defined by stochastic generative models of data. In scientific fields ranging from population biology to cosmology, low-level mechanistic components are composed to create complex generative models. These models lead to intractable likelihoods and are typically non-differentiable, which poses challenges for traditional approaches to inference. We extend previous work in "inference compilation", which combines universal probabilistic programming and deep learning methods, to large-scale scientific simulators, and introduce a C++ based probabilistic programming library called CPProb. We successfully use CPProb to interface with SHERPA, a large code-base used in particle physics. Here we describe the technical innovations realized and planned for this library.},
	author = {Mario Lezcano Casado, Atilim Gunes Baydin, David Martínez Rubio, Tuan Anh Le, Frank Wood, Lukas Heinrich, Gilles Louppe, Kyle Cranmer, Karen Ng, Wahid Bhimji},
	title = {Improvements to inference compilation for probabilistic programming in large-scale scientific simulators},
	year = {2017}
}

@misc{wood-inference-compilation-and-2017,
	abstract = {We introduce a method for using deep neural networks to amortize the cost of inference in models from the family induced by universal probabilistic programming languages, establishing a framework that combines the strengths of probabilistic programming and deep learning methods. We call what we do “compilation of inference” because our method transforms a denotational specification of an inference problem in the form of a probabilistic program written in a universal programming language into a trained neural network denoted in a neural network specification language. When at test time this neural network is fed observational data and executed, it performs approximate inference in the original model specified by the probabilistic program. Our training objective and learning procedure are designed to allow the trained neural network to be used as a proposal distribution in a sequential importance sampling inference engine. We illustrate our method on mixture models and Captcha solving and show significant speedups in the efficiency of inference.},
	author = {Tuan Anh Le, Atilim Gunes Baydin, Frank Wood},
	title = {Inference compilation and universal probabilistic programming},
	year = {2017}
}

@misc{wood-interpreting-lion-behaviour-2017,
	abstract = {Interpreting Lion Behaviour as Probabilistic Programs - Utrecht University Skip to main navigation 
Skip to search Skip to main content Utrecht University Home Utrecht University Logo Help & FAQ 
Home People Research units Research output Activities In the media Prizes & Grants Search 
by expertise, name or affiliation Interpreting Lion Behaviour as Probabilistic Programs Neil Dhir, 
Matthijs Vákár, Matthew Wijers, Andrew Markham, Frank D. Wood Sub Software Technology 
Research output: Chapter in Book/Report/Conference proceeding › Conference contribution 
› Academic › peer-review Overview Original language English Title of host publication 
Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence, UAI 
2017, Sydney, Australia, August 11-15, 2017 Editors Gal Elidan, Kristian Kersting, Alexander 
Ihler Publisher AUAI Press Publication status Published - 2017 Access to Document http:…},
	author = {Neil Dhir, Matthijs Vákár, Matthew Wijers, Andrew Markham, Frank D Wood},
	title = {Interpreting Lion Behaviour as Probabilistic Programs},
	year = {2017}
}

@misc{macdonald-interpreting-lion-behaviour-2017,
	abstract = {We consider the problem of unsupervised learning of meaningful behavioural segments of high-dimensional time-series observations, collected from a pride of African lions1. We demonstrate, by way of a probabilistic programming system (PPS), a methodology which allows for quick iteration over models and Bayesian inferences, which enables us to learn meaningful behavioural segments. We introduce a new Bayesian nonparametric (BNP) state-space model, which extends the hierarchical Dirichlet process (HDP) hidden Markov model (HMM) with an explicit BNP treatment of duration distributions, to deal with different levels of granularity of the latent behavioural space of the lions. The ease with which this is done exemplifies the flexibility that a PPS gives a scientist2. Furthermore, we combine this approach with unsupervised feature learning, using variational autoencoders.},
	author = {Neil Dhir, Frank Wood, Matthijs Vákár, Andrew Markham, Matthew Wijers, Paul Trethowan, Byron Du Preez, Andrew Loveridge, David Macdonald},
	title = {Interpreting lion behaviour with nonparametric probabilistic programs},
	year = {2017}
}

@misc{torr-learning-disentangled-representations-2017,
	abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
	author = {Brooks Paige, Jan-Willem Van De Meent, Alban Desmaison, Noah Goodman, Pushmeet Kohli, Frank Wood, Philip Torr},
	title = {Learning disentangled representations with semi-supervised deep generative models},
	year = {2017}
}

@misc{perov-methods-and-devices-2017,
	abstract = {(54) METHODS AND DEVICES FOR EXECUTING Publication Classification PROGRAM CODE OF A PROBABILISTIC(51) Int. Cl PROGRAMMING LANGUAGE we G06F 9/44(2006.01)(71) Applicant: Oxford University Innovation G06F 7/18(2006.01) Limited, Oxford (GB)(52) US Cl. CPC................ G06F 8/31 (2013.01); G06F 17/18},
	author = {Frank Wood, Timothy Brooks Paige, Vikash Kumar Mansinghka, Jan Willem Van De Meent, Lurii Perov},
	title = {Methods and devices for executing program code of a probabilistic programming language},
	year = {2017}
}

@misc{wood-on-the-opportunities-2017,
	abstract = {We present a formalization of nested Monte Carlo (NMC) estimation, whereby terms in an outer estimator are themselves the output of separate, nested, Monte Carlo (MC) estimators. We demonstrate that NMC can provide consistent estimates of nested expectations, including cases of repeated nesting, under mild conditions; establish corresponding rates of convergence; and provide empirical evidence that suggests these rates are observed in practice. We further establish a number of pitfalls that can arise from naıve nesting of MC estimators and provide guidelines about how they can be avoided. Our results show that whenever an outer estimator depends nonlinearly on an inner estimator, then the number of samples used in both the inner and outer estimators must, in general, be driven to infinity for convergence. We also lay out novel methods for reformulating certain classes of nested expectation problems into a single expectation, leading to improved convergence rates compared with naıve NMC. Finally, we derive a new estimator for use in discrete Bayesian experimental design problems which has a better convergence rate than existing methods.},
	author = {Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank Wood},
	title = {On the opportunities and pitfalls of nesting Monte Carlo estimators},
	year = {2017}
}

@misc{wood-online-learning-rate-2017,
	abstract = {We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this" hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.},
	author = {Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, Frank Wood},
	title = {Online Learning Rate Adaptation with Hypergradient Descent},
	year = {2017}
}

@misc{wood-online-learning-rate-2017,
	abstract = {We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice. We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms. Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself. Computing this "hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.},
	author = {Atilim Gunes Baydin, Robert Cornish, David Martinez Rubio, Mark Schmidt, Frank Wood},
	title = {Online learning rate adaptation with hypergradient descent},
	year = {2017}
}

@misc{wood-updating-the-2017,
	abstract = {We present an updated version of the VESICLE-CNN algorithm presented by Roncal et al. (2014). The original implementation makes use of a patch-based approach. This methodology is known to be slow due to repeated computations. We update this implementation to be fully convolutional through the use of dilated convolutions, recovering the expanded field of view achieved through the use of strided maxpools, but without a degradation of spatial resolution. This updated implementation performs as well as the original implementation, but with a  speedup at test time. We release source code and data into the public domain.},
	author = {Andrew Warrington, Frank Wood},
	title = {Updating the VESICLE-CNN Synapse Detector},
	year = {2017}
}

@misc{wood-using-synthetic-data-2017,
	abstract = {We draw a formal connection between using synthetic training data to optimize neural network parameters and approximate, Bayesian, model-based reasoning. In particular, training a neural network using synthetic data can be viewed as learning a proposal distribution generator for approximate inference in the synthetic-data generative model. We demonstrate this connection in a recognition task where we develop a novel Captcha-breaking architecture and train it using synthetic data, demonstrating both state-of-the-art performance and a way of computing task-specific posterior uncertainty. Using a neural network trained this way, we also demonstrate successful breaking of real-world Captchas currently used by Facebook and Wikipedia. Reasoning from these empirical results and drawing connections with Bayesian modeling, we discuss the robustness of synthetic data results and suggest important …},
	author = {Tuan Anh Le, Atilim Giineş Baydin, Robert Zinkov, Frank Wood},
	title = {Using synthetic data to train neural networks is model-based reasoning},
	year = {2017}
}

@misc{wood-an-interface-for-2016,
	abstract = {We define a family of parameter learning methods for probabilistic program systems (PPSs)[1–10] that perform stochastic gradient ascent using sample-based estimates of the gradient. These algorithms are specified in terms of the interaction between a back end B and a probabilistic program P, which is treated as a black box computation. The result is a language-agnostic infterface for learning of probabilistic programs. The described methods can be used to perform both variational Bayes (VB)[11–13], which approximates a posterior distribution, and empirical Bayes (EB)[14, 15], which maximizes the marginal likelihood with respect to the prior hyperparameters. Moreover, EB estimation is equivalent to (upper-level) policy search [15, 16] in programs where the exponent of the reward takes the place of the likelihood [17, 18].In this abstract we are interested in algorithms that combine inference with learning. As a motivating example we consider a program (see Figure 1), written in the language Anglican [7], which simulates the Canadian traveler problem (CTP) domain. In the CTP, an agent must travel along a graph, which represents a network of roads, to get from the start node (green) to the target node (red). Due to bad weather some roads are blocked, but the agent does not know which in advance. The agent performs depth-first search along the graph, which will require a varying number of steps, depending on which edges are closed, and incurs a cost for the traveled distance. The program in Figure 1 defines two types of policies for the CTP. For the policy where edges are chosen at random, we may perform online planning by simulating …},
	author = {Jan-Willem van de Meent, Brooks Paige, David Tolpin, Frank Wood},
	title = {An Interface for Black Box Learning in Probabilistic Programs},
	year = {2016}
}

@misc{wood-automatic-sampler-discovery-2016,
	abstract = {We describe an approach to automatic discovery of samplers in the form of human interpretable probabilistic programs. Specifically, we learn the procedure code of samplers for one-dimensional distributions. We formulate a Bayesian approach to this problem by specifying an adaptor grammar prior over probabilistic program code, and use approximate Bayesian computation to learn a program whose execution generates samples that match observed data or analytical characteristics of a distribution of interest. In our experiments we leverage the probabilistic programming system Anglican to perform Markov chain Monte Carlo sampling over the space of programs. Our results are competive relative to state-of-the-art genetic programming methods and demonstrate that we can learn approximate and even exact samplers.},
	author = {Yura Perov, Frank Wood},
	title = {Automatic sampler discovery via probabilistic programming and approximate bayesian computation},
	year = {2016}
}

@misc{wood-bayesian-optimization-for-2016,
	abstract = {We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.},
	author = {Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A Osborne, Frank Wood},
	title = {Bayesian optimization for probabilistic programs},
	year = {2016}
}

@misc{wood-policy-search-2016,
	abstract = {In this work we show how to represent policies as programs: that is, as stochastic simulators with tunable parameters. To learn the parameters of such policies we develop connections between black box variational inference and existing policy search approaches. We then explain how such learning can be implemented in a probabilistic programming system. Using our own novel implementation of such a system we demonstrate both conciseness of policy representation and automatic policy parameter learning for a set of canonical reinforcement learning problems.},
	author = {Jan-Willem Vandemeent, Brooks Paige, David Tolpin, Frank Wood},
	title = {Black-box policy search with probabilistic programs},
	year = {2016}
}

@misc{wood-design-and-implementation-2016,
	abstract = {Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We introduce the programming language Anglican, outline our design choices, and discuss in depth the implementation of the Anglican language and runtime, including macro-based compilation, extended CPS-based evaluation model, and functional representations for probabilistic paradigms, such as a distribution, a random process, and an inference algorithm.We show that a probabilistic functional language can be implemented efficiently and integrated tightly with a conventional functional language with only moderate computational overhead. We also demonstrate how advanced probabilistic modelling concepts are mapped naturally to the functional foundation.},
	author = {David Tolpin, Jan-Willem van de Meent, Hongseok Yang, Frank Wood},
	title = {Design and implementation of probabilistic programming language anglican},
	year = {2016}
}

@misc{torr-inducing-interpretable-representations-2016,
	abstract = {We develop a framework for incorporating structured graphical models in the \emph{encoders} of variational autoencoders (VAEs) that allows us to induce interpretable representations through approximate variational inference. This allows us to both perform reasoning (e.g. classification) under the structural constraints of a given graphical model, and use deep generative models to deal with messy, high-dimensional domains where it is often difficult to model all the variation. Learning in this framework is carried out end-to-end with a variational objective, applying to both unsupervised and semi-supervised schemes.},
	author = {N Siddharth, Brooks Paige, Alban Desmaison, Jan-Willem Van de Meent, Frank Wood, Noah D Goodman, Pushmeet Kohli, Philip HS Torr},
	title = {Inducing interpretable representations with variational autoencoders},
	year = {2016}
}

@misc{wood-inference-networks-for-2016,
	abstract = {We introduce a new approach for amortizing inference in directed graphical models by learning heuristic approximations to stochastic inverses, designed specifically for use as proposal distributions in sequential Monte Carlo methods. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation to the distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output of these networks can be used as automatically-learned high-quality proposal distributions to accelerate sequential Monte Carlo across a diverse range of problem settings.},
	author = {Brooks Paige, Frank Wood},
	title = {Inference networks for sequential Monte Carlo in graphical models},
	year = {2016}
}

@misc{meent-interacting-particle-markov-2016,
	abstract = {We introduce interacting particle Markov chain Monte Carl (iPMCMC), a PMCMC method that introduces a coupling between multiple standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non-interacting PMCMC samplers and a single PMCMC sampler with an equivalent total computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures.},
	author = {A Doucet, T Rainforth, C Naesseth, F Lindsten, Brooks Paige, F Wood, J van de Meent},
	title = {Interacting Particle Markov Chain Monte Carlo},
	year = {2016}
}

@misc{wood-interacting-particle-markov-2016,
	abstract = {We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC method based on an interacting pool of standard and conditional sequential Monte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte Carlo sampler on an extended space. We present empirical results that show significant improvements in mixing rates relative to both non-interacting PMCMC samplers and a single PMCMC sampler with an equivalent memory and computational budget. An additional advantage of the iPMCMC method is that it is suitable for distributed and multi-core architectures.},
	author = {Tom Rainforth, Christian Naesseth, Fredrik Lindsten, Brooks Paige, Jan-Willem Vandemeent, Arnaud Doucet, Frank Wood},
	title = {Interacting particle markov chain monte carlo},
	year = {2016}
}

@misc{wood-nested-compiled-inference-2016,
	abstract = {Probabilistic programming languages (PPLs) allow the representation of probability distributions as computer programs by means of stochastic language primitives and conditioning of variable values on observations [8, 7]. Probabilistic programs are generative models as they generate samples from a joint distribution p (x, y), where x are latent variables and y are output (or observed) data. Inference in PPLs amounts to computing the posterior p (x| y) conditioned on observed data. PPLs allow decoupling model specification from inference, which can be handled by techniques including Markov chain Monte Carlo [18], variational methods [19], expectation propagation [16], and sequential Monte Carlo [20]. Inference is often computationally expensive. Amortized inference [5] strategies are devised to store and reuse past inferences so that future inferences run faster.},
	author = {Tuan Anh Le, Atılım Günes Baydin, Frank Wood},
	title = {Nested compiled inference for hierarchical reinforcement learning},
	year = {2016}
}

@misc{wood-nonparametric-bayesian-models-2016,
	abstract = {Human locomotion and activity recognition systems form a critical part in a robot's ability to safely and effectively operate in a environment populated with human end users. Previous work in this area relies upon strong assumptions about the labels in the training data; e.g. that are noise-free and that they exist at all. Our approach does not predefine the relevant behaviours or their number, as both are learned directly from observations, similar to real-world human-robot interactions, where labels are neither available. Instead we introduce models that make no assumptions about the state space, by presenting a fully unsupervised nonparametric Bayesian recognition approach, in which we leverage recent advances in state space modelling with automatic inference using probabilistic programming. We demonstrate the utility of full model optimisation using Bayesian optimisation and validate our approach on several …},
	author = {Neil Dhir, Yura Perov, Frank Wood},
	title = {Nonparametric Bayesian models for unsupervised activity recognition and tracking},
	year = {2016}
}

@misc{wood-on-the-pitfalls-2016,
	abstract = {There is an increasing interest in estimating expectations outside of the classical inference framework, such as for models expressed as probabilistic programs. Many of these contexts call for some form of nested inference to be applied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC) schemes, for which classical convergence proofs are insufficient. We give conditions under which NMC will converge, establish a rate of convergence, and provide empirical data that suggests that this rate is observable in practice. Finally, we prove that general-purpose nested inference schemes are inherently biased. Our results serve to warn of the dangers associated with naive composition of inference and models.},
	author = {Tom Rainforth, Robert Cornish, Hongseok Yang, Frank Wood},
	title = {On the pitfalls of nested Monte Carlo},
	year = {2016}
}

@misc{wood-probabilistic-structure-discovery-2016,
	abstract = {Existing methods for structure discovery in time series data construct interpretable, compositional kernels for Gaussian process regression models. While the learned Gaussian process model provides posterior mean and variance estimates, typically the structure is learned via a greedy optimization procedure. This restricts the space of possible solutions and leads to over-confident uncertainty estimates. We introduce a fully Bayesian approach, inferring a full posterior over structures, which more reliably captures the uncertainty of the model.},
	author = {David Janz, Brooks Paige, Tom Rainforth, Jan-Willem van de Meent, Frank Wood},
	title = {Probabilistic structure discovery in time series data},
	year = {2016}
}

@misc{kammar-semantics-for-probabilistic-2016,
	abstract = {We study the semantic foundation of expressive probabilistic programming languages, that support higher-order functions, continuous distributions, and soft constraints (such as Anglican, Church, and Venture). We define a metalanguage (an idealised version of Anglican) for probabilistic computation with the above features, develop both operational and denotational semantics, and prove soundness, adequacy, and termination. This involves measure theory, stochastic labelled transition systems, and functor categories, but admits intuitive computational readings, one of which views sampled random variables as dynamically allocated read-only variables. We apply our semantics to validate nontrivial equations underlying the correctness of certain compiler optimisations and inference algorithms such as sequential Monte Carlo simulation. The language enables defining probability distributions on higher-order …},
	author = {Sam Staton, Hongseok Yang, Frank Wood, Chris Heunen, Ohad Kammar},
	title = {Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints},
	year = {2016}
}

@misc{yang-spreadsheet-probabilistic-programming-2016,
	abstract = {Spreadsheet workbook contents are simple programs. Because of this, probabilistic programming techniques can be used to perform Bayesian inversion of spreadsheet computations. What is more, existing execution engines in spreadsheet applications such as Microsoft Excel can be made to do this using only built-in functionality. We demonstrate this by developing a native Excel implementation of both a particle Markov Chain Monte Carlo variant and black-box variational inference for spreadsheet probabilistic programming. The resulting engine performs probabilistically coherent inference over spreadsheet computations, notably including spreadsheets that include user-defined black-box functions. Spreadsheet engines that choose to integrate the functionality we describe in this paper will give their users the ability to both easily develop probabilistic models and maintain them over time by including actuals via a simple user-interface mechanism. For spreadsheet end-users this would mean having access to efficient and probabilistically coherent probabilistic modeling and inference for use in all kinds of decision making under uncertainty.},
	author = {Mike Wu, Yura Perov, Frank Wood, Hongseok Yang},
	title = {Spreadsheet probabilistic programming},
	year = {2016}
}

@misc{wood-with-a-2016,
	abstract = {We introduce an alternative to reservoir sampling, a classic and popular algorithm for drawing a fixed-size subsample from streaming data in a single pass. Rather than draw a random sample, our approach performs an online optimization which aims to select the subset that provides the best overall approximation to the full data set, as judged using a kernel two-sample test. This produces subsets which minimize the worst-case relative error when computing expectations of functions in a specified function class, using just the samples from the subset. Kernel functions are approximated using random Fourier features, and the subset of samples itself is stored in a random projection tree. The resulting algorithm runs in a single pass through the whole data set, and has a per-iteration computational complexity logarithmic in the size of the subset. These “supersamples” subsampled from the full data provide a concise summary, as demonstrated empirically on mixture models and the MNIST dataset.},
	author = {Brooks Paige, Dino Sejdinovic, Frank D Wood},
	title = {Super-sampling with a reservoir.},
	year = {2016}
}

@misc{wood-adaptive-scheduling-in-2015,
	abstract = {We introduce an adaptive output-sensitive inference algorithm for MCMC and probabilistic programming, Adaptive Random Database. The algorithm is based on a single-site updating Metropolis-Hasting sampler, the Random Database (RDB) algorithm. Adaptive RDB (ARDB) differs from the original RDB in that the schedule of selecting variables proposed for modification is adapted based on the output of of the probabilistic program, rather than being fixed and uniform. We show that ARDB still converges to the correct distribution. We compare ARDB to RDB on several test problems highlighting different aspects of the adaptation scheme.},
	author = {David Tolpin, Jan Willem van de Meent, Brooks Paige, Frank Wood},
	title = {Adaptive Scheduling in MCMC and Probabilistic Programming.},
	year = {2015}
}

@misc{wood-canonical-correlation-forests-2015,
	abstract = {We introduce canonical correlation forests (CCFs), a new decision tree ensemble method for classification and regression. Individual canonical correlation trees are binary decision trees with hyperplane splits based on local canonical correlation coefficients calculated during training. Unlike axis-aligned alternatives, the decision surfaces of CCFs are not restricted to the coordinate system of the inputs features and therefore more naturally represent data with correlated inputs. CCFs naturally accommodate multiple outputs, provide a similar computational complexity to random forests, and inherit their impressive robustness to the choice of input parameters. As part of the CCF training algorithm, we also introduce projection bootstrapping, a novel alternative to bagging for oblique decision tree ensembles which maintains use of the full dataset in selecting split points, often leading to improvements in predictive accuracy. Our experiments show that, even without parameter tuning, CCFs out-perform axis-aligned random forests and other state-of-the-art tree ensemble methods on both classification and regression problems, delivering both improved predictive accuracy and faster training times. We further show that they outperform all of the 179 classifiers considered in a recent extensive survey.},
	author = {Tom Rainforth, Frank Wood},
	title = {Canonical correlation forests},
	year = {2015}
}

@misc{wood-sequential-monte-2015,
	abstract = {Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC) algorithms in existing probabilistic programming systems suboptimally use only model priors as proposal distributions. In this work, we describe an approach for training a discriminative model, namely a neural network, in order to approximate the optimal proposal by using posterior estimates from previous runs of inference. We show an example that incorporates a data-driven proposal for use in a non-parametric model in the Anglican probabilistic programming system. Our results show that data-driven proposals can significantly improve inference performance so that considerably fewer particles are necessary to perform a good posterior estimation.},
	author = {Yura N Perov, Tuan Anh Le, Frank Wood},
	title = {Data-driven sequential Monte Carlo in probabilistic programming},
	year = {2015}
}

@misc{wood-maximum-a-posteriori-2015,
	abstract = {We introduce an approximate search algorithm for fast maximum a posteriori probability estimation in probabilistic programs, which we call Bayesian ascent Monte Carlo (BaMC). Probabilistic programs represent probabilistic models with varying number of mutually dependent finite, countable, and continuous random variables. BaMC is an anytime MAP search algorithm applicable to any combination of random variables and dependencies. We compare BaMC to other MAP estimation algorithms and show that BaMC is faster and more robust on a range of probabilistic models.},
	author = {David Tolpin, Frank Wood},
	title = {Maximum a posteriori estimation by search in probabilistic programs},
	year = {2015}
}

@misc{wood-adaptive-2015,
	abstract = {We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for probabilistic models expressed as programs, Adaptive Lightweight Metropolis-Hastings (AdLMH). This algorithm extends Lightweight Metropolis-Hastings (LMH) by adjusting the probabilities of proposing random variables for modification to improve convergence of the program output. We show that AdLMH converges to the correct equilibrium distribution and compare convergence of AdLMH to that of LMH on several test problems to highlight different aspects of the adaptation scheme. We observe consistent improvement in convergence on the test problems.},
	author = {David Tolpin, Jan-Willem van de Meent, Brooks Paige, Frank Wood},
	title = {Output-sensitive adaptive metropolis-hastings for probabilistic programs},
	year = {2015}
}

@misc{wood-particle-gibbs-with-2015,
	abstract = {Particle Markov chain Monte Carlo techniques rank among current state-of-the-art methods for probabilistic program inference. A drawback of these techniques is that they rely on importance resampling, which results in degenerate particle trajectories and a low effective sample size for variables sampled early in a program. We here develop a formalism to adapt ancestor resampling, a technique that mitigates particle degeneracy, to the probabilistic programming setting. We present empirical results that demonstrate nontrivial performance gains.},
	author = {Jan-Willem Meent, Hongseok Yang, Vikash Mansinghka, Frank Wood},
	title = {Particle Gibbs with ancestor sampling for probabilistic programs},
	year = {2015}
}

@misc{wood-path-finding-under-2015,
	abstract = {We introduce a new approach to solving path-finding problems under uncertainty by representing them as probabilistic models and applying domain-independent inference algorithms to the models. This approach separates problem representation from the inference algorithm and provides a framework for efficient learning of path-finding policies. We evaluate the new approach on the Canadian Traveler Problem, which we formulate as a probabilistic model, and show how probabilistic inference allows high performance stochastic policies to be obtained for this problem.},
	author = {David Tolpin, Brooks Paige, Jan Willem van de Meent, Frank Wood},
	title = {Path Finding under Uncertainty through Probabilistic Inference},
	year = {2015}
}

@misc{wood-probabilistic-programming-in-2015,
	abstract = {Anglican is a probabilistic programming system designed to interoperate with Clojure and other JVM languages. We describe the implementation of Anglican and illustrate how its design facilitates both explorative and industrial use of probabilistic programming.},
	author = {David Tolpin, Jan-Willem van de Meent, Frank Wood},
	title = {Probabilistic programming in Anglican},
	year = {2015}
}

@misc{wood-a-compilation-target-2014,
	abstract = {Forward inference techniques such as sequential Monte Carlo and particle Markov chain Monte Carlo for probabilistic programming can be implemented in any programming language by creative use of standardized operating system functionality including processes, forking, mutexes, and shared memory. Exploiting this we have defined, developed, and tested a probabilistic programming language intermediate representation language we call probabilistic C, which itself can be compiled to machine code by standard compilers and linked to operating system libraries yielding an efficient, scalable, portable probabilistic programming compilation target. This opens up a new hardware and systems research path for optimizing probabilistic programming systems.},
	author = {Brooks Paige, Frank Wood},
	title = {A compilation target for probabilistic programming languages},
	year = {2014}
}

@misc{mansinghka-a-new-approach-2014,
	abstract = {We introduce and demonstrate a new approach to inference in expressive probabilistic programming languages based on particle Markov chain Monte Carlo. Our approach is easy to implement and to parallelize, applies to Turing-complete probabilistic programming languages, and supports accurate inference in models that make use of complex control flow, including stochastic recursion, as well as primitives from nonparametric Bayesian statistics. Our experiments show that this approach can be more efficient than previously introduced single-site Metropolis-Hastings samplers.},
	author = {Frank Wood, Jan Willem Meent, Vikash Mansinghka},
	title = {A new approach to probabilistic programming inference},
	year = {2014}
}

@misc{teh-asynchronous-anytime-sequential-2014,
	abstract = {We introduce a new sequential Monte Carlo algorithm we call the particle cascade. The particle cascade is an asynchronous, anytime alternative to traditional sequential Monte Carlo algorithms that is amenable to parallel and distributed implementations. It uses no barrier synchronizations which leads to improved particle throughput and memory efficiency. It is an anytime algorithm in the sense that it can be run forever to emit an unbounded number of particles while keeping within a fixed memory budget. We prove that the particle cascade provides an unbiased marginal likelihood estimator which can be straightforwardly plugged into existing pseudo-marginal methods.},
	author = {Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh},
	title = {Asynchronous anytime sequential monte carlo},
	year = {2014}
}

@misc{elhadad-diagnosis-code-2014,
	abstract = { Background and objective The volume of healthcare data is growing rapidly with the adoption of health information technology. We focus on automated ICD9 code assignment from discharge summary content and methods for evaluating such assignments. Methods We study ICD9 diagnosis codes and discharge summaries from the publicly available Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC II) repository. We experiment with two coding approaches: one that treats each ICD9 code independently of each other (flat classifier), and one that leverages the hierarchical nature of ICD9 codes into its modeling (hierarchy-based classifier). We propose novel evaluation metrics, which reflect the distances among gold-standard and predicted codes and their locations in the ICD9 tree. Experimental setup, code for modeling, and evaluation scripts are made available to the research …},
	author = {Adler Perotte, Rimma Pivovarov, Karthik Natarajan, Nicole Weiskopf, Frank Wood, Noémie Elhadad},
	title = {Diagnosis code assignment: models and evaluation metrics},
	year = {2014}
}

@misc{wood-improved-activity-recognition-2014,
	abstract = {Improving activity recognition, with special focus on fall-detection, is the subject of this study. We show that Kalman smoothed in-painting of missing pose information and task-specific dimensionality reduction of activity feature vectors leads to significantly improved activity classification performance. We illustrate our findings by applying common classification algorithms to dimensionally reduced feature vectors, and compare our accuracy to previous work. In part two we investigate our methods on a small subset of the data, in order to ascertain what accuracy performance is achievable with the smallest amount of information available.},
	author = {Neil Dhir, Frank Wood},
	title = {Improved activity recognition via Kalman smoothing and multiclass linear discriminant analysis},
	year = {2014}
}

@misc{wood-infinite-structured-hidden-2014,
	abstract = {This paper reviews recent advances in Bayesian nonparametric techniques for constructing and performing inference in infinite hidden Markov models. We focus on variants of Bayesian nonparametric hidden Markov models that enhance a posteriori state-persistence in particular. This paper also introduces a new Bayesian nonparametric framework for generating left-to-right and other structured, explicit-duration infinite hidden Markov models that we call the infinite structured hidden semi-Markov model.},
	author = {Jonathan H Huggins, Frank Wood},
	title = {Infinite structured hidden semi-Markov models},
	year = {2014}
}

@misc{wood-learning-probabilistic-programs-2014,
	abstract = {We develop a technique for generalising from data in which models are samplers represented as program text. We establish encouraging empirical results that suggest that Markov chain Monte Carlo probabilistic programming inference techniques coupled with higher-order probabilistic programming languages are now sufficiently powerful to enable successful inference of this kind in nontrivial domains. We also introduce a new notion of probabilistic program compilation and show how the same machinery might be used in the future to compile probabilistic programs for efficient reusable predictive inference.},
	author = {Yura N Perov, Frank D Wood},
	title = {Learning probabilistic programs},
	year = {2014}
}

@misc{perotte-mixed-membership-classification-2014,
	abstract = {Documents frequently come with additional information like labels or popularity ratings. Contemporary examples include the product ratings that accompany product descriptions, the number of “likes” that webpages have attracted, grades associated with assigned essays, and so forth. This chapter covers one way to jointly model documents and the labels applied to them. In particular we focus on our own work on modeling documents with more complicated labels that themselves possess some kind of structural organization. Consider typical product catalogs. They usually contain text descriptions of products that have been organized into hierarchical product directories. The situation of a product into such a hierarchy (the path or paths in the product hierarchy that lead to it) can be thought of as a structured label. Jointly modeling the document and such a label is useful for automatically labeling new documents …},
	author = {Frank D Wood, Adler J Perotte},
	title = {Mixed Membership Classification for Documents with Hierarchically Structured Labels.},
	year = {2014}
}

@misc{wood-tempering-by-subsampling-2014,
	abstract = {In this paper we demonstrate that tempering Markov chain Monte Carlo samplers for Bayesian models by recursively subsampling observations without replacement can improve the performance of baseline samplers in terms of effective sample size per computation. We present two tempering by subsampling algorithms, subsampled parallel tempering and subsampled tempered transitions. We provide an asymptotic analysis of the computational cost of tempering by subsampling, verify that tempering by subsampling costs less than traditional tempering, and demonstrate both algorithms on Bayesian approaches to learning the mean of a high dimensional multivariate Normal and estimating Gaussian process hyperparameters.},
	author = {Jan-Willem van de Meent, Brooks Paige, Frank Wood},
	title = {Tempering by subsampling},
	year = {2014}
}

@misc{xing-the-dependent-dirichlet-2014,
	abstract = {This paper explores how to find, track, and learn models of arbitrary objects in a video without a predefined method for object detection. We present a model that localizes objects via unsupervised tracking while learning a representation of each object, avoiding the need for pre-built detectors. Our model uses a dependent Dirichlet process mixture to capture the uncertainty in the number and appearance of objects and requires only spatial and color video data that can be efficiently extracted via frame differencing. We give two inference algorithms for use in both online and offline settings, and use them to perform accurate detection-free tracking on multiple real videos. We demonstrate our method in difficult detection scenarios involving occlusions and appearance shifts, on videos containing a large number of objects, and on a recent human-tracking benchmark where we show performance comparable to state of the art detector-based methods.},
	author = {Willie Neiswanger, Frank Wood, Eric Xing},
	title = {The dependent Dirichlet process mixture of objects for detection-free tracking and object modeling},
	year = {2014}
}

@misc{wood-a-joint-learning-2013,
	abstract = {We present a cognitive model of early lexical acquisition which jointly performs word segmentation and learns an explicit model of phonetic variation. We define the model as a Bayesian noisy channel; we sample segmentations and word forms simultaneously from the posterior, using beam sampling to control the size of the search space. Compared to a pipelined approach in which segmentation is performed first, our model is qualitatively more similar to human learners. On data with variable pronunciations, the pipelined approach learns to treat syllables or morphemes as words. In contrast, our joint model, like infant learners, tends to learn multiword collocations. We also conduct analyses of the phonetic variations that the model learns to accept and its patterns of word recognition errors, and relate these to developmental evidence.},
	author = {Micha Elsner, Sharon Goldwater, Naomi Feldman, Frank Wood},
	title = {A joint learning model of word segmentation, lexical acquisition, and phonetic variability},
	year = {2013}
}

@misc{roy-bayesian-nonparametric-methods-2013,
	abstract = {Making intelligent decisions from incomplete information is critical in many applications: for example, robots must choose actions based on imperfect sensors, and speech-based interfaces must infer a user’s needs from noisy microphone inputs. What makes these tasks hard is that often we do not have a natural representation with which to model the domain and use for choosing actions; we must learn about the domain’s properties while simultaneously performing the task. Learning a representation also involves trade-offs between modeling the data that we have seen previously and being able to make predictions about new data. This article explores learning representations of stochastic systems using Bayesian nonparametric statistics. Bayesian nonparametric methods allow the sophistication of a representation to scale gracefully with the complexity in the data. Our main contribution is a careful empirical …},
	author = {Finale Doshi-Velez, David Pfau, Frank Wood, Nicholas Roy},
	title = {Bayesian nonparametric methods for partially-observable reinforcement learning},
	year = {2013}
}

@misc{wiggins-hidden-markov-2013,
	abstract = {We address the problem of analyzing sets of noisy time-varying signals that all report on the same process but confound straightforward analyses due to complex inter-signal heterogeneities and measurement artifacts. In particular we consider single-molecule experiments which indirectly measure the distinct steps in a biomolecular process via observations of noisy time-dependent signals such as a fluorescence intensity or bead position. Straightforward hidden Markov model (HMM) analyses attempt to characterize such processes in terms of a set of conformational states, the transitions that can occur between these states, and the associated rates at which those transitions occur; but require ad-hoc post-processing steps to combine multiple signals. Here we develop a hierarchically coupled HMM that allows experimentalists to deal with inter-signal variability in a principled and automatic way. Our approach is a generalized expectation maximization hyperparameter point estimation procedure with variational Bayes at the level of individual time series that learns an single interpretable representation of the overall data generating process.},
	author = {Jan-Willem Meent, Jonathan Bronson, Frank Wood, Ruben Gonzalez Jr, Chris Wiggins},
	title = {Hierarchically-coupled hidden Markov models for learning kinetic rates from single-molecule data},
	year = {2013}
}

@misc{wood-inferring-team-strengths-2013,
	abstract = {We propose an original model for inferring team strengths using a Markov Random Field, which can be used to generate historical estimates of the offensive and defensive strengths of a team over time. This model was designed to be applied to sports such as soccer or hockey, in which contest outcomes take value in a limited discrete space. We perform inference using a combination of Expectation Maximization and Loopy Belief Propagation. The challenges of working with a non-convex optimization problem and a high-dimensional parameter space are discussed. The performance of the model is demonstrated on professional soccer data from the English Premier League.},
	author = {John Zech, Frank Wood},
	title = {Inferring Team Strengths Using a Discrete Markov Random Field},
	year = {2013}
}

@misc{ghahramani-a-bayesian-2012,
	abstract = {We present a non-parametric Bayesian approach to structure learning with hidden causes. Previous Bayesian treatments of this problem define a prior over the number of hidden causes and use algorithms such as reversible jump Markov chain Monte Carlo to move between solutions. In contrast, we assume that the number of hidden causes is unbounded, but only a finite number influence observable variables. This makes it possible to use a Gibbs sampler to approximate the distribution over causal structures. We evaluate the performance of both approaches in discovering hidden causes in simulated data, and use our non-parametric approach to discover hidden causes in a real medical dataset.},
	author = {Frank Wood, Thomas Griffiths, Zoubin Ghahramani},
	title = {A non-parametric Bayesian method for inferring hidden causes},
	year = {2012}
}

@misc{wood-inference-in-hidden-2012,
	abstract = {In this letter, we borrow from the inference techniques developed for unbounded state-cardinality (nonparametric) variants of the HMM and use them to develop a tuning-parameter free, black-box inference procedure for explicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have latent states consisting of both discrete state-indicator and discrete state-duration random variables. In contrast to the implicit geometric state duration distribution possessed by the standard HMM, EDHMMs allow the direct parameterization and estimation of per-state duration distributions. As most duration distributions are defined over the positive integers, truncation or other approximations are usually required to perform EDHMM inference.},
	author = {Michael Dewar, Chris Wiggins, Frank Wood},
	title = {Inference in hidden Markov models with explicit state duration distributions},
	year = {2012}
}

@misc{paninski-low-rank-2012,
	abstract = {Constructing tractable dependent probability distributions over structured continuous random vectors is a central problem in statistics and machine learning. It has proven difficult to find general constructions for models in which efficient exact inference is possible, outside of the classical cases of models with restricted graph structure (chain, tree, etc.) and linear-Gaussian or discrete potentials. In this work we identify a tree graphical model class in which exact inference can be performed efficiently, owing to a certain “low-rank” structure in the potentials. We explore this new class of models by applying the resulting inference methods to neural spike rate estimation and motion-capture joint-angle smoothing tasks.},
	author = {Carl Smith, Frank Wood, Liam Paninski},
	title = {Low rank continuous-space graphical models},
	year = {2012}
}

@misc{wood-unsupervised-detection-and-2012,
	abstract = {This paper proposes a technique for the unsupervised detection and tracking of arbitrary objects in videos. It is intended to reduce the need for detection and localization methods tailored to specific object types and serve as a general framework applicable to videos with varied objects, backgrounds, and image qualities. The technique uses a dependent Dirichlet process mixture (DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data that can be easily and efficiently extracted from the regions in a video that represent objects. This paper describes a specific implementation of the model using spatial and color pixel data extracted via frame differencing and gives two algorithms for performing inference in the model to accomplish detection and tracking. This technique is demonstrated on multiple synthetic and benchmark video datasets that illustrate its ability to, without modification, detect and track objects with diverse physical characteristics moving over non-uniform backgrounds and through occlusion.},
	author = {Willie Neiswanger, Frank Wood},
	title = {Unsupervised detection and tracking of arbitrary objects with dependent dirichlet process mixtures},
	year = {2012}
}

@misc{wood-deplump-for-streaming-2011,
	abstract = {We present a general-purpose, loss less compressor for streaming data. This compressor is based on the deplump probabilistic compressor for batch data. Approximations to the inference procedure used in the probabilistic model underpinning deplump are introduced that yield the computational asyptotics necessary for stream compression. We demonstrate the performance of this streaming deplump variant relative to the batch compressor on a benchmark corpus and find that it performs equivalently well despite these approximations. We also explore the performance of the streaming variant on corpora that are too large to be compressed by batch deplump and demonstrate excellent compression performance.},
	author = {Nicholas Bartlett, Frank Wood},
	title = {Deplump for streaming data},
	year = {2011}
}

@misc{wood-discussion-of-2011,
	abstract = {Mixed-membership models (eg “topic models”) are inarguably popular; especially latent Dirichlet allocation (LDA)[Blei et al., 2003] and its variants. Such models have become a fundamental tool in the analysis and exploration of many types of data. Originally designed to model text documents as per-word draws from a document-specific weighting of a finite collection of “topics”(distributions over words), mixedmembership models now are applied very broadly. Example usage includes applications in information retrieval, image processing, audio classification, and more. Because of the wide applicability of mixedmembership modeling, improving models of this type has the potential to have significant impact. The discrete infinite logistic normal distribution (DILN) for mixed-membership modeling is a significant advance in mixed-membership modeling.},
	author = {Frank Wood},
	title = {Discussion of “The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling”},
	year = {2011}
}

@misc{bartlett-hierarchically-supervised-latent-2011,
	abstract = {We introduce hierarchically supervised latent Dirichlet allocation (HSLDA), a model for hierarchically and multiply labeled bag-of-word data. Examples of such data include web pages and their placement in directories, product descriptions and associated categories from product hierarchies, and free-text clinical records and their assigned diagnosis codes. Out-of-sample label prediction is the primary goal of this work, but improved lower-dimensional representations of the bag-of-word data are also of interest. We demonstrate HSLDA on large-scale data from clinical document labeling and retail product categorization tasks. We show that leveraging the structure from hierarchical labels improves out-of-sample label prediction substantially when compared to models that do not.},
	author = {Adler Perotte, Frank Wood, Noemie Elhadad, Nicholas Bartlett},
	title = {Hierarchically supervised latent Dirichlet allocation},
	year = {2011}
}

@misc{wood-modeling-streaming-data-2011,
	abstract = {We interpret results from a study where data was modeled using constant space approximations to the sequence memoizer. The sequence memoizer (SM) is a non-constant-space, Bayesian nonparametric model in which the data are the sufficient statistic in the streaming setting. We review approximations to the probabilistic model underpinning the SM that yield the computational asymptotic complexities necessary for modeling very large (streaming) datasets with fixed computational resource. Results from modeling a benchmark corpus are shown for both the effectively parametric, approximate models and the fully nonparametric SM. We find that the approximations perform nearly as well in terms of predictive likelihood. We argue from this single example that, due to the lack of sufficiency, Bayesian nonparametric models may, in general, not be suitable as models of streaming data, and propose that nonstationary parametric models and estimators for the same inspired by Bayesian nonparametric models may be worth investigating more fully.},
	author = {Frank Wood},
	title = {Modeling streaming data in the absence of sufficiency},
	year = {2011}
}

@misc{teh-the-sequence-memoizer-2011,
	abstract = {Probabilistic models of sequences play a central role in most machine translation, automated speech recognition, lossless compression, spell-checking, and gene identification applications to name but a few. Unfortunately, real-world sequence data often exhibit long range dependencies which can only be captured by computationally challenging, complex models. Sequence data arising from natural processes also often exhibits power-law properties, yet common sequence models do not capture such properties. The sequence memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive. Its utility as a language model and general purpose lossless compressor is demonstrated.},
	author = {Frank Wood, Jan Gasthaus, Cédric Archambeau, Lancelot James, Yee Whye Teh},
	title = {The sequence memoizer},
	year = {2011}
}

@misc{teh-the-stochastic-memoizer-2011,
	abstract = {Probabilistic models of sequences play a central role in most machine translation, automated speech recognition, lossless compression, spell-checking, and gene identification applications to name but a few. Unfortunately, real-world sequence data often exhibit long range dependencies which can only be captured by computationally challenging, complex models. Sequence data arising from natural processes also often exhibits power-law properties, yet common sequence models do not capture such properties. The sequence memoizer is a new hierarchical Bayesian model for discrete sequence data that captures long range dependencies and power-law characteristics, while remaining computationally attractive. Its utility as a language model and general purpose lossless compressor is demonstrated.},
	author = {Frank Wood, Jan Gasthaus, Cédric Archambeau, Lancelot James, Yee Whye Teh},
	title = {The stochastic memoizer},
	year = {2011}
}

@misc{wood-applied-regression-2010,
	abstract = {General linear test can be used to determine whether or not a predictor variable (or sets of variables) should be included in the model},
	author = {Frank Wood},
	title = {Applied Regression},
	year = {2010}
}

@misc{wood-forgetting-constant-2010,
	abstract = {We propose a novel dependent hierarchical Pitman-Yor process model for discrete data. An incremental Monte Carlo inference procedure for this model is developed. We show that inference in this model can be performed in constant space and linear time. The model is demonstrated in a discrete sequence prediction task where it is shown to achieve state of the art sequence prediction performance while using significantly less memory.},
	author = {Nicholas Bartlett, David Pfau, Frank Wood},
	title = {Forgetting counts: Constant memory inference for a dependent hierarchical Pitman-Yor process},
	year = {2010}
}

@misc{wood-introduction-to-bayesian-2010,
	abstract = {A primary motivation for believing Bayesian thinking important is that it facilitates a common-sense interpretation of statistical conclusions. For instance, a Bayesian (probability) interval for an unknown quantity of interest can be directly regarded as having a high probability of containing the unknown quantity, in contrast to a frequentist (confidence) interval, which may strictly be interpreted only in relation to a sequence of similar inferences that might be made in repeated practice.},
	author = {Frank Wood},
	title = {Introduction to Bayesian Inference},
	year = {2010}
}

@misc{teh-lossless-compression-based-2010,
	abstract = {In this work we describe a sequence compression method based on combining a Bayesian nonparametric sequence model with entropy encoding. The model, a hierarchy of Pitman-Yor processes of unbounded depth previously proposed by Wood et al. [16] in the context of language modelling, allows modelling of long-range dependencies by allowing conditioning contexts of unbounded length. We show that incremental approximate inference can be performed in this model, thereby allowing it to be used in a text compression setting. The resulting compressor reliably outperforms several PPM variants on many types of data, but is particularly effective in compressing data that exhibits power law properties.},
	author = {Jan Gasthaus, Frank Wood, Yee Whye Teh},
	title = {Lossless compression based on the Sequence Memoizer},
	year = {2010}
}

@misc{wood-matrix-approach-to-2010,
	abstract = {Matrix Approach to Linear Regresssion Page 1 Matrix Approach to Linear Regresssion Frank 
Wood November 3, 2010 Page 2 Random Vectors and Matrices Let’s say we have a vector 
consisting of three random variables y =   Y1 Y2 Y3   The expectation of a random vector 
is defined as E(y) =   E(Y1) E(Y2) E(Y3)   Page 3 Expectation of a Random Matrix The 
expectation of a random matrix is defined similarly E(y)=[E(Yij )] i = 1, ...n;j = 1, ...,p Page 4 
Covariance Matrix of a Random Vector The correlation of variances and covariances of and 
between the elements of a random vector can be collection into a matrix called the covariance 
matrix cov(y) = σ2{y} =   σ2(Y1) σ(Y1,Y2) σ(Y1,Y3) σ(Y2,Y1) σ2(Y2) σ(Y2,Y3) σ(Y3,Y1) 
σ(Y3,Y2) σ2(Y3)   remember σ(Y2,Y1) = σ(Y1,Y2) so the covariance matrix is symmetric 
Page 5 Derivation of Covariance Matrix In vector terms the covariance matrix is defined by σ2{y} …},
	author = {Frank Wood},
	title = {Matrix approach to linear regresssion},
	year = {2010}
}

@misc{wood-multiple-regression-2010,
	abstract = {Multiple Regression Page 1 Multiple Regression Frank Wood March 4, 2010 Page 2 Review 
Regression Estimation We can solve this equation X Xb = X y (if the inverse of XX exists) by the 
following (XX)−1X Xb = (XX)−1X y and since (XX) −1 XX = I we have b = (XX) −1 X y Page 3 
Least Square Solution The matrix normal equations can be derived directly from the 
minimization of Q = (y − Xβ) (y − Xβ) wrt to β Page 4 Fitted Values and Residuals Let the vector 
of the fitted values are y =         ˆy1 ˆy2 . . . yn         in matrix notation 
we then have y = Xb Page 5 Hat Matrix-Puts hat on y We can also directly express the fitted 
values in terms of X and y matrices y = X(XX)−1X y and we can further define H, the “hat matrix” 
y = Hy H = X(XX)−1X The hat matrix plans an important role in diagnostics for regression 
analysis. Page 6 Hat Matrix Properties 1. the hat matrix is symmetric 2. the hat matrix is …},
	author = {Frank Wood},
	title = {Multiple Regression},
	year = {2010}
}

@misc{wood-probabilistic-deterministic-infinite-2010,
	abstract = {We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA). We define and develop and sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA). Posterior predictive inference in this model, given a finite training sequence, can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states. We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing. We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks. The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models.},
	author = {David Pfau, Nicholas Bartlett, Frank Wood},
	title = {Probabilistic deterministic infinite automata},
	year = {2010}
}

@misc{teh-a-hierarchical-nonparametric-2009,
	abstract = {In this paper we present a doubly hierarchical Pitman-Yor process language model. Its bottom layer of hierarchy consists of multiple hierarchical Pitman-Yor process language models, one each for some number of domains. The novel top layer of hierarchy consists of a mechanism to couple together multiple language models such that they share statistical strength. Intuitively this sharing results in the “adaptation” of a latent shared language model to each domain. We introduce a general formalism capable of describing the overall model which we call the graphical Pitman-Yor process and explain how to perform Bayesian inference in it. We present encouraging language model domain adaptation results that both illustrate the potential benefits of our new model and suggest new avenues of inquiry.},
	author = {Frank Wood, Yee Whye Teh},
	title = {A hierarchical nonparametric Bayesian approach to statistical language model domain adaptation},
	year = {2009}
}

@misc{teh-a-stochastic-memoizer-2009,
	abstract = {We propose an unbounded-depth, hierarchical, Bayesian nonparametric model for discrete sequence data. This model can be estimated from a single training sequence, yet shares statistical strength between subsequent symbol predictive distributions in such a way that predictive performance generalizes well. The model builds on a specific parameterization of an unbounded-depth hierarchical Pitman-Yor process. We introduce analytic marginalization steps (using coagulation operators) to reduce this model to one that can be represented in time and space linear in the length of the training sequence. We show how to perform inference in such a model without truncation approximation and introduce fragmentation operators necessary to do predictive inference. We demonstrate the sequence memoizer by using it as a language model, achieving state-of-the-art results.},
	author = {Frank Wood, Cédric Archambeau, Jan Gasthaus, Lancelot James, Yee Whye Teh},
	title = {A stochastic memoizer for sequence data},
	year = {2009}
}

@misc{paninski-nonparametric-bayesian-deconvolution-2009,
	abstract = {Let n∈ NT 0 be a vector with T elements, each of which is an integer valued indicator variable (N0={0, 1, 2,...}). Let nt∈ N0 be an element of this vector.},
	author = {Frank Wood, Joshua T Vogelstein, Liam Paninski},
	title = {Nonparametric Bayesian Deconvolution},
	year = {2009}
}

@misc{teh-a-hierarchical-2008,
	abstract = {In this paper we present a novel nonparametric Bayesian approach to domain adaptation for statistical language models. Specifically we describe a model consisting of a hierarchy of hierarchical Pitman-Yor language models (Teh, 2006; Goldwater et al., 2007), show one way to estimate such a model, and explain how inference in such a model can be intepreted as a kind of Bayesian interpolation between language models. We provide empirical evidence that this approach is sound by demonstrating improved smoothing between disparate corpora.},
	author = {Frank Wood, Yee Whye Teh},
	title = {A hierarchical, hierarchical Pitman Yor process language model},
	year = {2008}
}

@misc{black-a-nonparametric-bayesian-2008,
	abstract = {The analysis of extra-cellular neural recordings typically begins with careful spike sorting and all analysis of the data then rests on the correctness of the resulting spike trains. In many situations this is unproblematic as experimental and spike sorting procedures often focus on well isolated units. There is evidence in the literature, however, that errors in spike sorting can occur even with carefully collected and selected data. Additionally, chronically implanted electrodes and arrays with fixed electrodes cannot be easily adjusted to provide well isolated units. In these situations, multiple units may be recorded and the assignment of waveforms to units may be ambiguous. At the same time, analysis of such data may be both scientifically important and clinically relevant. In this paper we address this issue using a novel probabilistic model that accounts for several important sources of uncertainty and error in spike sorting …},
	author = {Frank Wood, Michael J Black},
	title = {A nonparametric Bayesian alternative to spike sorting},
	year = {2008}
}

@misc{pillow-characterizing-neural-dependencies-2008,
	abstract = {The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. However, there is no simple model that combines the observations that (1) marginal distributions over single-neuron spike counts are often approximately Poisson; and (2) joint distributions over the responses of multiple neurons are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using Poisson copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefficients. We explore a variety of Poisson copula models for joint neural response distributions, and derive an efficient maximum likelihood procedure for estimating them. We apply these models to neuronal data collected in and macaque motor cortex, and quantify the improvement in coding accuracy afforded by incorporating the dependency structure between pairs of neurons.},
	author = {Pietro Berkes, Frank Wood, Jonathan Pillow},
	title = {Characterizing neural dependencies with copula models},
	year = {2008}
}

@misc{teh-dependent-dirichlet-process-2008,
	abstract = {In this paper we propose a new incremental spike sorting model that automatically eliminates refractory period violations, accounts for action potential waveform drift, and can handle appearance" and" disappearance" of neurons. Our approach is to augment a known time-varying Dirichlet process that ties together a sequence of infinite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent likelihood that prohibits refractory period violations. We demonstrate this model by showing results from sorting two publicly available neural data recordings for which the a partial ground truth labeling is known."},
	author = {Jan Gasthaus, Frank Wood, Dilan Gorur, Yee Teh},
	title = {Dependent Dirichlet process spike sorting},
	year = {2008}
}

@misc{wood-modeling-neural-dependencies-2008,
	abstract = {The coding of information by neural populations depends critically on the statistical dependencies between neuronal responses. At the moment, however, we lack of a simple model that can simultaneously account for (1) marginal distributions over single-neuron spike counts that are typically close to Poisson; and (2) joint distributions over the responses of multiple neurons that are often strongly dependent. Here, we show that both marginal and joint properties of neural responses can be captured using Poisson copula models. Copulas are joint distributions that allow random variables with arbitrary marginals to be combined while incorporating arbitrary dependencies between them. Different copulas capture different kinds of dependencies, allowing for a richer and more detailed description of dependencies than traditional summary statistics, such as correlation coefficients. We explore a variety of Poisson copula …},
	author = {Pietro Berkes, Jonathan Pillow, Frank Wood},
	title = {Modeling neural dependencies with Poisson copulas},
	year = {2008}
}

@misc{wood-spike-sorting-using-2008,
	abstract = {Spike sorting is the task of grouping action potentials observed in extracellular electrophysiological recordings by source neuron. In this thesis a new incremental spike sorting model is proposed that accounts for action potential waveform drift over time, automatically eliminates refractory period violations, and can handle “appearance” and “disappearance” of neurons during the course of the recording. The approach is to augment a known time-varying Dirichlet process that ties together a sequence of infinite Gaussian mixture models, one per action potential waveform observation, with an interspike-interval-dependent term that prohibits refractory period violations.The relevant literature on spike sorting as well as (time-varying) Dirchlet process mixture models is reviewed and the new spike sorting model is described in detail, including Monte Carlo methods for performing inference in the model. The performance of the model is compared to two recent spike sorting methods on synthetic data sets as well as on neural data recordings for which a partial ground truth labeling is known. It is shown that the model performs no worse on stationary data and compares favorably if the data contains waveform change over time. Additionally, the behaviour of the model under different parameter settings and under difficult conditions is assessed and possible extensions of the model are discussed.},
	author = {Jan A Gasthaus, F Wood},
	title = {Spike sorting using time-varying DIRICHLET process mixture models},
	year = {2008}
}

@misc{black-incremental-nonparametric-bayesian-2007,
	abstract = {In this paper we develop an incremental estimation algorithm for infinite mixtures of Gaussian process experts. Incremental, local, non-linear regression algorithms are required for a wide variety of applications, ranging from robotic control to neural decoding. Arguably the most popular and widely used of such algorithms is currently Locally Weighted Projection Regression (LWPR) which has been shown empirically to be both computationally efficient and sufficiently accurate for a number of applications. While incremental variants of non-linear Bayesian regression models have superior theoretical properties and have been shown to produce better function approximations than LWPR, they suffer from high computational and storage costs. Through exploitation of locality, infinite mixtures of Gaussian process experts (IMGPE) offer the same function approximation performance with reduced computation and storage cost. Our contribution is an incremental regression approach that has the theoretical benefits of a fully Bayesian model and computational benefits that derive from exploiting locality.},
	author = {F Wood, DH Grollman, KA Heller, OC Jenkins, M Black},
	title = {Incremental nonparametric Bayesian regression},
	year = {2007}
}

@misc{wood-nonparametric-bayesian-models-2007,
	abstract = {Many neural data analyses can be cast as latent variable modeling problems. Specific examples include spike sorting and neurological data analysis. Challenges in spike sorting include figuring out how many neurons generated a set of recorded action potentials and, further, which neuron generated each action potential. A challenge in analyzing neurological data is to infer both the number and the characteristics of lesions that may be causal with respect to clinical signs presented by stroke patients. A shared characteristic of both of these problems is that the true underlying generative process is unobservable and potentially quite complex, so care must be taken in not only choosing a family of models but also in selecting a model of appropriate complexity. In such cases it may be preferable to employ a model that allows model complexity to be inferred from the data. Non-parametric Bayesian (NPB) modeling is a …},
	author = {Frank Wood},
	title = {Nonparametric bayesian models for neural data},
	year = {2007}
}

@misc{wood-sequential-monte-carlo-2007,
	abstract = {• Sample from sequence of distributions that “converge” to the distribution of interest• This is a very general technique that can be applied to a very large number of models and in a wide variety of settings.• Today: particle filtering for a first order Markov model},
	author = {Frank Wood},
	title = {Sequential Monte Carlo and Particle Filtering},
	year = {2007}
}

@misc{black-a-bayesian-2006,
	abstract = {In this work we present and apply infinite Gaussian mixture modeling, a non-parametric Bayesian method, to the problem of spike sorting. As this approach is Bayesian, it allows us to integrate prior knowledge about the problem in a principled way. Because it is non-parametric we are able to avoid model selection, a difficult problem that most current spike sorting methods do not address. We compare this approach to using penalized log likelihood to select the best from multiple finite mixture models trained by expectation maximization. We show favorable offline sorting results on real data and discuss ways to extend our model to online applications},
	author = {Frank Wood, Sharon Goldwater, Michael J Black},
	title = {A non-parametric Bayesian approach to spike sorting},
	year = {2006}
}

@misc{wood-discovering-natural-kinds-2006,
	abstract = {We address the symbol grounding problem for robot perception through a data‐driven approach to deriving categories from robot sensor data. Unlike model‐based approaches, where human intuitive correspondences are sought between sensor readings and features of an environment (corners, doors, etc.), our method learns intrinsic categories (or natural kinds) from the raw data itself. We approximate a manifold underlying sensor data using Isomap nonlinear dimension reduction and apply Bayesian clustering (Gaussian mixture models) with model identification techniques to discover categories (or kinds). We demonstrate our method through the learning of sensory kinds from trials in various indoor and outdoor environments with different sensor modalities. Learned kinds are then used to classify new sensor data (out‐of‐sample readings). We present results indicating greater consistency in classifying sensor …},
	author = {Daniel H Grollman, Odest Chadwicke Jenkins, Frank Wood},
	title = {Discovering natural kinds of robot sensory experiences in unstructured environments},
	year = {2006}
}

@misc{wood-improving-visual-2006,
	abstract = {We propose a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a modified state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve new state of the art performance on Meta-Dataset, and produce competitive results on mini-and tiered-ImageNet benchmarks.},
	author = {Peyman Bateni, Jarred Barber, Jan-Willem van de Meent, Frank Wood},
	title = {Improving Few-Shot Visual Classification with Unlabeled Examples},
	year = {2006}
}

@misc{black-inferring-attentional-state-2006,
	abstract = {Recent methods for motor cortical decoding have demonstrated relatively accurate reconstructions of hand trajectory from small populations of neurons in primary motor cortex. Decoding results are often reported only for periods when the subject is attending to the task. In a neural prosthetic interface, however, the subject must be able to switch between controlling a device or performing other mental functions. In this work we demonstrate a method for detecting whether or not a subject is attending to a motor control task. Using the firing activity of the same neural population used for decoding hand kinematics we demonstrate that a Fisher linear discriminant performs well in classifying the attentional state of a monkey. We use the output of this classifier to augment a hidden state in a first order Markov model and use particle filtering to recursively infer hand kinematics and attentional state conditioned on neural firing …},
	author = {Frank Wood, John P Donoghue, Michael J Black},
	title = {Inferring attentional state and kinematics from motor cortical firing rates},
	year = {2006}
}

@misc{wu-method-and-system-2006,
	abstract = {Frank Wood, Providence, RI (US);(57) ABSTRACT Wei Wu, Chicago, IL (US) A Switching Kalman Filter Model for the real-time inference of hand kinematics from a population of motor cortical neurons. Firing rates are modeled as a Gaussian mixture where the mean of each Gaussian component is a linear},
	author = {Michael Black, Frank Wood, Wei Wu},
	title = {Method and system for automatic decoding of motor cortical activity},
	year = {2006}
}

@misc{griffiths-particle-filtering-for-2006,
	abstract = {Many unsupervised learning problems can be expressed as a form of matrix factorization, reconstructing an observed data matrix as the product of two matrices of latent variables. A standard challenge in solving these problems is determining the dimensionality of the latent matrices. Nonparametric Bayesian matrix factorization is one way of dealing with this challenge, yielding a posterior distribution over possible factorizations of unbounded dimensionality. A drawback to this approach is that posterior estimation is typically done using Gibbs sampling, which can be slow for large problems and when conjugate priors cannot be used. As an alternative, we present a particle filter for posterior estimation in nonparametric Bayesian matrix factorization models. We illustrate this approach with two matrix factorization models and show favorable performance relative to Gibbs sampling.},
	author = {Frank Wood, Thomas Griffiths},
	title = {Particle filtering for nonparametric Bayesian matrix factorization},
	year = {2006}
}

@misc{black-statistical-analysis-of-2006,
	abstract = {Neural prosthetic technology has moved from the laboratory to clinical settings with human trials. The motor cortical control of devices in such settings raises important questions about the design of computational interfaces that produce stable and reliable control over a wide range of operating conditions. In particular, non-stationarity of the neural code across different behavioral conditions or attentional states becomes a potential issue. Non-stationarity has been previously observed in animals where the encoding model representing the mathematical relationship between neural population activity and behavioral variables such as hand motion changes over time. If such an encoding model is formed and learned during a particular training period, decoding performance (neural control) with the model may not be consistent during successive periods even when the same task is repeated. It is critical in both …},
	author = {Sung-Phil Kim, Frank Wood, Matthew Fellows, John P Donoghue, Michael J Black},
	title = {Statistical analysis of the non-stationarity of neural population codes},
	year = {2006}
}

@misc{wood-extensible-classification-2005,
	abstract = {We present an unsupervised method for online classification of robot sensor data. Unlike previous work in which human intuitive correspondences are sought between sensor data and classes of physical space (room, wall, corner, door, etc.), our method learns intrinsic classes based on the characteristics of sensor data. We approximate the manifold underlying sensor data using Isomap nonlinear dimension reduction and use classical Bayesian clustering (Gaussian mixture models) with model identification techniques to discover classes. The learned model can then be used to classify new (out-ofsample) data in real time. We apply our method to sensor data of different modalities and from different physical spaces. Our results demonstrate the robustness of the method with respect to noise and robot location variance. The extensibility of our approach allows us to classify large and streaming data sets. We conclude that data-driven classification may serve as a more solid foundation for applications that require the ability to discriminate physical spaces surrounding a robot.},
	author = {Daniel H Grollman, Odest Chadwicke Jenkins, Frank Wood},
	title = {Extensible Data-driven Classification of Robot Sensor Data},
	year = {2005}
}

@misc{black-modeling-neural-population-2005,
	abstract = {Probabilistic modeling of correlated neural population firing activity is central to understanding the neural code and building practical decoding algorithms. No parametric models currently exist for modeling multivariate correlated neural data and the high dimensional nature of the data makes fully non-parametric methods impractical. To address these problems we propose an energy-based model in which the joint probability of neural activity is represented using learned functions of the 1D marginal histograms of the data. The parameters of the model are learned using contrastive divergence and an optimization procedure for finding appropriate marginal directions. We evaluate the method using real data recorded from a population of motor cortical neurons. In particular, we model the joint probability of population spiking times and 2D hand position and show that the likelihood of test data under our model is significantly higher than under other models. These results suggest that our model captures correlations in the firing activity. Our rich probabilistic model of neural population activity is a step towards both measurement of the importance of correlations in neural coding and improved decoding of population activity.},
	author = {Frank Wood, Stefan Roth, Michael Black},
	title = {Modeling neural population spiking activity with Gibbs distributions},
	year = {2005}
}

@misc{black-automatic-spike-sorting-2004,
	abstract = {While various automated spike sorting techniques have been developed, their impact on neural decoding has not been investigated. In this paper we extend previous Gaussian mixture models and expectation maximization (EM) techniques for automatic spike sorting. We suggest that good initialization of EM is critical and can be achieved via spectral clustering. To account for noise we extend the mixture model to include a uniform outlier process. Automatically determining the number of neurons recorded per electrode is a challenging problem which we solve using a greedy optimization algorithm that selects models with different numbers of neurons according to their decoding accuracy. We focus on data recorded from motor cortex and evaluate performance with respect to the decoding of hand kinematics from firing rates. We found that spike trains obtained by our automated technique result in more accurate …},
	author = {E Wood, M Fellows, JR Donoghue, MJ Black},
	title = {Automatic spike sorting for neural decoding},
	year = {2004}
}

@misc{donoghue-on-the-variability-2004,
	abstract = {The analysis of action potentials, or "spikes," is central to systems neuroscience research. Spikes are typically identified from raw waveforms manually for off-line analysis or automatically by human-configured algorithms for on-line applications. The variability of manual spike "sorting" is studied and its implications for neural prostheses discussed. Waveforms were recorded using a micro-electrode array and were used to construct a statistically similar synthetic dataset. Results showed wide variability in the number of neurons and spikes detected in real data. Additionally, average error rates of 23% false positive and 30% false negative were found for synthetic data.},
	author = {Frank Wood, Michael J Black, Carlos Vargas-Irwin, Matthew Fellows, John P Donoghue},
	title = {On the variability of manual spike sorting},
	year = {2004}
}

@misc{wood-image-and-text-2003,
	abstract = {BACKGROUNDThe process of finding and retrieving images Stored electronically (eg, on a computer or the Internet) has become increasingly difficult for a variety of reasons. For instance, with the explosive growth of the Internet, the number of Searchable images available on the Internet has dramatically increased. With the increased number of images, the ability of conventional Systems, methods, and computer programs to perform Searching and retrieval func tions in an efficient, useful, and timely manner has been challenged.The ability of conventional Systems, methods, and com puter programs to efficiently find and retrieve desired images in a database has been hampered by poor indexing methodologies, inefficient organization, and inconsistent formatting of the images being Searched and/or retrieved. Similar problems also may be experienced by other elec tronic applications involving a large quantity of …},
	author = {Gregory S Pass, Frank Wood},
	title = {Image and text searching techniques},
	year = {2003}
}

@misc{wood-image-searching-techniques-2003,
	abstract = {(57) ABSTRACT Related US Application Data (60) Provisional application No. 60/255,399, filed on Dec. 15, A Search for an image includes receiving a Set of features 2000. corresponding to image characteristics that then are com},
	author = {Gregory S Pass, Frank Wood},
	title = {Image searching techniques},
	year = {2003}
}

@misc{wood-indexing-of-images-2003,
	abstract = {Indexing images for use in a Searchable indeX includes receiving an image. A joint histogram may be computed for the received image Such that the joint histogram includes joint features. A Searchable indeX based on the joint features within the joint histogram may be created using an index methodology. Text also may be received and a Searchable index of the received text may be created. A weighting factor may be applied to at least one of the joint features within the joint histogram Such that the joint histogram represents a weighted joint histogram. A Searchable indeX may include a first joint histogram feature corresponding to at least a first image, which includes the first joint histogram feature. The Searchable indeX also may include a Second joint histogram feature corresponding to at least a Second image, which includes the Second joint histogram feature.},
	author = {Gregory S Pass, Frank Wood},
	title = {Indexing of images and/or text},
	year = {2003}
}

@misc{wood-representing-an-image-2003,
	abstract = {BACKGROUNDThe process of finding and retrieving images Stored on electronic media (eg, a computer, the Internet) has become increasingly difficult for a variety of reasons. For instance, with the explosive growth of the Internet, the number of Searchable images available on the Internet has dramatically increased. With the increased number of images, the abilities of conventional Systems, methods, and computer programs to perform Searching and retrieval functions in an efficient, useful, and timely manner have been challenged. The ability of conventional Systems, methods, and com puter programs to efficiently find and retrieve desired images in a database has been hampered by poor organization and inconsistent formatting of the images being Searched and/or retrieved. Similar problems also may be experienced by other electronic applications involving a large quantity of images that may be searched for …},
	author = {Gregory S Pass, Frank Wood},
	title = {Representing an image with a posterized joint histogram},
	year = {2003}
}

@misc{wood-representing-an-image-2003,
	abstract = {Representing an image includes extracting Several types of information about an image that are then used to compute a joint histogram. A weighting factor may be applied to at least one of the types of information extracted about the image Such that the joint histogram represents a Weighted joint histogram. Representing an image may further include cal culating a posterized joint histogram. The posterized joint histogram may be calculated after the weighting factor has been applied.},
	author = {Gregory S Pass, Frank Wood},
	title = {Representing an image with weighted joint histogram},
	year = {2003}
}

@misc{vance-course-15-notes-1997,
	abstract = {This course addresses the field of virtual reality from the end-user’s perspective. The course it is focused on “what we can do” with VR technology, not “how to develop” the technology. The course provides attendees with criteria to identify whether or not VR technology could be a tool in their working environment. The course will cover several working VR applications in academia and industry along with discussions of their design processes.The course objective is to provide an understanding of the unique features of virtual reality and how these features can be identified and used in developing useful applications. This tutorial will answer the question-why do we need VR? What does VR have to offer that I can’t already develop using existing three-dimensional interactive computer graphics techniques? This course examines the features of VR technology and relates these features to specific applications. The course concentrates on the applicability of VR technology not on the development of hardware/software to control the various devices required in a virtual environment.},
	author = {CAROLINA CRUZ-NEIRA, RUDY DARKEN, MARY LYNNE DITTMAR, RICHARD GILLILAN, OLIVER RIEDEL, FRANK WOOD, JUDY VANCE},
	title = {COURSE 15 NOTES},
	year = {1997}
}

@misc{faerman-workspace-and-the-1996,
	abstract = {Chagas' disease afflicts more than 18 million people throughout South and Central America. Some areas of North America have also seen an increased incidence in recent years. The Trypanosoma cruzi (T. cruzi) parasite, which causes the disease, is most often transmitted by Triatomid bugs living in close proximity to humans. The T. cruzi parasite depends heavily on a small molecule called trypanothione to protect itself against damage from free radicals produced during normal metabolism. The concentration of trypanothione in the cell is carefully maintained by an enzyme called trypanothione reductase (TR). Drugs that inhibit TR should cause the parasite to die. Human cells have a similar enzyme-substrate pair: glutathione and glutathione reductase (GR). Fortunately, the enzyme GR differs electrostatically from TR, so there is hope of developing a drug that will be safe for human cells. The molecular structures …},
	author = {Frank Wood, Daniel Brown, Robert A Amidon, Jonathan Alferness, Brian Joseph, Richard E Gillilan, Carlos Faerman},
	title = {WorkSpace and the study of Chagas' disease},
	year = {1996}
}

@misc{wood-virtual-1995,
	abstract = {Introduction This paper presents our perspective on the utility of data-flow programming in the field of scientific visualization. The Cornell Theory Center (C'I'C) is an established center of visualizetion production. Scientists from across the country use the center's resources co explore the data collected from their research. The CTC uses IBM's Visualization Data Explorer TM to do most of its visualization, and maintains a repository of DX extensions that are available free to the public. The discussion in this paper primarily focuses on the flexibility and speed of development afforded by the use of modular programming. In particular, DX is shown to provide sufficient flexibility to be useful in set-tings ranging from animation production to Cornell computer science education. Examples from actual work in progress are used in this paper to underpin our advocacy OF modular data-flow programming. We be~ n by examining …},
	author = {Richard E Gillilan, Frank Wood},
	title = {Visualization, virtual reality, and animation within the data flow model of computing},
	year = {1995}
}

@misc{tamminga-schizophrenia-neuroanatomy-and-1991,
	abstract = {Accruing new knowledge by interpreting data in hypothesis testing experiments has been the hallmark of science. A quarter century ago, Platt called for scientists to make a devoted effort to conduct their work based on strong inference and theory falsification. This approach has been easier to praise than to implement in mental illness research. While positive experimental results can support hypotheses, negative findings are rarely decisive. For example, there is no dopamine parameter on which negative findings in schizophrenic patients would falsify the dopamine hypothesis.There is now considerable interest in the neuroanatomy of schizophrenia. This work can be conducted in the strong inference/theory falsification framework. Specific manifestations of schizophrenia are hypothesized to emanate from dysfunction in distinct neural circuits. Several techniques can be employed to ascertain structure and …},
	author = {WT Carpenter Jr, RW Buchanan, B Kirkpatrick, F Wood, C Tamminga},
	title = {Schizophrenia neuroanatomy and theory falsification},
	year = {1991}
}

@misc{wood-algorithmic-optimisation-of-nodate,
	abstract = {Introduction We present a methodology for optimising the parameter values of a neuron morphology generator. To date, neuron morphology generators have only been optimised by hand or by means not well defined in the literature. We optimise a neuron generator similar to that proposed by Koene [1] by approximately matching features extracted from both generated and ground truth neurons using probabilistic programming and Bayesian optimisation. We have created a software package capable of carrying out this task for a given set of ground truth neurons. One use of a model trained by our method is probabilistic neuronal tracing, using the generative model to propose potential elongations that are scored and continued as appropriate. This approach may be used as a top-down prior for automated segmentation of connectomics data, such as the data presented by Kasthuri [2]. Prior Art All neuron morphology …},
	author = {Andrew Warrington, Frank Wood},
	title = {Algorithmic Optimisation of Neuron Generator Parameters}
}

@misc{rainforth-amortized-monte-carlo-nodate,
	abstract = {Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is in turn used to calculate expectations for one or more target functions. In this paper, we address the inefficiency of this computational pipeline when the target function (s) are known upfront. To this end, we introduce a method for amortizing Monte Carlo integration. Our approach operates in a similar manner to amortized inference, but tailors the produced amortization artifacts to maximize the accuracy of the resulting expectation calculation (s). We show that while existing approaches have fundamental limitations in the level of accuracy that can be achieved for a given run time computational budget, our framework can, at least in theory, produce arbitrary small errors for a wide range of target functions with O (1) computational cost at run time. Furthermore, our framework allows not only for amortizing over possible datasets, but also over possible target functions.},
	author = {Yee Whye Teh, Frank Wood, Tom Rainforth},
	title = {Amortized Monte Carlo Integration}
}

@misc{rainforth-appendices-for-amortized-nodate,
	abstract = {Figure 4: Additional results for one-dimensional tail integral example as per Figure 1a.[left] Relative mean squared errors (as per (25)).[right] Mean squared error E [(µ (y, θ)− ˆµ (y, θ) 2]. Conventions as per Figure 1. The results for SNIS q1 indicate that it severely underestimates E2 leading to very large errors, especially when the mismatch between p (x| y) and f (x; θ) is as significant as in the tail integral case.},
	author = {Adam Golinski, Frank Wood, Tom Rainforth},
	title = {Appendices for Amortized Monte Carlo Integration}
}

@misc{teh-asynchronous-anytime-sequential-nodate,
	abstract = {X0∼ µ, Xn| X0: n− 1= x0: n− 1, Y0: n− 1∼ fn (xn| x0: n− 1) for n≥ 1,},
	author = {Brooks Paige, Frank Wood, Arnaud Doucet, Yee Whye Teh},
	title = {Asynchronous Anytime Sequential Monte Carlo: Supplemental Material}
}

@misc{wood-bayesian-optimization-for-nodate,
	abstract = {Anglican is a probabilistic programming language integrated into Clojure (a dialect of Lisp) and inherits most of the corresponding syntax. Anglican extends Clojure with the special forms sample and observe [1]. Each random draw in an Anglican program corresponds to a sample call, which can be thought of as a term in the prior. Each observe statement applies weighting to a program trace and thus constitutes a term in the likelihood. Compilation of an Anglican program, performed by the macro query, corresponds to transforming the code into a variant of continuation-passing style (CPS) code, which results in a function that can be executed using a particular inference algorithm.Anglican program code is represented by a nested list of expressions, symbols, non-literals for contructing data structures (eg [...] for vectors), and command dependent literals (eg [...] as a second argument of a let statement which is used for binding pairs). In order to perform program transformations, we can recursively traverse this nested list which can be thought of as an abstract syntax tree of the program.},
	author = {Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A Osborne, Frank Wood},
	title = {Bayesian Optimization for Probabilistic Programs-Supplementary Materials}
}

@misc{perov-coding-dirichlet-process-nodate,
	abstract = {Dirichlet process mixtures, reviewed in depth in [Teh, 2010, Orbanz and Teh, 2010] and the subject of excellent tutorial presentations by Teh [2007], are widely used in Bayesian unsupervised clustering and density estimation tasks. In particular the infinite Gaussian mixture model Rasmussen [1999] has been widely used. A canonical example application is neural spike sorting Wood and Black [2008](this latter applied work also highlights efficient sequential inference).Stick-breaking constructions [Ishwaran and James, 2001] make coding some Bayesian nonparametric primitives in probabilistic programming systems relatively straightforward. Additionally there is an interesting and deep (in not fully or even well described in the literature) connection between the action of Dirichlet-like stochastic processes and relaxations of the programming languages technique called memoization [Michie, 1968]. The latter, simply put, is the idea of wrapping a function in a hashmap so that it remembers and thus never needs to recompute a return value if called again with the same arguments. Memoization can sometimes give rise to very simple dynamic programming algorithms.},
	author = {FRANK WOOD, YURA PEROV},
	title = {CODING DIRICHLET PROCESS MODELS VIA PROBABILISTIC PROGRAMMING}
}

@misc{wood-compilation-nodate,
	abstract = {Daphne is a probabilistic programming system that provides an expressive syntax to denote a large, but restricted, class of probabilistic models. Programs written in the Daphne language can be compiled into a general graph data structure of a corresponding probabilistic graphical model with simple link functions that can easily be implemented in a wide range of programming environments. Alternatively Daphne can also further compile such a graphical model into understandable and vectorized PyTorch code that can be used to train neural networks for inference. The Daphne compiler is structured in a layered multi-pass compiler framework that allows independent and easy extension of the syntax by adding additional passes. It leverages extensive partial evaluation to reduce all syntax extensions to the graphical model at compile time.},
	author = {Christian Dietrich Weilbach, Frank Wood},
	title = {Daphne: Multi-Pass Compilation of Probabilistic Programs into Graphical Models and Neural Networks}
}

@misc{rainforth-effective-approximate-inference-nodate,
	abstract = {We introduce two approaches for conducting effective approximate inference in stochastic simulators containing nested stochastic subprocedures, ie internal procedures for which the density cannot be calculated directly such as rejection sampling loops and nested inferences. The resulting class of simulators are used extensively throughout the sciences, but fall outside the standard class of Bayesian models: they are doubly intractable. Drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalized density, preventing the use of standard procedures. In particular, the small number of specialized existing methods that can deal with such models are based around forward sampling and thus scale catastrophically poorly in the dimensionality. To address this, we introduce algorithms based on a two-step approach that first approximates the conditional densities of …},
	author = {Bradley J Gram-Hansen, Adam Golinski, Christian Schroeder de Witt, Saeid Naderiparizi, Adam Scibior, Andreas Munk, Frank Wood, Philip Torr, Yee Whye Teh, Atilim Gunes Baydin, Tom Rainforth},
	title = {Effective Approximate Inference for Nested Simulators}
}

@misc{wood-gentle-introduction-to-nodate,
	abstract = {Conclusions• Bayesian mixture modeling is principled way to add prior information into the modeling process• IMM/CRP is a way estimate the number of hidden classes},
	author = {Frank Wood},
	title = {Gentle Introduction to Infinite Gaussian Mixture Modeling}
}

@misc{wood-inference-networks-for-nodate,
	abstract = {We introduce a new approach for amortizing inference in directed graphical models. Inference in graphical models entails characterizing the joint distribution of latent random variables conditioned on observed random variables. We describe a procedure for constructing and learning a structured neural network which represents an inverse factorization of the graphical model, resulting in a conditional density estimator that takes as input particular values of the observed random variables, and returns an approximation the posterior distribution of the latent variables. This recognition model can be learned offline, independent from any particular dataset, prior to performing inference. The output from this network can be used either directly as a crude approximate estimator, or refined via importance sampling or sequential Monte Carlo to provide consistent estimates of posterior expectations and unbiased estimates of the marginal likelihood.},
	author = {Brooks Paige, Frank Wood},
	title = {Inference Networks for Graphical Models}
}

@misc{wood-interacting-particle-markov-nodate,
	abstract = {This is the supplementary material to the main manuscript Interacting Particle Markov Chain Monte Carlo. We provide additional details for using all the particles and present further results on the experiments and choosing number of conditional nodes or workers.},
	author = {Tom Rainforth, Fredrik Lindsten, IT UU, Brooks Paige, Jan-Willem van de Meent, Arnaud Doucet, Frank Wood},
	title = {Interacting Particle Markov Chain Monte Carlo-Supplementary Material}
}

@misc{wood-glimpse-sequences-nodate,
	abstract = {Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies. Hard attention mechanisms are typically non-differentiable. They can be trained with reinforcement learning but the high-variance training this entails hinders more widespread application. We show how hard attention for image classification can be framed as a Bayesian optimal experimental design (BOED) problem. From this perspective, the optimal locations to attend to are those which provide the greatest expected reduction in the entropy of the classification distribution. We introduce methodology from the BOED literature to approximate this optimal behaviour, and use it to generate `near-optimal' sequences of attention locations. We then show how to use such sequences to partially supervise, and therefore speed up, the training of a hard attention mechanism. Although generating these sequences is computationally expensive, they can be reused by any other networks later trained on the same task.},
	author = {William Harvey, Michael Teng, Frank Wood},
	title = {Near-Optimal Glimpse Sequences for Training Hard Attention Neural Networks}
}

@misc{wood-on-nesting-monte-nodate,
	abstract = {Proof. Though the Theorem follows directly from Theorem 3, we also provide the following proof for this simplified case to provide a more accessible intuition behind the result. Note that the approach taken is distinct from the proof of Theorem 3. Using Minkowski’s inequality, we can bound the mean squared error of IN, M by E [(I− IN, M) 2]= I− IN, M},
	author = {Tom Rainforth, Robert Cornish, Hongseok Yang Andrew Warrington, Frank Wood},
	title = {On Nesting Monte Carlo Estimators–Supplementary Material}
}

@misc{wood-performing-inference-in-nodate,
	abstract = {One of the key characteristics of higher-order probabilistic programming languages equiped with eval is that program text both can be generated and evaluated. In higher-order languages (Lisp, Scheme, Church, Anglican and Venture) functions are first class objects–evaluating program text that defines a valid procedure returns a procedure that can be applied to arguments.In the context of probabilistic programming this means that we can write programs that program. We can make the computer write its own code. In the following we set up what is essentially a regression problem and ask the computer to find a solution in the space of arithmetic expressions that could give rise to the observed relationship. Note very carefully that this approach stands in sharp contrast to picking a function family and looking for a parameterization of a member of this family that is optimal with respect to some cost function and observed input output values. The following program generates simple arithmetic expression code using a probabilistic context-fee grammar (PCFG) Johnson [1998] generative model (productions) and then evaluates those expressions on known input values and compares the output to known outputs. It then predicts both the procedure text and the value of the procedure applied to a new argument.},
	author = {FRANK WOOD},
	title = {PERFORMING INFERENCE IN THE SPACE OF EXPRESSIONS VIA PROBABILISTIC PROGRAMMING}
}

@misc{wood-power-law-unbounded-nodate,
	abstract = {We consider compression of sequences using a predictive model that incrementally estimates a distribution over what symbol comes next from the preceding sequence of symbols. As our predictive model we use the sequence memoizer (SM)[1], a nonparametric Bayesian model for sequences of unbounded complexity. This model is combined with an entropy coder, for instance the arithmetic coder of [2], to yield a method for compressing sequence data.At the algorithmic level, the proposed method is somewhat similar to the unbounded context length variants of the well known PPM and CTW algorithms [3, 4]. At a conceptual level, however, our approach is quite different: We take a Bayesian approach, treating the distributions over next symbols as latent variables on which we place a hierarchical nonparametric prior, and predicting the next symbol by averaging over the posterior distribution (conditioned on the …},
	author = {Jan Gasthaus, Yee Whye Teh, Frank Wood},
	title = {Power Law Unbounded Markov Prediction}
}

@misc{zhang-seeing-eye-to-nodate,
	abstract = {The quality of communication in videoconferencing is compromised by the fact that participants cannot maintain eye contact. We propose an original solution to this problem. Using color and depth information from a Kinect camera, we will use a Conditional Random Field to model each participant’s eye position in 3D space, which will allow us to reorient the image to create the illusion of looking through a window. Finally, we will use a Fields of Experts model to fill in unobserved parts of the transformed image.},
	author = {Ben Cheng, Jessica Forde, Hirotaka Miura, Benjamin Rapaport, Frank Wood, Xinyue Zhang},
	title = {Seeing Eye to Eye: Perspective Adjustment for Videoconferencing}
}

@misc{wood-semantics-of-nodate,
	abstract = {Probabilistic programming is the idea of using programs for expressing probabilistic models. It aims at enabling scientists to develop sophisticated probabilistic models easily. Most probabilistic programming languages blend usual programming constructs with special probabilistic primitives, so that scientists can use both and express advanced models succinctly. Also, these languages come with generic inference algorithms, and relieve scientists of the nontrivial task of designing custom inference algorithms for their models. Several probabilistic programming languages are available by now. Some languages such as Infer. net and Stan restrict the form of programs but provide powerful inference algorithms, while others such as Church, Venture and Anglican have less-performant inference algorithms but support continuous and other advanced distributions and include powerful features of general-purpose …},
	author = {Sam Staton, Hongseok Yang, Chris Heunen, Ohad Kammar, Frank Wood},
	title = {Semantics of Higher-order Probabilistic Programs}
}

@misc{sigal-supplemental-material-for-nodate,
	abstract = {Section 3.2 of [39] explains the sampling procedure to generate tasks from the Meta-Dataset [39], used during both training and testing. This results in tasks with varying of number of shots/ways. Figure 9a and 9b show the ways/shots frequency graphs at test time. For evaluating on Meta-Dataset and mini/tiered-ImageNet datasets, we use episodic training [36] to train models to remain consistent with the prior works [3, 30, 36, 39]. We train for 110K tasks, 16 tasks per batch, totalling 6,875 gradient steps using Adam with learning rate of 0.0005. We validate (on 8 in-domain and 1 out-of-domain datasets) every 10K tasks, saving the best model/checkpoint for testing. Please visit the Pytorch implementation of Simple CNAPS for details.},
	author = {Peyman Bateni, Raghav Goyal, Vaden Masrani, Frank Wood, Leonid Sigal},
	title = {Supplemental Material for Improved Few-Shot Visual Classification}
}

@misc{wood-supplementary-materials-nodate,
	abstract = {Simulator failure is then defined as when the change in radius is greater than 0.03 in a single iteration. This effectively constrains the state to orbit in a roughly circular orbit around the origin, with large perturbations to velocity tangentially allowed, but only relatively small perturbations to the velocity in the radial direction.To compute the variances of the SMC sweep we generate 100 random traces, on traces of length 100. We then perform 100 SMC sweeps per trace, using 100 particles, and compute a pseudo-marginal estimate of the evidence as the product of the expected likelihoods at each observation.},
	author = {Andrew Warrington, Saeid Naderiparizi, Frank Wood},
	title = {Supplementary Materials for: Coping With Simulators That Don’t Always Return}
}

@misc{wood-towards-inference-amortization-nodate,
	abstract = {● Inference amortization is a technique that greatly reduces the computational cost of run-time inference by training a neural network approximating the posterior distribution q (x| y; φ)~ p (x| y) ahead of the time of the system operation φ are the learnt parameters of the neural network● Anglican is a universal, research-oriented PPL which implements some of the cutting-edge inference techniques including inference amortization● To enable BUGS models to use inference amortization we have created a compiler translating models from BUGS to Anglican},
	author = {Adam Goliński, Frank Wood},
	title = {Towards Inference Amortization for BUGS models: BUGS to Anglican compilation}
}

@misc{wood-diffusion-nodate,
	abstract = {Recent progress with conditional image diffusion models has been stunning, and this holds true whether we are speaking about models conditioned on a text description, a scene layout, or a sketch. Unconditional image diffusion models are also improving but lag behind, as do diffusion models which are conditioned on lower-dimensional features like class labels. We advocate for a simple method that leverages this phenomenon for better unconditional generative modeling. In particular, we suggest a two-stage sampling procedure. In the first stage we sample an embedding describing the semantic content of the image. In the second stage we use a conditional image diffusion model to sample the image conditioned on this embedding, and then discard the embedding. The combined model can therefore leverage the power of conditional diffusion models on the unconditional generation task, achieving large improvements in unconditional image generation. The same method can be generalized to yield similar improvements for image generation conditioned on a low-dimensional signal like a class label.},
	author = {William Harvey, Frank Wood},
	title = {Two-Stage Diffusion Models: Better Image Synthesis by Explicitly Modeling Semantics}
}

@misc{paige-writing-a-rejection-nodate,
	abstract = {In this exercise we will write a rejection sampler which draws from a Poisson distribution. An algorithm to sample a Poisson random variate k with rate λ is given by [1]. Initialize L← e-λ, k← 0, and p← 1, and then loop:},
	author = {FRANK WOOD, BROOKS PAIGE},
	title = {WRITING A REJECTION SAMPLER}
}

@misc{faerman-workspace-and-the-nodate,
	abstract = {Researchers are only b beginning to examine the value of using immersive environments in their work. In particular, immersive study may have unique benefits in meeting unique visualization challenges in the study of biological systems. The Workspace toolkit was originally designed to study the Chagas' disease system, which is rich in detail on scales ranging from the ecological to the molecular. Understanding this disease system requires understanding interactions at scales that vary through 10 orders of magnitude. With this in mind we designed the Workspace toolkit to handle navigation through space and scale. It has two primary components:},
	author = {Frank Wood, Daniel Brown, Robert A Amidon, Jonathan Alferness, Brian Joseph, Richard E Gillilan, Carlos Faerman},
	title = {Workspace and the Study of Chagas'}
}
