@unpublished{yoo2020ensemble,
      title={Ensemble Squared: A Meta AutoML System}, 
      author={Jason Yoo and Tony Joseph and Dylan Yung and S. Ali Nasseri and Frank Wood},
      year={2020},
      eprint={2012.05390},
      archivePrefix={arXiv},
      primaryClass={cs.LG},  
      url_ArXiv={https://arxiv.org/abs/2012.05390},
      url_Paper={https://arxiv.org/pdf/2012.05390.pdf},
      support = {D3M},
      abstract = {The continuing rise in the number of problems amenable to machine learning solutions, coupled with simultaneous growth in both computing power and variety of machine learning techniques has led to an explosion of interest in automated machine learning (AutoML). This paper presents Ensemble Squared (Ensemble2), a "meta" AutoML system that ensembles at the level of AutoML systems. Ensemble2 exploits the diversity of existing, competing AutoML systems by ensembling the top-performing models simultaneously generated by a set of them. Our work shows that diversity in AutoML systems is sufficient to justify ensembling at the AutoML system level. In demonstrating this, we also establish a new state of the art AutoML result on the OpenML classification challenge.}
}

@article{RAI-20,
  author  = {Tom Rainforth and Adam Golinski and Frank Wood and Sheheryar Zaidi},
  title   = {Target-Aware Bayesian Inference: How to Beat Optimal Conventional Estimators},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {88},
  pages   = {1-54},
  url_Link = {http://jmlr.org/papers/v21/19-102.html},
  url_Paper = {https://www.jmlr.org/papers/volume21/19-102/19-102.pdf}
}

@InProceedings{Le-20, 
  title = {Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow}, 
  author = {Le, Tuan Anh and Kosiorek, Adam R. and Siddharth, N. and Teh, Yee Whye and Wood, Frank}, 
  pages = {1039--1049}, 
  year = {2020}, 
  editor = {Ryan P. Adams and Vibhav Gogate}, 
  volume = {115}, 
  series = {Proceedings of Machine Learning Research}, 
  address = {Tel Aviv, Israel}, 
  month = {22--25 Jul}, 
  publisher = {PMLR}, 
  url_Link = {http://proceedings.mlr.press/v115/le20a.html}, 
  url_Paper = {http://proceedings.mlr.press/v115/le20a/le20a.pdf}, 
  support = {D3M},
  abstract = {Stochastic control-flow models (SCFMs) are a class of generative models that involve branching on choices from discrete random variables. Amortized gradient-based learning of SCFMs is challenging as most approaches targeting discrete variables rely on their continuous relaxations—which can be intractable in SCFMs, as branching on relaxations requires evaluating all (exponentially many) branching paths. Tractable alternatives mainly combine REINFORCE with complex control-variate schemes to improve the variance of naive estimators. Here, we revisit the reweighted wake-sleep (RWS) [5] algorithm, and through extensive evaluations, show that it outperforms current state-of-the-art methods in learning SCFMs. Further, in contrast to the importance weighted autoencoder, we observe that RWS learns better models and inference networks with increasing numbers of particles. Our results suggest that RWS is a competitive, often preferable, alternative for learning SCFMs.} 
  }

@unpublished{MUN-20a,
  title={Assisting the Adversary to Improve {GAN} Training}, 
  author={Andreas Munk and William Harvey and Frank Wood},
  year={2020},
  eprint={2010.01274},
  journal={arXiv preprint arXiv:2010.01274},
  year={2020},
  eid = {arXiv:2010.01274},
  archivePrefix = {arXiv},
  url_ArXiv={https://arxiv.org/abs/2010.01274},
  url_Paper={https://arxiv.org/pdf/2010.01274.pdf},
  support = {D3M,ETALUMIS},
  abstract={We propose a method for improved training of generative adversarial networks (GANs). Some of the most popular methods for improving the stability and performance of GANs involve constraining or regularizing the discriminator. Our method, on the other hand, involves regularizing the generator. It can be used alongside existing approaches to GAN training and is simple and straightforward to implement. Our method is motivated by a common mismatch between theoretical analysis and practice: analysis often assumes that the discriminator reaches its optimum on each iteration. In practice, this is essentially never true, often leading to poor gradient estimates for the generator. To address this, we introduce the Adversary's Assistant (AdvAs). It is a theoretically motivated penalty imposed on the generator based on the norm of the gradients used to train the discriminator. This encourages the generator to move towards points where the discriminator is optimal. We demonstrate the effect of applying AdvAs to several GAN objectives, datasets and network architectures. The results indicate a reduction in the mismatch between theory and practice and that AdvAs can lead to improvement of GAN training, as measured by FID scores.}
}

@unpublished{NAD-20a,
  title={Uncertainty in Neural Processes}, 
  author={Saeid Naderiparizi and Kenny Chiu and Benjamin Bloem-Reddy and Frank Wood},
  journal={arXiv preprint arXiv:1906.05462},
  year={2020},
  eid = {arXiv:2010.03753},
  archivePrefix = {arXiv},
  eprint = {2010.03753},
  url_ArXiv={https://arxiv.org/abs/2010.03753},
  url_Paper={https://arxiv.org/pdf/2010.03753.pdf},
  support = {D3M,ETALUMIS},
  abstract={We explore the effects of architecture and training objective choice on amortized posterior predictive inference in probabilistic conditional generative models. We aim this work to be a counterpoint to a recent trend in the literature that stresses achieving good samples when the amount of conditioning data is large. We instead focus our attention on the case where the amount of conditioning data is small. We highlight specific architecture and objective choices that we find lead to qualitative and quantitative improvement to posterior inference in this low data regime. Specifically we explore the effects of choices of pooling operator and variational family on posterior quality in neural processes. Superior posterior predictive samples drawn from our novel neural process architectures are demonstrated via image completion/in-painting experiments.}
}

@inproceedings{TEN-20,
  title={Semi-supervised Sequential Generative Models},
  author={Teng, Michael and Le, Tuan Anh and Scibior, Adam and Wood, Frank},
  booktitle={Conference on Uncertainty in Artificial Intelligence (UAI)},
  eid = {arXiv:2007.00155},
  archivePrefix = {arXiv},
  eprint = {2007.00155},
  url_Link = {http://www.auai.org/~w-auai/uai2020/accepted.php},
  url_Paper={http://www.auai.org/uai2020/proceedings/272_main_paper.pdf},
  url_ArXiv = {https://arxiv.org/abs/2007.00155},
  support = {D3M},
  year={2020}
}

@inproceedings{WEI-20,
  title={Structured Conditional Continuous Normalizing Flows for Efficient Amortized Inference in Graphical Models},
  author={Weilbach, Christian and Beronov, Boyan and Wood, Frank and Harvey, William},
  booktitle={Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={4441--4451},
  year={2020},
  url_Link={http://proceedings.mlr.press/v108/weilbach20a.html},
  url_Paper={http://proceedings.mlr.press/v108/weilbach20a/weilbach20a.pdf},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/PROBPROG2020_WEI.pdf},
  support = {D3M},
  bibbase_note = {PMLR 108:4441-4451},
  abstract = {We exploit minimally faithful inversion of graphical model structures to specify sparse continuous normalizing flows (CNFs) for amortized inference. We find that the sparsity of this factorization can be exploited to reduce the numbers of parameters in the neural network, adaptive integration steps of the flow, and consequently FLOPs at both training and inference time without decreasing performance in comparison to unconstrained flows. By expressing the structure inversion as a compilation pass in a probabilistic programming language, we are able to apply it in a novel way to models as complex as convolutional neural networks. Furthermore, we extend the training objective for CNFs in the context of inference amortization to the symmetric Kullback-Leibler divergence, and demonstrate its theoretical and practical advantages.}
}

@unpublished{HAR-20a,
  title={Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training},
  author={Harvey, William and Teng, Michael and Wood, Frank},
  journal={arXiv preprint arXiv:1906.05462},
  year={2019},
  eid = {arXiv:1906.05462},
  archivePrefix = {arXiv},
  eprint = {1906.05462},
  url_ArXiv={https://arxiv.org/abs/1906.05462},
  url_Paper={https://arxiv.org/pdf/1906.05462.pdf},
  support = {D3M,LwLL},
  abstract={Hard visual attention is a promising approach to reduce the computational burden of modern computer vision methodologies. Hard attention mechanisms are typically non-differentiable. They can be trained with reinforcement learning but the high-variance training this entails hinders more widespread application. We show how hard attention for image classification can be framed as a Bayesian optimal experimental design (BOED) problem. From this perspective, the optimal locations to attend to are those which provide the greatest expected reduction in the entropy of the classification distribution. We introduce methodology from the BOED literature to approximate this optimal behaviour, and use it to generate `near-optimal' sequences of attention locations. We then show how to use such sequences to partially supervise, and therefore speed up, the training of a hard attention mechanism. Although generating these sequences is computationally expensive, they can be reused by any other networks later trained on the same task.}
}

@unpublished{BAT-20a,
  title={Improving Few-Shot Visual Classification with Unlabelled Examples},
  author={Bateni, Peyman and Barber, Jarred and van de Meent, Jan-Willem and Wood, Frank},
  journal={arXiv preprint},
  year={2020},
  eid = {arXiv:2006.12245},
  archivePrefix = {arXiv},
  eprint = {2006.12245},
  url_Paper={https://arxiv.org/pdf/2006.12245.pdf},
  url_ArXiv={https://arxiv.org/abs/2006.12245},
  support = {D3M,LwLL},
  abstract={We propose a transductive meta-learning method that uses unlabelled instances to improve few-shot image classification performance. Our approach combines a regularized Mahalanobis-distance-based soft k-means clustering procedure with a state of the art neural adaptive feature extractor to achieve improved test-time classification accuracy using unlabelled data. We evaluate our method on transductive few-shot learning tasks, in which the goal is to jointly predict labels for query (test) examples given a set of support (training) examples. We achieve new state of the art in-domain performance on Meta-Dataset, and improve accuracy on mini- and tiered-ImageNet as compared to other conditional neural adaptive methods that use the same pre-trained feature extractor.}  
}

@inproceedings{NAD-20,
  title={Amortized rejection sampling in universal probabilistic programming},
  author={Naderiparizi, Saeid and {\'S}cibior, Adam and Munk, Andreas and Ghadiri, Mehrdad and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Gram-Hansen, Bradley and de Witt, Christian Schroeder and Zinkov, Robert and Torr, Philip HS and Rainforth, Tom and others},
  booktitle={International Conference on Probabilistic Programming (PROBPROG)},
  year = 2020,
  eid = {arXiv:1910.09056},
  archivePrefix = {arXiv},
  eprint = {1910.09056},
  url_Paper={https://arxiv.org/pdf/1910.09056.pdf},
  url_ArXiv={https://arxiv.org/abs/1910.09056},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/PROBPROG2020_NAD.pdf},
  support = {D3M,ETALUMIS},
  abstract={Existing approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. An instance of this is importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.}
}

@inproceedings{BRE-20,
  author = {{Brekelmans}, Rob and {Masrani}, Vaden and {Wood}, Frank and {Ver Steeg}, Greg and {Galstyan}, Aram},
  title = {All in the Exponential Family: Bregman Duality in Thermodynamic Variational Inference},
  booktitle={Thirty-seventh International Conference on Machine Learning (ICML 2020)},
  keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
  year = 2020,
  month = jul,
  eid = {arXiv:2007.00642},
  archivePrefix = {arXiv},
  eprint = {2007.00642},
  url_Link = {https://proceedings.icml.cc/book/2020/hash/12311d05c9aa67765703984239511212},
  url_Paper={https://proceedings.icml.cc/static/paper_files/icml/2020/2826-Paper.pdf},
  url_ArXiv={https://arxiv.org/abs/2007.00642},
  support = {D3M},
  abstract={The recently proposed Thermodynamic Variational Objective (TVO) leverages thermodynamic integration to provide a family of variational inference objectives, which both tighten and generalize the ubiquitous Evidence Lower Bound (ELBO). However, the tightness of TVO bounds was not previously known, an expensive grid search was used to choose a "schedule" of intermediate distributions, and model learning suffered with ostensibly tighter bounds. In this work, we propose an exponential family interpretation of the geometric mixture curve underlying the TVO and various path sampling methods, which allows us to characterize the gap in TVO likelihood bounds as a sum of KL divergences. We propose to choose intermediate distributions using equal spacing in the moment parameters of our exponential family, which matches grid search performance and allows the schedule to adaptively update over the course of training. Finally, we derive a doubly reparameterized gradient estimator which improves model learning and allows the TVO to benefit from more refined bounds. To further contextualize our contributions, we provide a unified framework for understanding thermodynamic integration and the TVO using Taylor series remainders.}
  }

@unpublished{WOO-20,
  author = {{Wood}, Frank and {Warrington}, Andrew and {Naderiparizi}, Saeid and {Weilbach}, Christian and {Masrani}, Vaden and {Harvey}, William and {Scibior}, Adam and {Beronov}, Boyan and {Nasseri}, Ali},
  title = {Planning as Inference in Epidemiological Models},
  journal = {arXiv e-prints},
  keywords = {Quantitative Biology - Populations and Evolution, Computer Science - Machine Learning, Statistics - Machine Learning},
  year = {2020},
  eid = {arXiv:2003.13221},
  archivePrefix = {arXiv},
  eprint = {2003.13221},
  support = {D3M,COVID},
  url_ArXiv={https://arxiv.org/abs/2003.13221},
  url_Paper={https://arxiv.org/pdf/2003.13221.pdf},
  abstract={In this work we demonstrate how existing software tools can be used to automate parts of infectious disease-control policy-making via performing inference in existing epidemiological dynamics models. The kind of inference tasks undertaken include computing, for planning purposes, the posterior distribution over putatively controllable, via direct policy-making choices, simulation model parameters that give rise to acceptable disease progression outcomes. Neither the full capabilities of such inference automation software tools nor their utility for planning is widely disseminated at the current time. Timely gains in understanding about these tools and how they can be used may lead to more fine-grained and less economically damaging policy prescriptions, particularly during the current COVID-19 pandemic.}
}

@inproceedings{WAR-20,
  title={Coping With Simulators That Don’t Always Return},
  author={Warrington, A and Naderiparizi, S and Wood, F},
  booktitle={The 23rd International Conference on Artificial Intelligence and Statistics (AISTATS)},
  archiveprefix = {arXiv},
  eprint = {1906.05462},
  year={2020},
  url_Link = {http://proceedings.mlr.press/v108/warrington20a.html},
  url_Paper = {http://proceedings.mlr.press/v108/warrington20a/warrington20a.pdf},
  url_Poster = {https://github.com/plai-group/bibliography/blob/master/presentations_posters/WAR-20.pdf},
  url_ArXiv = {https://arxiv.org/abs/2003.12908},
  keywords = {simulators, smc, autoregressive flow},
  support = {D3M,ETALUMIS},
  bibbase_note={PMLR 108:1748-1758},
  abstract = {Deterministic models are approximations of reality that are easy to interpret and often easier to build than stochastic alternatives. Unfortunately, as nature is capricious, observational data can never be fully explained by deterministic models in practice. Observation and process noise need to be added to adapt deterministic models to behave stochastically, such that they are capable of explaining and extrapolating from noisy data. We investigate and address computational inefficiencies that arise from adding process noise to deterministic simulators that fail to return for certain inputs; a property we describe as "brittle." We show how to train a conditional normalizing flow to propose perturbations such that the simulator succeeds with high probability, increasing computational efficiency.}
  }

@techreport{WOO-19,
  title={Hasty-A Generative Model Complier},
  author={Wood, Frank and Teng, Michael and Zinkov, Rob},
  year={2019},
  institution={University of Oxford Oxford United Kingdom},
  url_Link={https://apps.dtic.mil/sti/citations/AD1072839},
  url_Paper={https://apps.dtic.mil/sti/pdfs/AD1072839.pdf},
  support = {D3M},
  abstract = {This work describes our contribution of proof of concept primitives to the D3M program and research progress made towards an initial version of Hasty. Although we were unable to complete the initial version of Hasty, or contribute to the D3M primitive library the types of primitives that Hasty will enable we did train a number of Highly Qualified Personnel HQP and have interacted with the AutoML, probabilistic programming languages, neural networking, and other communities which our work is expected to impact.}
}

@inproceedings{WAR-19a,
  title={Coping With Simulators That Don’t Always Return},
  author={Warrington, A and Naderiparizi, S and Wood, F},
  booktitle={2nd Symposium on Advances in Approximate Bayesian Inference (AABI)},
  year={2019},
  url_Link={https://openreview.net/forum?id=SJecKyhEKr&noteId=SJecKyhEKr},
  url_Paper={https://openreview.net/pdf?id=SJecKyhEKr},
  keywords = {simulators, smc, autoregressive flow},
  support = {D3M,ETALUMIS},
  abstract = {Deterministic models are approximations of reality that are often easier to build and interpret than stochastic alternatives.  
Unfortunately, as nature is capricious, observational data can never be fully explained by deterministic models in practice.  
Observation and process noise need to be added to adapt deterministic models to behave stochastically, such that they are capable of explaining and extrapolating from noisy data.
Adding process noise to deterministic simulators can induce a failure in the simulator resulting in no return value for certain inputs -- a property we describe as ``brittle.''
We investigate and address the wasted computation that arises from these failures, and the effect of such failures on downstream inference tasks.
We show that performing inference in this space can be viewed as rejection sampling, and train a conditional normalizing flow as a proposal over noise values such that there is a low probability that the simulator crashes, increasing computational efficiency and inference fidelity for a fixed sample budget when used as the proposal in an approximate inference algorithm.}
}

@inproceedings{HAR-19,
  title={Near-Optimal Glimpse Sequences for Improved Hard Attention Neural Network Training},
  author={Harvey, William and Teng, Michael and Wood, Frank},
  booktitle={NeurIPS Workshop on Bayesian Deep Learning},
  year={2019},
  support = {D3M,LwLL},
  archiveprefix = {arXiv},
  eprint = {1906.05462},
  url_Paper={http://bayesiandeeplearning.org/2019/papers/38.pdf},
  url_ArXiv={https://arxiv.org/abs/1906.05462},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/HAR-19.pdf},
  abstract = {We introduce the use of Bayesian optimal experimental design techniques for generating glimpse sequences to use in semi-supervised training of hard attention networks. Hard attention holds the promise of greater energy efficiency and superior inference performance. Employing such networks for image classification usually involves choosing a sequence of glimpse locations from a stochastic policy. As the outputs of observations are typically non-differentiable with respect to their glimpse locations, unsupervised gradient learning of such a policy requires REINFORCE-style updates. Also, the only reward signal is the final classification accuracy. For these reasons hard attention networks, despite their promise, have not achieved the wide adoption that soft attention networks have and, in many practical settings, are difficult to train. We find that our method for semi-supervised training makes it easier and faster to train hard attention networks and correspondingly could make them practical to consider in situations where they were not before.},
}

@inproceedings{HAR-20,
  title={Attention for Inference Compilation},
  author={Harvey, W and Munk, A and Baydin, AG and Bergholm, A and Wood, F},
  booktitle={The second International Conference on Probabilistic Programming (PROBPROG)},
  year={2020},
  archiveprefix = {arXiv},
  eprint = {1910.11961},
  support = {D3M,LwLL},
  url_Paper={https://arxiv.org/pdf/1910.11961.pdf},
  url_ArXiv={https://arxiv.org/abs/1910.11961},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/PROBPROG2020_HAR.pdf},
  abstract = {We present a new approach to automatic amortized inference in universal probabilistic programs which improves performance compared to current methods. Our approach is a variation of inference compilation (IC) which leverages deep neural networks to approximate a posterior distribution over latent variables in a probabilistic program. A challenge with existing IC network architectures is that they can fail to model long-range dependencies between latent variables. To address this, we introduce an attention mechanism that attends to the most salient variables previously sampled in the execution of a probabilistic program. We demonstrate that the addition of attention allows the proposal distributions to better match the true posterior, enhancing inference about latent variables in simulators.},
}

@inproceedings{MUN-20,
  title={Deep probabilistic surrogate networks for universal simulator approximation},
  author={Munk, Andreas and Ścibior, Adam and Baydin, AG and Stewart, A and Fernlund, A and Poursartip, A and Wood, Frank},
  booktitle={The second International Conference on Probabilistic Programming (PROBPROG)},
  year={2020},
  archiveprefix = {arXiv},
  eprint = {1910.11950},
  support = {D3M,ETALUMIS},
  url_Paper={https://arxiv.org/pdf/1910.11950.pdf},
  url_ArXiv={https://arxiv.org/abs/1910.11950},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/PROBPROG2020_MUN.pdf},
  abstract = {We present a framework for automatically structuring and training fast, approximate, deep neural surrogates of existing stochastic simulators. Unlike traditional approaches to surrogate modeling, our surrogates retain the interpretable structure of the reference simulators. The particular way we achieve this allows us to replace the reference simulator with the surrogate when undertaking amortized inference in the probabilistic programming sense. The fidelity and speed of our surrogates allow for not only faster "forward" stochastic simulation but also for accurate and substantially faster inference. We support these claims via experiments that involve a commercial composite-materials curing simulator. Employing our surrogate modeling technique makes inference an order of magnitude faster, opening up the possibility of doing simulator-based, non-invasive, just-in-time parts quality testing; in this case inferring safety-critical latent internal temperature profiles of composite materials undergoing curing from surface temperature profile measurements.},
}

@inproceedings{WEI-19,
  title={Efficient Inference Amortization in Graphical Models using Structured Continuous Conditional Normalizing Flows},
  author={Weilbach, Christian and Beronov, Boyan and Harvey, William and Wood, Frank},
  booktitle={2nd Symposium on Advances in Approximate Bayesian Inference (AABI)},
  support = {D3M},
  url_Link={https://openreview.net/forum?id=BJlhYknNFS},
  url_Paper={https://openreview.net/pdf?id=BJlhYknNFS},
  abstract = {We introduce a more efficient neural architecture for amortized inference, which combines continuous and conditional normalizing flows using a principled choice of structure. Our gradient flow derives its sparsity pattern from the minimally faithful inverse of its underlying graphical model. We find that this factorization reduces the necessary numbers both of parameters in the neural network and of adaptive integration steps in the ODE solver. Consequently, the throughput at training time and inference time is increased, without decreasing performance in comparison to unconstrained flows. By expressing the structural inversion and the flow construction as compilation passes of a probabilistic programming language, we demonstrate their applicability to the stochastic inversion of realistic models such as convolutional neural networks (CNN).},
  year={2019}
}

@inproceedings{BAT-20,
  author = {{Bateni}, Peyman and {Goyal}, Raghav and {Masrani}, Vaden and {Wood}, Frank and {Sigal}, Leonid},
  title = {Improved Few-Shot Visual Classification},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {LwLL, Computer Science - Computer Vision and Pattern Recognition},
  year = {2020},
  eid = {arXiv:1912.03432},
  archivePrefix = {arXiv},
  eprint = {1912.03432},
  support = {D3M,LwLL},
  url_Link = {https://openaccess.thecvf.com/content_CVPR_2020/html/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.html},
  url_Paper={http://openaccess.thecvf.com/content_CVPR_2020/papers/Bateni_Improved_Few-Shot_Visual_Classification_CVPR_2020_paper.pdf},
  url_ArXiv={https://arxiv.org/abs/1912.03432},
  abstract={Few-shot learning is a fundamental task in computer vision that carries the promise of alleviating the need for exhaustively labeled data. Most few-shot learning approaches to date have focused on progressively more complex neural feature extractors and classifier adaptation strategies, as well as the refinement of the task definition itself. In this paper, we explore the hypothesis that a simple class-covariance-based distance metric, namely the Mahalanobis distance, adopted into a state of the art few-shot learning approach (CNAPS) can, in and of itself, lead to a significant performance improvement. We also discover that it is possible to learn adaptive feature extractors that allow useful estimation of the high dimensional feature covariances required by this metric from surprisingly few samples. The result of our work is a new "Simple CNAPS" architecture which has up to 9.2% fewer trainable parameters than CNAPS and performs up to 6.1% better than state of the art on the standard few-shot image classification benchmark dataset.}
}

%@inproceedings{WAN-19,
%  title={Safer End-to-End Autonomous Driving via Conditional Imitation Learning and Command Augmentation},
%  author={Wang, R and Scibior, A and Wood F},
%  booktitle={NeurIPS self-driving car workshop},
%  year={2019},
%  archiveprefix = {arXiv},
%  eprint = {1909.09721},
%  support = {D3M},
%  url_Paper = {https://arxiv.org/pdf/1909.09721.pdf},
%  url_ArXiv={https://arxiv.org/abs/1909.09721},
%  abstract={Imitation learning is a promising approach to end-to-end training of autonomous vehicle controllers. Typically the driving process with such approaches is entirely automatic and black-box, although in practice it is desirable to control the vehicle through high-level commands, such as telling it which way to go at an intersection. In existing work this has been accomplished by the application of a branched neural architecture, since directly providing the command as an additional input to the controller often results in the command being ignored. In this work we overcome this limitation by learning a disentangled probabilistic latent variable model that generates the steering commands. We achieve faithful command-conditional generation without using a branched architecture and demonstrate improved stability of the controller, applying only a variational objective without any domain-specific adjustments. On top of that, we extend our model with an additional latent variable and augment the dataset to train a controller that is robust to unsafe commands, such as asking it to turn into a wall. The main contribution of this work is a recipe for building controllable imitation driving agents that improves upon multiple aspects of the current state of the art relating to robustness and interpretability.}
%}

@inproceedings{CAM-19,
  title={Sparse Variational Inference: Bayesian Coresets from Scratch},
  author={Campbell, Trevor and Beronov, Boyan},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  pages={11457--11468},
  year={2019},
  eid = {arXiv:1906.03329},
  archivePrefix = {arXiv},
  eprint = {1906.03329},
  support = {D3M},
  url_Link={http://papers.nips.cc/paper/9322-sparse-variational-inference-bayesian-coresets-from-scratch},
  url_Paper={http://papers.nips.cc/paper/9322-sparse-variational-inference-bayesian-coresets-from-scratch.pdf},
  url_Poster={https://github.com/plai-group/bibliography/raw/master/presentations_posters/CAM-19.pdf},
  bibbase_note={1st prize, Student poster competition, AICan (Annual Meeting, Pan-Canadian AI Strategy, Canadian Institute for Advanced Research). Vancouver, Canada, Dec. 9, 2019},
    abstract={The proliferation of automated inference algorithms in Bayesian statistics has provided practitioners newfound access to fast, reproducible data analysis and powerful statistical models. Designing automated methods that are also both computationally scalable and theoretically sound, however, remains a significant challenge. Recent work on Bayesian coresets takes the approach of compressing the dataset before running a standard inference algorithm, providing both scalability and guarantees on posterior approximation error. But the automation of past coreset methods is limited because they depend on the availability of a reasonable coarse posterior approximation, which is difficult to specify in practice. In the present work we remove this requirement by formulating coreset construction as sparsity-constrained variational inference within an exponential family. This perspective leads to a novel construction via greedy optimization, and also provides a unifying information-geometric view of present and past methods. The proposed Riemannian coreset construction algorithm is fully automated, requiring no problem-specific inputs aside from the probabilistic model and dataset. In addition to being significantly easier to use than past methods, experiments demonstrate that past coreset constructions are fundamentally limited by the fixed coarse posterior approximation; in contrast, the proposed algorithm is able to continually improve the coreset, providing state-of-the-art Bayesian dataset summarization with orders-of-magnitude reduction in KL divergence to the exact posterior.}
}

@inproceedings{GRA-19,
  title={Efficient Bayesian Inference for Nested Simulators},
  author={Gram-Hansen, B and Schroeder de Witt, C and Zinkov, R and Naderiparizi, S and Scibior, A and Munk, A and Wood, F and Ghadiri, M and Torr, P and Whye Teh, Y and Gunes Baydin, A and Rainforth, T},
  booktitle={2nd Symposium on Advances in Approximate Bayesian Inference (AABI)},
  year={2019},
  support = {D3M},
  url_Link={https://openreview.net/forum?id=rJeMcy2EtH},
  url_Paper={https://openreview.net/pdf?id=rJeMcy2EtH},
  abstact={We introduce two approaches for conducting efficient Bayesian inference in stochastic simulators containing nested stochastic sub-procedures, i.e., internal procedures for which the density cannot be calculated directly such as rejection sampling loops. The resulting class of simulators are used extensively throughout the sciences and can be interpreted as probabilistic generative models. However, drawing inferences from them poses a substantial challenge due to the inability to evaluate even their unnormalised density, preventing the use of many standard inference procedures like Markov Chain Monte Carlo (MCMC). To address this, we introduce inference algorithms based on a two-step approach that first approximates the conditional densities of the individual sub-procedures, before using these approximations to run MCMC methods on the full program. Because the sub-procedures can be dealt with separately and are lower-dimensional than that of the overall problem, this two-step process allows them to be isolated and thus be tractably dealt with, without placing restrictions on the overall dimensionality of the problem. We demonstrate the utility of our approach on a simple, artificially constructed simulator.}
}

@unpublished{NAD-19,
  title={Amortized rejection sampling in universal probabilistic programming},
  author={Naderiparizi, S and Ścibior, A and Munk, A and Ghadiri, M and Baydin, AG and Gram-Hansen, B and Schroeder de Witt, C and Zinkov, R and Torr, PHS and Rainforth, T and Whye Teh, Y and Wood, F},
  year={2019},
  archiveprefix = {arXiv},
  eprint = {1910.09056},
  support = {D3M,ETALUMIS},
  url_Paper={https://arxiv.org/pdf/1910.09056.pdf},
  url_ArXiv={https://arxiv.org/abs/1910.09056},
  abstract={Existing approaches to amortized inference in probabilistic programs with unbounded loops can produce estimators with infinite variance. An instance of this is importance sampling inference in programs that explicitly include rejection sampling as part of the user-programmed generative procedure. In this paper we develop a new and efficient amortized importance sampling estimator. We prove finite variance of our estimator and empirically demonstrate our method's correctness and efficiency compared to existing alternatives on generative programs containing rejection sampling loops and discuss how to implement our method in a generic probabilistic programming framework.}
}


@unpublished{TEN-19,
  title={Imitation Learning of Factored Multi-agent Reactive Models},
  author={Teng, Michael and Le, Tuan Anh and Scibior, Adam and Wood, Frank},
  archiveprefix = {arXiv},
  eprint = {1903.04714},
  year={2019},
  url_Paper={https://arxiv.org/pdf/1903.04714.pdf},
  url_ArXiv={https://arxiv.org/abs/1903.04714},
  support = {D3M},
  abstract={We apply recent advances in deep generative modeling to the task of imitation learning from biological agents. Specifically, we apply variations of the variational recurrent neural network model to a multi-agent setting where we learn policies of individual uncoordinated agents acting based on their perceptual inputs and their hidden belief state. We learn stochastic policies for these agents directly from observational data, without constructing a reward function. An inference network learned jointly with the policy allows for efficient inference over the agent's belief state given a sequence of its current perceptual inputs and the prior actions it performed, which lets us extrapolate observed sequences of behavior into the future while maintaining uncertainty estimates over future trajectories. We test our approach on a dataset of flies interacting in a 2D environment, where we demonstrate better predictive performance than existing approaches which learn deterministic policies with recurrent neural networks. We further show that the uncertainty estimates over future trajectories we obtain are well calibrated, which makes them useful for a variety of downstream processing tasks.},
}

@inproceedings{MAS-19,
  title={The Thermodynamic Variational Objective},
  author={Masrani, Vaden and Le, Tuan Anh and Wood, Frank},
  booktitle={Thirty-third Conference on Neural Information Processing Systems (NeurIPS)},
  archiveprefix = {arXiv},
  eprint = {1907.00031},
  url_Paper={https://arxiv.org/pdf/1907.00031.pdf},
  url_ArXiv={https://arxiv.org/abs/1907.00031},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/neurips_tvo_poster.pdf},
  support = {D3M},
  abstract={We introduce the thermodynamic variational objective (TVO) for learning in both continuous and discrete deep generative models. The TVO arises from a key connection between variational inference and thermodynamic integration that results in a tighter lower bound to the log marginal likelihood than the standard variational variational evidence lower bound (ELBO) while remaining as broadly applicable. We provide a computationally efficient gradient estimator for the TVO that applies to continuous, discrete, and non-reparameterizable distributions and show that the objective functions used in variational inference, variational autoencoders, wake sleep, and inference compilation are all special cases of the TVO. We use the TVO to learn both discrete and continuous deep generative models and empirically demonstrate state of the art model and inference network learning.},
  year={2019}
}


@inproceedings{BAY-19,
  title={Etalumis: Bringing Probabilistic Programming to Scientific Simulators at Scale},
  author={Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Shao, Lei and Bhimji, Wahid and Heinrich, Lukas and Meadows, Lawrence and Liu, Jialin and Munk, Andreas and Naderiparizi, Saeid and Gram-Hansen, Bradley and Louppe, Gilles and others},
  booktitle={the International Conference for High Performance Computing, Networking, Storage and Analysis (SC ’19)},
  archiveprefix = {arXiv},
  eprint = {1907.03382},
  support = {D3M,ETALUMIS},
  url_Paper={https://arxiv.org/pdf/1907.03382.pdf},
  url_ArXiv={https://arxiv.org/abs/1907.03382},
  abstract={Probabilistic programming languages (PPLs) are receiving widespread attention for performing Bayesian inference in complex generative models. However, applications to science remain limited because of the impracticability of rewriting complex scientific simulators in a PPL, the computational cost of inference, and the lack of scalable implementations. To address these, we present a novel PPL framework that couples directly to existing scientific simulators through a cross-platform probabilistic execution protocol and provides Markov chain Monte Carlo (MCMC) and deep-learning-based inference compilation (IC) engines for tractable inference. To guide IC inference, we perform distributed training of a dynamic 3DCNN--LSTM architecture with a PyTorch-MPI-based framework on 1,024 32-core CPU nodes of the Cori supercomputer with a global minibatch size of 128k: achieving a performance of 450 Tflop/s through enhancements to PyTorch. We demonstrate a Large Hadron Collider (LHC) use-case with the C++ Sherpa simulator and achieve the largest-scale posterior inference in a Turing-complete PPL.},
  year={2019},
  doi={10.1145/3295500.3356180}
}

@inproceedings{WAR-19,
  title={The Virtual Patch Clamp: Imputing C. elegans Membrane Potentials from Calcium Imaging},
  author={Warrington, Andrew and Spencer, Arthur and Wood, Frank},
  booktitle={NeurIPS 2019 Workshop Neuro AI},
  archiveprefix = {arXiv},
  eprint = {1907.11075},
  support = {D3M},
  url_Paper={https://arxiv.org/pdf/1907.11075.pdf},
  url_ArXiv={https://arxiv.org/abs/1907.11075},
  url_Poster={https://github.com/plai-group/bibliography/blob/master/presentations_posters/WAR-19.pdf},
  abstract={We develop a stochastic whole-brain and body simulator of the nematode roundworm Caenorhabditis elegans (C. elegans) and show that it is sufficiently regularizing to allow imputation of latent membrane potentials from partial calcium fluorescence imaging observations. This is the first attempt we know of to "complete the circle," where an anatomically grounded whole-connectome simulator is used to impute a time-varying "brain" state at single-cell fidelity from covariates that are measurable in practice. The sequential Monte Carlo (SMC) method we employ not only enables imputation of said latent states but also presents a strategy for learning simulator parameters via variational optimization of the noisy model evidence approximation provided by SMC. Our imputation and parameter estimation experiments were conducted on distributed systems using novel implementations of the aforementioned techniques applied to synthetic data of dimension and type representative of that which are measured in laboratories currently.},
  year={2019}
}

@inproceedings{GOL-19,
  title={Amortized Monte Carlo Integration},
  author={Goli{\'n}ski, Adam and Wood, Frank and Rainforth, Tom},
  booktitle={Proceedings of the International Conference on Machine Learning (ICML)},
  year={2019},
  archiveprefix = {arXiv},
  eprint = {1907.08082},
  url_Paper={https://arxiv.org/pdf/1907.08082.pdf},
  url_ArXiv={https://arxiv.org/abs/1907.08082},
  url_Presentation={https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4702},
  support = {D3M},
  abstract={Current approaches to amortizing Bayesian inference focus solely on approximating the posterior distribution. Typically, this approximation is, in turn, used to calculate expectations for one or more target functions - a computational pipeline which is inefficient when the target function(s) are known upfront. In this paper, we address this inefficiency by introducing AMCI, a method for amortizing Monte Carlo integration directly. AMCI operates similarly to amortized inference but produces three distinct amortized proposals, each tailored to a different component of the overall expectation calculation. At runtime, samples are produced separately from each amortized proposal, before being combined to an overall estimate of the expectation. We show that while existing approaches are fundamentally limited in the level of accuracy they can achieve, AMCI can theoretically produce arbitrarily small errors for any integrable target function using only a single sample from each proposal at runtime. We further show that it is able to empirically outperform the theoretically optimal self-normalized importance sampler on a number of example problems. Furthermore, AMCI allows not only for amortizing over datasets but also amortizing over target functions.}
}


@inproceedings{ZHO-19,
  title={{LF-PPL}: A Low-Level First Order Probabilistic Programming Language for Non-Differentiable Models},
  author={Zhou, Yuan and Gram-Hansen, Bradley J and Kohn, Tobias and Rainforth, Tom and Yang, Hongseok and Wood, Frank},
  booktitle={Proceedings of the Twentieth International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2019},
  archiveprefix = {arXiv},
  eprint = {1903.02482},
  support = {D3M},
  url_Paper={https://arxiv.org/pdf/1903.02482.pdf},
  url_ArXiv={https://arxiv.org/abs/1903.02482},
  abstract={We develop a new Low-level, First-order Probabilistic Programming Language (LF-PPL) suited for models containing a mix of continuous, discrete, and/or piecewise-continuous variables. The key success of this language and its compilation scheme is in its ability to automatically distinguish parameters the density function is discontinuous with respect to, while further providing runtime checks for boundary crossings. This enables the introduction of new inference engines that are able to exploit gradient information, while remaining efficient for models which are not everywhere differentiable. We demonstrate this ability by incorporating a discontinuous Hamiltonian Monte Carlo (DHMC) inference engine that is able to deliver automated and efficient inference for non-differentiable models. Our system is backed up by a mathematical formalism that ensures that any model expressed in this language has a density with measure zero discontinuities to maintain the validity of the inference engine.}
}

@inproceedings{LE-19,
  title={Revisiting Reweighted Wake-Sleep for Models with Stochastic Control Flow},
  author={Le, Tuan Anh and Kosiorek, Adam R and Siddharth, N and Teh, Yee Whye and Wood, Frank},
  year={2019},
  booktitle={Proceedings of the conference on Uncertainty in Artificial Intelligence (UAI)},
  archiveprefix = {arXiv},
  eprint = {1805.10469},
  support = {D3M},
  url_Link={https://arxiv.org/abs/1805.10469},
  url_Paper={https://arxiv.org/pdf/1805.10469.pdf},
  abstract={Stochastic control-flow models (SCFMs) are a class of generative models that involve branching on choices from discrete random variables. Amortized gradient-based learning of SCFMs is challenging as most approaches targeting discrete variables rely on their continuous relaxations---which can be intractable in SCFMs, as branching on relaxations requires evaluating all (exponentially many) branching paths. Tractable alternatives mainly combine REINFORCE with complex control-variate schemes to improve the variance of naive estimators. Here, we revisit the reweighted wake-sleep (RWS) (Bornschein and Bengio, 2015) algorithm, and through extensive evaluations, show that it outperforms current state-of-the-art methods in learning SCFMs. Further, in contrast to the importance weighted autoencoder, we observe that RWS learns better models and inference networks with increasing numbers of particles. Our results suggest that RWS is a competitive, often preferable, alternative for learning SCFMs.}
}

@inproceedings{TEN-18,
  title={Bayesian Distributed Stochastic Gradient Descent},
  author={Teng, Michael and Wood, Frank},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={6378--6388},
  year={2018},
  url_Link={https://papers.nips.cc/paper/7874-bayesian-distributed-stochastic-gradient-descent},
  url_Paper={https://papers.nips.cc/paper/7874-bayesian-distributed-stochastic-gradient-descent.pdf},
  abstract={We introduce Bayesian distributed stochastic gradient descent (BDSGD), a high-throughput algorithm for training deep neural networks on parallel clusters. This algorithm uses amortized inference in a deep generative model to perform joint posterior predictive inference of mini-batch gradient computation times in a compute cluster specific manner. Specifically, our algorithm mitigates the straggler effect in synchronous, gradient-based optimization by choosing an optimal cutoff beyond which mini-batch gradient messages from slow workers are ignored. In our experiments, we show that eagerly discarding the mini-batch gradient computations of stragglers not only increases throughput but actually increases the overall rate of convergence as a function of wall-clock time by virtue of eliminating idleness. The principal novel contribution and finding of this work goes beyond this by demonstrating that using the predicted run-times from a generative model of cluster worker performance improves substantially over the static-cutoff prior art, leading to reduced deep neural net training times on large computer clusters.}
}

@inproceedings{WEB-18,
  title={Faithful inversion of generative models for effective amortized inference},
  author={Webb, Stefan and Golinski, Adam and Zinkov, Rob and Narayanaswamy, Siddharth and Rainforth, Tom and Teh, Yee Whye and Wood, Frank},
  booktitle={Advances in Neural Information Processing Systems 31},
  pages={3070--3080},
  year={2018},
  archiveprefix = {arXiv},
  eprint = {1712.00287},
  url_Paper={https://arxiv.org/pdf/1712.00287.pdf},
  url_ArXiv={https://arxiv.org/abs/1712.00287},
  abstract={Inference amortization methods share information across multiple posterior-inference problems, allowing each to be carried out more efficiently. Generally, they require the inversion of the dependency structure in the generative model, as the modeller must learn a mapping from observations to distributions approximating the posterior. Previous approaches have involved inverting the dependency structure in a heuristic way that fails to capture these dependencies correctly, thereby limiting the achievable accuracy of the resulting approximations. We introduce an algorithm for faithfully, and minimally, inverting the graphical model structure of any generative model. Such inverses have two crucial properties: (a) they do not encode any independence assertions that are absent from the model and; (b) they are local maxima for the number of true independencies encoded. We prove the correctness of our approach and empirically show that the resulting minimally faithful inverses lead to better inference amortization than existing heuristic approaches.}
}


@inproceedings{RAI-18a,
  title={On Nesting Monte Carlo Estimators},
  author={Rainforth, Tom and Cornish, Robert and Yang, Hongseok and Warrington, Andrew and Wood, Frank},
  booktitle={Thirty-fifth International Conference on Machine Learning (ICML)},
  year={2018},
  archiveprefix = {arXiv},
  eprint = {1709.06181},
  url_Paper={https://arxiv.org/pdf/1709.06181.pdf},
  url_ArXiv={https://arxiv.org/abs/1709.06181},
  abstract={Many problems in machine learning and statistics involve nested expectations and thus do not permit conventional Monte Carlo (MC) estimation. For such problems, one must nest estimators, such that terms in an outer estimator themselves involve calculation of a separate, nested, estimation. We investigate the statistical implications of nesting MC estimators, including cases of multiple levels of nesting, and establish the conditions under which they converge. We derive corresponding rates of convergence and provide empirical evidence that these rates are observed in practice. We further establish a number of pitfalls that can arise from naive nesting of MC estimators, provide guidelines about how these can be avoided, and lay out novel methods for reformulating certain classes of nested expectation problems into single expectations, leading to improved convergence rates. We demonstrate the applicability of our work by using our results to develop a new estimator for discrete Bayesian experimental design problems and derive error bounds for a class of variational objectives.}
}

@inproceedings{RAI-18b,
  title={Tighter variational bounds are not necessarily better},
  author={Rainforth, Tom and Kosiorek, Adam R and Le, Tuan Anh and Maddison, Chris J and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  booktitle={Thirty-fifth International Conference on Machine Learning (ICML)},
  year={2018},
  archiveprefix = {arXiv},
  eprint = {1802.04537},
  url_Link={https://arxiv.org/abs/1802.04537},
  url_Paper={https://arxiv.org/pdf/1802.04537.pdf},
  abstract={We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted auto-encoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.}
}

@inproceedings{IGL-18,
  title={Deep Variational Reinforcement Learning for POMDPs},
  author={Igl, Maximilian and Zintgraf, Luisa and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
  booktitle={Thirty-fifth International Conference on Machine Learning (ICML)},
  year={2018},
  archiveprefix = {arXiv},
  eprint = {1806.02426},
  url_Link={https://arxiv.org/abs/1806.02426},
  url_Paper={https://arxiv.org/pdf/1806.02426.pdf},
  abstract={Many real-world sequential decision making problems are partially observable by nature, and the environment model is typically unknown. Consequently, there is great need for reinforcement learning methods that can tackle such problems given only a stream of incomplete and noisy observations. In this paper, we propose deep variational reinforcement learning (DVRL), which introduces an inductive bias that allows an agent to learn a generative model of the environment and perform inference in that model to effectively aggregate the available information. We develop an n-step approximation to the evidence lower bound (ELBO), allowing the model to be trained jointly with the policy. This ensures that the latent state representation is suitable for the control task. In experiments on Mountain Hike and flickering Atari we show that our method outperforms previous approaches relying on recurrent neural networks to encode the past.}
}

@unpublished{MEE-18,
  title={An introduction to probabilistic programming},
  author={van de Meent, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  journal={arXiv preprint},
  archiveprefix = {arXiv},
  eprint = {1809.10756},
  year={2018},
  url_Link={https://arxiv.org/abs/1809.10756},
  url_Paper={https://arxiv.org/pdf/1809.10756.pdf},
  abstract={This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages.\\
  We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs.\\
  In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller.\\
  This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.}
}

@inproceedings{BAY-19a,
  title={Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model},
  author={Baydin, Atilim Gunes and Heinrich, Lukas and Bhimji, Wahid and Gram-Hansen, Bradley and Louppe, Gilles and Shao, Lei and Cranmer, Kyle and Wood, Frank and others},
  booktitle={Thirty-second Conference on Neural Information Processing Systems (NeurIPS)},
  archiveprefix = {arXiv},
  eprint = {1807.07706},
  year={2019},
  url_Link={https://papers.nips.cc/paper/8785-efficient-probabilistic-inference-in-the-quest-for-physics-beyond-the-standard-model},
  url_Link={https://arxiv.org/abs/1807.07706},
  url_Paper={https://papers.nips.cc/paper/8785-efficient-probabilistic-inference-in-the-quest-for-physics-beyond-the-standard-model.pdf},
  url_Paper={https://arxiv.org/pdf/1807.07706.pdf},
  abstract={We present a novel probabilistic programming framework that couples directly to existing large-scale simulators through a cross-platform probabilistic execution protocol, which allows general-purpose inference engines to record and control random number draws within simulators in a language-agnostic way. The execution of existing simulators as probabilistic programs enables highly interpretable posterior inference in the structured model defined by the simulator code base. We demonstrate the technique in particle physics, on a scientifically accurate simulation of the tau lepton decay, which is a key ingredient in establishing the properties of the Higgs boson. Inference efficiency is achieved via inference compilation where a deep recurrent neural network is trained to parameterize proposal distributions and control the stochastic simulator in a sequential importance sampling scheme, at a fraction of the computational cost of a Markov chain Monte Carlo baseline.}
}


@inproceedings{RAI-18c,
  title={Inference trees: Adaptive inference with exploration},
  author={Rainforth, Tom and Zhou, Yuan and Lu, Xiaoyu and Teh, Yee Whye and Wood, Frank and Yang, Hongseok and van de Meent, Jan-Willem},
  booktitle={1st Symposium on Advances in Approximate Bayesian Inference},
  archiveprefix = {arXiv},
  eprint = {1806.09550},
  year={2018},
  url_Link={https://arxiv.org/abs/1806.09550},
  url_Paper={https://arxiv.org/pdf/1806.09550.pdf},
  url_Presentation={http://www.approximateinference.org/2018/schedule/Rainforth2018.pdf},
  abstract={We introduce inference trees (ITs), a new class of inference methods that build on ideas from Monte Carlo tree search to perform adaptive sampling in a manner that balances exploration with exploitation, ensures consistency, and alleviates pathologies in existing adaptive methods. ITs adaptively sample from hierarchical partitions of the parameter space, while simultaneously learning these partitions in an online manner. This enables ITs to not only identify regions of high posterior mass, but also maintain uncertainty estimates to track regions where significant posterior mass may have been missed. ITs can be based on any inference method that provides a consistent estimate of the marginal likelihood. They are particularly effective when combined with sequential Monte Carlo, where they capture long-range dependencies and yield improvements beyond proposal adaptation alone.}
}

@unpublished{COR-18,
  title={Towards a Testable Notion of Generalization for Generative Adversarial Networks},
  author={Cornish, Robert and Yang, Hongseok and Wood, Frank},
  year={2018},
  }

@inproceedings{BAY-18a,
  title={Online learning rate adaptation with hypergradient descent},
  author={Baydin, Atilim Gunes and Cornish, Robert and Rubio, David Martinez and Schmidt, Mark and Wood, Frank},
  booktitle={Sixth International Conference on Learning Representations (ICLR 2018)},
  archiveprefix = {arXiv},
  eprint = {1703.04782},
  year={2018},
  url_Link={https://arxiv.org/abs/1703.04782},
  url_Paper={https://arxiv.org/pdf/1703.04782.pdf},
  url_Link={https://iclr.cc/Conferences/2018/Schedule?showEvent=14},
  url_Paper={https://openreview.net/pdf?id=BkrsAzWAb},
  abstract={We introduce a general method for improving the convergence rate of gradient-based optimizers that is easy to implement and works well in practice.  We demonstrate the effectiveness of the method in a range of optimization problems by applying it to stochastic gradient descent, stochastic gradient descent with Nesterov momentum, and Adam, showing that it significantly reduces the need for the manual tuning of the initial learning rate for these commonly used algorithms.  Our method works by dynamically updating the learning rate during optimization using the gradient with respect to the learning rate of the update rule itself.  Computing this "hypergradient" needs little additional computation, requires only one extra copy of the original gradient to be stored in memory, and relies upon nothing more than what is provided by reverse-mode automatic differentiation.}
}

inproceedings{iffsidnips2017,
  title={Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  author={N. Siddarth and B. Paige and A. Desmaison and J.W. van~de~Meent and N. Goodman and P. Kohli and F. Wood and P.H.S Torr},
  booktitle ={NIPS},
  year={2017}
}

article{milutinovic2017end,
  title={End-to-end Training of Differentiable Pipelines Across Machine Learning Frameworks},
  author={Milutinovic, Mitar and Baydin, At{\i}l{\i}m G{\"u}ne{\c{s}} and Zinkov, Robert and Harvey, William and Song, Dawn and Wood, Frank and Shen, Wade},
  year={2017}
}

article{warrington2017updating,
  title={Updating the VESICLE-CNN Synapse Detector},
  author={Warrington, Andrew and Wood, Frank},
  journal={arXiv preprint arXiv:1710.11397},
  year={2017}
}

article{casado2017improvements,
  title={Improvements to Inference Compilation for Probabilistic Programming in Large-Scale Scientific Simulators},
  author={Casado, Mario Lezcano and Baydin, Atilim Gunes and Rubio, David Mart{\'\i}nez and Le, Tuan Anh and Wood, Frank and Heinrich, Lukas and Louppe, Gilles and Cranmer, Kyle and Ng, Karen and Bhimji, Wahid and others},
  journal={arXiv preprint arXiv:1712.07901},
  year={2017}
}

inproceedings{iffsidnips2017,
  title={Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  author={N. Siddarth and B. Paige and A. Desmaison and J.W. van~de~Meent and F. Wood and N. Goodman and P. Kohli and P.H.S Torr},
  booktitle ={NeurIPS},
  year={2017}
}

article{Caron-2017-JMLR,
  title={Generalized {P}\'olya Urn for Time-Varying {P}itman-{Y}or Processes},
  author={Francois Caron and Willie Neiswanger and Frank Wood and Arnaud Doucet and Manuel Davy},
  journal={JMLR},
  pages={1--32},
  year={2017}
}

inproceedings{le2016synthetic,
  author = {Le, Tuan Anh and Baydin, AtÄ±lÄ±m GÃ¼neÅŸ and Zinkov, Robert and Wood, Frank},
  booktitle = {30th International Joint Conference on Neural Networks, May 14--19, 2017, Anchorage, AK, USA},
  title = {Using Synthetic Data to Train Neural Networks is Model-Based Reasoning},
  year = {2017}
}

inproceedings{le2016inference,
  author = {Le, Tuan Anh and Baydin, AtÄ±lÄ±m GÃ¼neÅŸ and Wood, Frank},
  booktitle = {20th International Conference on Artificial Intelligence and Statistics, April 20--22, 2017, Fort Lauderdale, FL, USA},
  title = {Inference {C}ompilation and {U}niversal {P}robabilistic {P}rogramming},
  year = {2017},
  link = {https://arxiv.org/abs/1610.09900}
}

inproceedings{webbnips2017workshop,
  title={Principled Inference Networks in Deep Generative Models},
  author={Stefan Webb, Adam Golinski, Robert Zinkov and Frank Wood},
  booktitle={NIPS Workshop on Bayesian Deep Learning},
  year={2017}
}

inproceedings{le2016inference,
  author = {Le, Tuan Anh and Baydin, Atılım Güneş and Wood, Frank},
  booktitle = {AISTATS},
  title = {Inference {C}ompilation and {U}niversal {P}robabilistic {P}rogramming},
  year = {2017},
  link = {https://arxiv.org/abs/1610.09900}
}

inproceedings{staton2016semanticslics,
  title={Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints},
  author={Staton, S. and Yang, H. and Heunen, C. and Kammar, O. and Wood, F.},
  Booktitle={Thirty-First Annual ACM/IEEE Symposium on Logic In Computer Science},
  year={2016}
}

inproceedings{perov-agi-2016,
  Author = {Perov, Y. and Wood, F.},
  Booktitle = {Artificial General Intelligence},
  Title = {Automatic Sampler Discovery via Probabilistic Programming and Approximate {B}ayesian Computation},
  Year = {2016}
}

inproceedings{paige2016supersampling,
  author={Paige, Brooks and Sejdinovic, Dino and Wood, Frank},
  title={Super-sampling with Reservoir},
  booktitle={Proceedings of the 32nd Annual Conference on Uncertainty in Artificial Intelligence (UAI-2016)},
  publisher={{AUAI Press}},
  year={2016}
}

inproceedings{paige2016inference,
  title={Inference Networks for Sequential {M}onte {C}arlo in Graphical Models},
  author={Paige, B. and Wood, F.},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning},
  series={JMLR},
  volume = {48},
  year={2016}
}

inproceedings{rainforth2016interacting,
  title={Interacting Particle {M}arkov Chain {M}onte {C}arlo},
  author={Rainforth, T. and Naesseth, C.A. and Lindsten, F. and Paige, B. and van de Meent, J.W. and Doucet, A. and Wood, F.},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning},
  series={JMLR},
  volume = {48},
  year={2016}
}

inproceedings{rainforth-nips-2016,
  title={{B}ayesian {O}ptimization for {P}robabilistic {P}rograms},
  author={Rainforth, Tom and Le, Tuan Anh and van de Meent, Jan-Willem and Osborne, Michael A and Wood, Frank},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2016},
  pages = {280--288}
}

inproceedings{Dhir-IROS-2016,
  author  = "Neil Dhir and Yura Perov and Frank Wood",
  title   = "Nonparametric Bayesian Models for Unsupervised Activity Recognition and Tracking",
  booktitle = "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016)",
  year = "2016"
}

inproceedings{staton2016semanticslics,
  title={Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints},
  author={Staton, S. and Yang, H. and Heunen, C. and Kammar, O. and Wood, F.},
  Booktitle={Thirty-First Annual ACM/IEEE Symposium on Logic In Computer Science},
  year={2016}
}

inproceedings{perov-agi-2016,
  Author = {Perov, Y. and Wood, F.},
  Booktitle = {Artificial General Intelligence},
  Title = {Automatic Sampler Discovery via Probabilistic Programming and Approximate {B}ayesian Computation},
  Pages = {262--273},
  Year = {2016}
}


inproceedings{paige2016supersampling,
  author={Paige, Brooks and Sejdinovic, Dino and Wood, Frank},
  title={Super-sampling with Reservoir},
  booktitle={Proceedings of the 32nd Annual Conference on Uncertainty in Artificial Intelligence (UAI-2016)},
  publisher={{AUAI Press}},
  year={2016}
}

inproceedings{paige2016inference,
  title={Inference Networks for Sequential {M}onte {C}arlo in Graphical Models},
  author={Paige, B. and Wood, F.},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning},
  series={JMLR},
  volume = {48},
  year={2016}
}

inproceedings{rainforth2016interacting,
  title={Interacting Particle {M}arkov Chain {M}onte {C}arlo},
  author={Rainforth, T. and Naesseth, C.A. and Lindsten, F. and Paige, B. and van de Meent, J.W. and Doucet, A. and Wood, F.},
  booktitle={Proceedings of the 33rd International Conference on Machine Learning},
  series={JMLR},
  volume = {48},
  year={2016}
}


inproceedings{vandemeent16,
  Author = {J.~W. van de Meent and D. Tolpin and B. Paige and F. Wood },
  Booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics},
  Title = {Black-Box Policy Search with Probabilistic Programs},
  pages={1195--1204},
  Year = {2016}
}

inproceedings{staton2016semanticslics,
  title={Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints},
  author={Staton, S. and Yang, H. and Heunen, C. and Kammar, O. and Wood, F.},
  Booktitle={Thirty-First Annual ACM/IEEE Symposium on Logic In Computer Science},
  year={2016}
}

inproceedings{perov-agi-2016,
  Author = {Perov, Y. and Wood, F.},
  Booktitle = {Artificial General Intelligence},
  Title = {Automatic Sampler Discovery via Probabilistic Programming and Approximate {B}ayesian Computation},
  Year = {2016}
}

article{rainforth2016nestedmc,
  title = {On the {P}itfalls of {N}ested {M}onte {C}arlo},
  author = {Rainforth, Tom and Cornish, Rob and Yang, Hongseok and Wood, Frank},
  year={2016},
  journal={NIPS Workshop on Advances in Approximate Bayesian Inference}
}

article{janz2016probstruct,
  title = {Probabilistic {S}tructure {D}iscovery in {T}ime {S}eries {D}ata},
  author = {Janz, David and Paige, Brooks and Rainforth, Tom and van de Meent, Jan-Willem and Wood, Frank},
  year = {2016},
  journal={NIPS Workshop on Artificial Intelligence for Data Science}
}

ARTICLE{10.3389/conf.fninf.2016.20.00046,
 AUTHOR={Warrington, Andrew  and  Wood, Frank},
 TITLE={Algorithmic Optimisation of Neuron Generator Parameters},
 JOURNAL={Frontiers in Neuroinformatics},
 VOLUME={},
 YEAR={2016},
 NUMBER={46},
 URL={http://www.frontiersin.org/neuroinformatics/10.3389/conf.fninf.2016.20.00046/full},
 DOI={10.3389/conf.fninf.2016.20.00046},
 ISSN={1662-5196} ,
 ABSTRACT={}
}

inproceedings{vandemeent-PPS-2016,
  Author = {van de Meent, Jan-Willem and Paige, T.~Brooks and Tolpin, David and Wood, Frank},
  Booktitle = {POPL Workshop on Probabilistic Programming Semantics},
  Title = {An Interface for Black Box Learning in Probabilistic Programs},
  Year = {2016}
}

inproceedings{staton-PPS-2016,
  Author = {Sam Staton and Hongseok Yang amd Chris Heunen and Ohad Kammar and Frank Wood},
  Booktitle = {POPL Workshop on Probabilistic Programming Semantics},
  Title = {Semantics of Higher-order Probabilistic Programs},
  Year = {2016}
}


inproceedings{vandemeent16,
  Author = {J.~W. van de Meent and D. Tolpin and B. Paige and F. Wood },
  Booktitle = {Proceedings of the Nineteenth International Conference on Artificial Intelligence and Statistics},
  Title = {Black-Box Policy Search with Probabilistic Programs},
  Year = {2016}
}

inproceedings{siddharth2016interpretable,
  author = {Siddharth, N. and Paige, Brooks and Desmaison, Alban and van de Meent,Jan-Willem and Wood, Frank and Goodman, Noah D. and Kohli, Pushmeet and Torr, Philip H.S.},
  booktitle = {NIPS Workshop on Interpretable ML for Complex Systems},
  title = {Inducing Interpretable Representations with Variational Autoencoders},
  year = {2016}
}

inproceedings{le2016nested,
  author = {Le, Tuan Anh and Baydin, Atılım Güneş and Wood, Frank},
  booktitle = {NIPS Workshop on Bayesian Deep Learning},
  title = {Nested Compiled Inference for Hierarchical Reinforcement Learning},
  year = {2016}
}

inproceedings{WarringtonWood-NEURO-OPT-NIPS-2016, 
  author = {Warrington, Andrew and and Wood, Frank}, 
  booktitle = {NIPS Workshop on Connectomics II: Opportunities and Challenges for Machine Learning}, 
  title = {Optimizing Neuron Generation Model Parameters for Top-Down Segmentation Regularization}, 
  year = {2016}
}

article{le2016inference,
  title={Inference Compilation and Universal Probabilistic Programming},
  author={Le, Tuan Anh and Baydin, Atılım Güneş and Wood, Frank},
  journal={arXiv preprint arXiv:1610.09900},
  year={2016}
}

article{paige2016inferencenetworks,
  title={Inference Networks for Sequential Monte Carlo in Graphical Models},
  author={Paige, Brooks and Wood, Frank},
  journal={arXiv preprint arXiv:1602.06701},
  year={2016}
}

article{rainforth2016interacting,
  title={Interacting Particle Markov Chain Monte Carlo},
  author={Rainforth, Tom and Naesseth, Christian A and Lindsten, Fredrik and Paige, Brooks and van de Meent, Jan-Willem and Doucet, Arnaud and Wood, Frank},
  journal={arXiv preprint arXiv:1602.05128},
  year={2016}
}

article{perov2015data,
  title={Data-driven Sequential Monte Carlo in Probabilistic Programming},
  author={Perov, Yura N and Le, Tuan Anh and Wood, Frank},
  journal={arXiv preprint arXiv:1512.04387},
  year={2015}
}

article{vandemeent_arxiv_1507_04635,
  journal = {ArXiv (accepted at AISTATS 2016)},
  archiveprefix = {arXiv},
  arxivid = {1507.04635},
  author = {van de Meent, Jan-Willem and Tolpin, David and Paige, Brooks and Wood, Frank},
  eprint = {1507.04635},
  pages = {1507.04635},
  title = {{Black-Box Policy Search with Probabilistic Programs}},
  year = {2015}
}

article{Rainforth-2015-CCF-arXiv,
  title = {Canonical Correlation Forests},
  url = {https://bitbucket.org/twgr/ccf},
  author = {Rainforth, Tom and Wood, Frank},
  journal = {arXiv preprint arXiv:1507.05444},
  year = {2015}
}

inproceedings{PaigeWood-INFNET-NIPS-2015,
  Author = {Paige, T.~Brooks and  and Wood, Frank},
  Booktitle = {NIPS Workshop on Advances in Approximate Bayesian Inference},
  Title = {Inference Networks for Graphical Models},
  Year = {2015}
}

inproceedings{Rainforth-NIPS-PPWORKSHOP15,
  Author = {Rainforth, Tom and Wood, Frank},
  Booktitle = {NIPS Workshop on Black Box Learning and Inference},
  Title = {Bayesian Optimization for Probabilistic Programs},
  Year = {2015}
}

inproceedings{Perov_DD_SMC_in_PP,
  Author = {Perov, Yura and Le, Tuan-Anh and Wood, Frank},
  Booktitle = {NIPS Workshop on Black Box Learning and Inference},
  Title = {Data-driven Sequential {M}onte {C}arlo in Probabilistic Programming},
  Year = {2015}
}

inproceedings{Tolpin-ECMLPKDD-2015,
  author = {Tolpin, David and van de Meent, Jan-Willem and Paige, Brooks Wood, Frank},
  booktitle = {ECML PKDD 2015},
  title = {{Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs}},
  year = {2015}
}

inproceedings{Tolpin-SOCS-2015,
  title={Maximum a Posteriori Estimation by Search in Probabilistic Programs},
  author={Tolpin, David and Wood, Frank},
  booktitle={Eighth Annual Symposium on Combinatorial Search},
  pages={201-205},
  year={2015}
}

incollection{Tolpin-ECML-Anglican-DemoTrack,
  year={2015},
  isbn={978-3-319-23460-1},
  booktitle={Machine Learning and Knowledge Discovery in Databases},
  volume={9286},
  series={Lecture Notes in Computer Science},
  editor={Bifet, Albert and May, Michael and Zadrozny, Bianca and Gavalda, Ricard and Pedreschi, Dino and Bonchi, Francesco and Cardoso, Jaime and Spiliopoulou, Myra},
  doi={10.1007/978-3-319-23461-8_36},
  title={Probabilistic Programming in Anglican},
  url={http://dx.doi.org/10.1007/978-3-319-23461-8_36},
  publisher={Springer International Publishing},
  keywords={Probabilistic programming},
  author={Tolpin, David and van de Meent, Jan-Willem and Wood, Frank},
  pages={308-311},
  language={English}
}

inproceedings{vandeMeent-AISTATS-2015,
  archiveprefix = {arXiv},
  arxivid = {1501.06769},
  author = {van de Meent, Jan-Willem and Yang, Hongseok and Mansinghka, Vikash and Wood, Frank},
  booktitle = {Artificial Intelligence and Statistics},
  eprint = {1501.06769},
  title = {{Particle Gibbs with Ancestor Sampling for Probabilistic Programs}},
  year = {2015}
}

inproceedings{Tolpin-ECMLPKDD-2015,
  author = {Tolpin, David and van de Meent, Jan-Willem and Paige, Brooks Wood, Frank},
  booktitle = {ECML PKDD 2015},
  title = {{Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs}},
  year = {2015}
}

inproceedings{Tolpin-SOCS-2015,
  title={Maximum a Posteriori Estimation by Search in Probabilistic Programs},
  author={Tolpin, David and Wood, Frank},
  booktitle={Eighth Annual Symposium on Combinatorial Search},
  pages={201-205},
  year={2015}
}

incollection{Tolpin-ECML-Anglican-DemoTrack,
  year={2015},
  isbn={978-3-319-23460-1},
  booktitle={Machine Learning and Knowledge Discovery in Databases},
  volume={9286},
  series={Lecture Notes in Computer Science},
  editor={Bifet, Albert and May, Michael and Zadrozny, Bianca and Gavalda, Ricard and Pedreschi, Dino and Bonchi, Francesco and Cardoso, Jaime and Spiliopoulou, Myra},
  doi={10.1007/978-3-319-23461-8_36},
  title={Probabilistic Programming in Anglican},
  url={http://dx.doi.org/10.1007/978-3-319-23461-8_36},
  publisher={Springer International Publishing},
  keywords={Probabilistic programming},
  author={Tolpin, David and van de Meent, Jan-Willem and Wood, Frank},
  pages={308-311},
  language={English}
}

inproceedings{vandeMeent-AISTATS-2015,
  archiveprefix = {arXiv},
  arxivid = {1501.06769},
  author = {van de Meent, Jan-Willem and Yang, Hongseok and Mansinghka, Vikash and Wood, Frank},
  booktitle = {Artificial Intelligence and Statistics},
  eprint = {1501.06769},
  title = {{Particle Gibbs with Ancestor Sampling for Probabilistic Programs}},
  year = {2015}
}

inproceedings{Paige-NIPS-2014,
  Author = {B. Paige and F. Wood and A. Doucet and Y.W. Teh},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Asynchronous Anytime Sequential Monte Carlo},
  pages={3410--3418},
  Year = {2014},
}

inproceedings{Dhir-EMBS-2014,
  Author = {N. Dhir and F. Wood},
  Booktitle = {Proceedings of the 36th IEEE Conference on Engineering in Medicine and Biological Systems},
  Title = {Improved Activity Recognition via Kalman Smoothing and Multiclass Linear Discriminant Analysis},
  Year = {2014}
}

inproceedings{Paige-ICML-2014,
  title={A Compilation Target for Probabilistic Programming Languages},
  author={Paige, Brooks and Wood, Frank},
  Booktitle={JMLR; ICML 2014},
  pages={1935--1943},
  year={2014}
}

inproceedings{Wood-AISTATS-2014,
  Author = {F. Wood and J. W. van de Meent and V. Mansinghka},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = {A New Approach to Probabilistic Programming Inference},
  Pages = {1024--1032},
  Year = {2014}
}

inproceedings{Neiswanger-AISTATS-2014,
  Author = {W. Neiswanger and F. Wood and E. Xing},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = { The Dependent {D}irichlet Process Mixture of Objects for Detection-free Tracking and Object Modeling},
  Pages = {660--668},
  Year = {2014}
}

inproceedings{Paige-NIPS-2014,
  Author = {B. Paige and F. Wood and A. Doucet and Y.W. Teh},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Asynchronous Anytime Sequential Monte Carlo},
  pages={3410--3418},
  Year = {2014},
}

inproceedings{Dhir-EMBS-2014,
  Author = {N. Dhir and F. Wood},
  Booktitle = {Proceedings of the 36th IEEE Conference on Engineering in Medicine and Biological Systems},
  Title = {Improved Activity Recognition via Kalman Smoothing and
  Multiclass Linear Discriminant Analysis},
  Year = {2014}
}

inproceedings{Paige-ICML-2014,
  title={A Compilation Target for Probabilistic Programming Languages},
  author={Paige, Brooks and Wood, Frank},
  Booktitle={JMLR; ICML 2014},
  pages={1935--1943},
  year={2014}
}

inproceedings{Wood-AISTATS-2014,
  Author = {F. Wood and J. W. van de Meent and V. Mansinghka},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = {A New Approach to Probabilistic Programming Inference},
  Pages = {1024--1032},
  Year = {2014}
}


inbook{Wood-HMMTH-2014,
  author={F. Wood and A. Perotte},
  year= 2014, 
  chapter={Mixed Membership Classification for Documents with Hierarchically Structured Labels}, 
  editor = {E. Airoldi, D. Blei, E. Erosheva and S. Fienberg}, 
  title= {Handbook of Mixed Membership Models and Their Applications}, 
  publisher= {Chapman and Hall/CRC},
}

inproceedings{Neiswanger-AISTATS-2014,
  Author = {W. Neiswanger and F. Wood and E. Xing},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = { The Dependent {D}irichlet Process Mixture of Objects for Detection-free Tracking and Object Modeling},
  Pages = {660--668},
  Year = {2014}
}

inproceedings{Perov-2014-NIPSPROBPROG,
  Author = {Y. N. Perov and F. Wood},
  Booktitle = {NIPS Probabilistic Programming Workshop},
  Title = {Learning probabilistic programs},
  Year = {2014}
}

inproceedings{vandeMeent-2014-NIPSPROBPROG,
  Author = {J. W. van~de~Meent and H. Yang and V. Mansinghka and F. Wood},
  Booktitle = {NIPS Probabilistic Programming Workshop},
  Title = {Particle {G}ibbs with Ancestor Resampling for Probabilistic Programs},
  Year = {2014}
}

inproceedings{Tolpin-2014-NIPSPROBPROG,
  Author = {D. Tolpin and J. W. van~de~Meent and B. Paige and F. Wood},
  Booktitle = {NIPS Probabilistic Programming Workshop},
  Title = {Adaptive Scheduling in {MCMC} and Probabilistic Programming},
  Year = {2014}
}

article{Perotte-JAMIA-2013,
  title={Diagnosis code assignment: models and evaluation metrics},
  author={Perotte, Adler and Pivovarov, Rimma and Natarajan, Karthik and Weiskopf, Nicole and Wood, Frank and Elhadad, No{\'e}mie},
  journal={Journal of the American Medical Informatics Association},
  volume={21},
  number={2},
  pages={231--237},
  year={2014},
  publisher={BMJ Publishing Group Ltd}
}

article{Paige-2014-arXiv,
  title={Asynchronous Anytime Sequential {M}onte {C}arlo},
  author={Paige, Brooks and Wood, Frank and Doucet, Arnaud and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1407.2864},
  year={2014}
}

article{Perov-2014-arXiv,
  title={Learning Probabilistic Programs},
  author={Perov, Yura and Wood, Frank},
  journal={arXiv preprint arXiv:1407.2646},
  year={2014}
}

article{paige2014compilation,
  title={A Compilation Target for Probabilistic Programming Languages},
  author={Paige, Brooks and Wood, Frank},
  journal={arXiv preprint arXiv:1403.0504},
  year={2014}
}

article{huggins2014infinite,
  title={Infinite structured hidden semi-{M}arkov models},
  author={Huggins, Jonathan H and Wood, Frank},
  journal={arXiv preprint arXiv:1407.0044},
  year={2014}
}

article{van2014tempering,
  title={Tempering by Subsampling},
  author={van de Meent, Jan-Willem and Paige, Brooks and Wood, Frank},
  journal={arXiv preprint arXiv:1401.7145},
  year={2014}
}

article{Zech-arXiv-2013,
  title={Inferring Team Strength Using a Discrete {M}arkov Random Field},
  author={J. Zech and F. Wood},
  journal={arXiv preprint arXiv:1305.1998},
  year={2013}
}

article{Doshi-Velez-TPAMI-2013,
  author = {F. Doshi-Velez and D. Pfau and F. Wood and N. Roy},
  title = {Bayesian Nonparametric Methods for Partially-Observable Reinforcement Learning},
  journal ={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {99},
   year = {2013},
  pages = {1},
  publisher = {IEEE Computer Society},
}

InProceedings{Elsner-EMNLP-2013,
  author = {Elsner, M.  and  Goldwater, S.  and  Feldman, N.  and  Wood, F.},
  title = {A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  month = {October},
  year = {2013},
  address = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  pages = {42--54},
}

inproceedings{vandeMeent-ICML-2013,
  Author = {J.W. van de Meent and J. E. Bronson and R. L. Gonzalez Jr. and F. Wood and C. H. Wiggins},
  Booktitle = {International Conference on Machine Learning},
  Title = {Learning biochemical kinetic models from single-molecule data with hierarchically-coupled hidden {M}arkov models},
  Year = {2013},
  Pages = {361--369}
}

InProceedings{Elsner-EMNLP-2013,
  author    = {Elsner, M.  and  Goldwater, S.  and  Feldman, N.  and  Wood, F.},
  title     = {A Joint Learning Model of Word Segmentation, Lexical Acquisition, and Phonetic Variability},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  month     = {October},
  year      = {2013},
  address   = {Seattle, Washington, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {42--54},
}


inproceedings{vandeMeent-ICML-2013,
  Author = {J.W. van de Meent and J. E. Bronson and R. L. Gonzalez Jr. and F. Wood and C. H. Wiggins},
  Booktitle = {International Conference on Machine Learning},
  Title = {Learning biochemical kinetic models from single-molecule data with hierarchically-coupled hidden {M}arkov models},
  Year = {2013},
  Pages = {361--369}
}

inproceedings{Smith-AISTATS-2012,
  Author = {C. Smith and F. Wood and L. Paninski},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = { Low rank continuous-space graphical models},
  Pages={1064--1072},
  Year = {2012}
}

inproceedings{Smith-AISTATS-2012,
  Author = {C. Smith and F. Wood and L. Paninski},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = { Low rank continuous-space graphical models},
  Pages={1064--1072},
  Year = {2012}
}

inproceedings{Paige-NYMLW-2012,
  title={Perspective Inference for Eye-to-Eye Videoconferencing: Empirical Evaluation Tools and Data},
  author={T. B. Paige and X. Zhang and J. Forde and F. Wood},
  booktitle={Proc. 7th Annual Machine Learning Workshop, New York Academy of Sciences},
  year={2012}
}

article{Dewar-IEEE-2012,
  title={Inference in Hidden {M}arkov Models with Explicit State Duration Distributions},
  author={Dewar, M. and Wiggins, C. and Wood, F.},
  journal={Signal Processing Letters, IEEE},
  volume={19},
  number={4},
  pages={235--238},
  year={2012}
}

article{Neiswanger-arXiv-2012,
  title={Unsupervised Detection and Tracking of Arbitrary Objects with Dependent Dirichlet Process Mixtures},
  author={Neiswanger, W. and Wood, F.},
  journal={arXiv preprint arXiv:1210.3288},
  year={2012}
}

article{Wood-CACM-2011,
  author = {F. Wood and J. Gasthaus and C. Archambeau and L. James and Y.W. Teh},
  title = "The Sequence Memoizer",
  year = "2011",
  volume = "54",
  number = "2",
  pages = "91--98",
  journal = "Communications of the ACM",
  publisher = "ACM Press",
}

inproceedings{Wood-NIPSNPBAYES-2011,
  Author = {F. Wood},
  Booktitle = {NIPS Nonparametric {B}ayes Workshop},
  Title = {Modeling Streaming Data In the Absence of Sufficiency},
  Year = {2011}
}

inproceedings{Perotte-NIPS-2011,
  Author = {A. Perotte and N. Bartlett and N. Elhadad and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Hierarchically Supervised Latent {D}irichlet Allocation},
  Year = {2011},
  Pages = {2609--2617}
}

inproceedings{Bartlett-DCC-2011,
  Author = {N. Bartlett and F. Wood},
  Booktitle = {Data Compression Conference},
  Title = {Deplump for Streaming Data},
  Pages = {363--372},
  Year = {2011}
}

inproceedings{Pfau-NIPS-2011,
  Author = {D. Pfau and N. Bartlett and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Probabilistic Deterministic Infinite Automata},
  Year = {2011},
  Pages = {1930--1938}
}

inproceedings{Perotte-NIPS-2011,
  Author = {A. Perotte and N. Bartlett and N. Elhadad and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Hierarchically Supervised Latent {D}irichlet Allocation},
  Year = {2011},
  Pages = {2609--2617}
}

inproceedings{Wood-AISTATS-2011-response,
  Author = {F. Wood},
  Booktitle = {Artificial Intelligence and Statistics},
  Title = {Discussion of "The Discrete Infinite Logistic Normal Distribution for Mixed-Membership Modeling"},
  Year = {2011},
  Pages = {}
}

inproceedings{Bartlett-DCC-2011,
  Author = {N. Bartlett and F. Wood},
  Booktitle = {Data Compression Conference},
  Title = {Deplump for Streaming Data},
  Pages = {363--372},
  Year = {2011}
}

inproceedings{Pfau-NIPS-2011,
  Author = {D. Pfau and N. Bartlett and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Title = {Probabilistic Deterministic Infinite Automata},
  Year = {2011},
  Pages = {1930--1938}
}

inproceedings{Bartlett-ICML-2010,
  Author = {N. Bartlett and D. Pfau and F. Wood},
  Booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  Title = {Forgetting Counts : Constant Memory Inference for a Dependent Hierarchical {P}itman-{Y}or Process},
  Year = {2010},Pages = {63--70}
}

inproceedings{Gasthaus-DCC-2010,
  Author = {J. Gasthaus and F. Wood and Y.W. Teh},
  Booktitle = {Data Compression Conference},
  pages = {337--345},
  Title = {Lossless compression based on the {S}equence {M}emoizer },
  Year = {2010}
}

inproceedings{Bartlett-ICML-2010,
  Author = {N. Bartlett and D. Pfau and F. Wood},
  Booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  Title = {Forgetting Counts : Constant Memory Inference for a Dependent Hierarchical {P}itman-{Y}or Process},
  Year = {2010},Pages = {63--70}
}
	
inproceedings{Gasthaus-DCC-2010,
  Author = {J. Gasthaus and F. Wood and Y.W. Teh},
  Booktitle = {Data Compression Conference},
  pages = {337--345},
  Title = {Lossless compression based on the {S}equence {M}emoizer },
  Year = {2010}
}

inproceedings{Wood-ICML-2009,
  Author = {F. Wood and C. Archambeau and J. Gasthaus and L. James and Y.W. Teh},
  Booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  Pages = {1129--1136},
  Title = {A Stochastic Memoizer for Sequence Data },
  Year = {2009}
}

inproceedings{Wood-AISTATS-2009,
  Author = {F. Wood and Y.W. Teh},
  Booktitle = {Artificial Intelligence and Statistics},
  Pages = {607--614},
  Title = { A Hierarchical Nonparametric {B}ayesian Approach to Statistical Language Model Domain Adaptation},
  Year = {2009}
}

inproceedings{Gasthaus-NIPS-2009,
  Author = {J. Gasthaus and F. Wood and D. G\"{o}r\"{u}r and Y.W. Teh},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {497--504},
  Title = {Dependent {D}irichlet Process Spike Sorting},
  Year = {2009}
}

inproceedings{Berkes-NIPS-2009,
  Author = {P. Berkes and J.W. Pillow and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {129 -- 136},
  Title = {Characterizing neural dependencies with {P}oisson copula models},
  Year = {2009}
}

inproceedings{Wood-ICML-2009,
  Author = {F. Wood and C. Archambeau and J. Gasthaus and L. James and Y.W. Teh},
  Booktitle = {Proceedings of the 26th International Conference on Machine Learning},
  Pages = {1129--1136},
  Title = {A Stochastic Memoizer for Sequence Data },
  Year = {2009}
}

inproceedings{Wood-AISTATS-2009,
  Author = {F. Wood and Y.W. Teh},
  Booktitle = {Artificial Intelligence and Statistics},
  Pages = {607--614},
  Title = { A Hierarchical Nonparametric {B}ayesian Approach to Statistical Language Model Domain Adaptation},
  Year = {2009}
}

inproceedings{Gasthaus-NIPS-2009,
  Author = {J. Gasthaus and F. Wood and D. G\"{o}r\"{u}r and Y.W. Teh},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {497--504},
  Title = {Dependent {D}irichlet Process Spike Sorting},
  Year = {2009}
}

inproceedings{Berkes-NIPS-2009,
  Author = {P. Berkes and J.W. Pillow and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {129 -- 136},
  Title = {Characterizing neural dependencies with {P}oisson copula models},
  Year = {2009}
}

inproceedings{Wood-ICMLNPBAYES-2008,
  Author = {F. Wood and Y.W. Teh},
  Booktitle = {ICML/UAI Nonparametric {B}ayes Workshop},
  Title = {A Hierarchical, Hierarchical {P}itman {Y}or Process Language Model},
  Year = {2008}
}

article{Wood-JNM-2008,
  Author = {F. Wood and M. J. Black},
  Journal = {Journal of Neuroscience Methods},
  Pages = {1--12},
  Title = {A Non-parametric {B}ayesian alternative to spike sorting},
  Volume = {173},
  Year = {2008}
}

techreport{Wood-TECHREPORT-2008,
	Author = {F. Wood and D.H. Grollman and K.A. Heller and O.C. Jenkins and M.J. Black},
	Institution = {Brown University, Department of Computer Science},
	Number = {CS-08-07},
	Title = {Incremental Nonparametric {B}ayesian regression},
	Year = {2008}
}

inproceedings{Wood-NIPS-2006,
  Author = {F. Wood and T. L. Griffiths},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {1513--1520},
  Title = {Particle Filtering for Non-Parametric {B}ayesian Matrix Factorization},
  Year = {2006}
}

inproceedings{Wood-EMBS-2006,
  Author = {F. Wood and S. Goldwater and M. J. Black},
  Booktitle = {Proceedings of the 28th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {1165--1169},
  Title = {A Non-Parametric {B}ayesian Approach to Spike Sorting},
  Year = {2006}
}

inproceedings{Wood-UAI-2006,
  Author = {F. Wood and T. L. Griffiths and Z. Ghahramani},
  Booktitle = {Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence },
  Pages = {536--543},
  Title = {A Non-Parametric {B}ayesian Method for Inferring Hidden Causes},
  Year = {2006}
}

inproceedings{Kim-BioRob-2006,
  Author = {S. P. Kim and F. Wood and M. J. Black},
  Booktitle = {The First IEEE / RAS-EMBS International Conference on Biomedical Robotics and Biomechatronics},
  Pages = {259--299},
  Title = {Statistical Analysis of the Non-stationarity of Neural Population Codes},
  Year = {2006}
}

inproceedings{Wood-NIPS-2006,
  Author = {F. Wood and T. L. Griffiths},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {1513--1520},
  Title = {Particle Filtering for Non-Parametric {B}ayesian Matrix Factorization},
  Year = {2006}
}

inproceedings{Wood-EMBS-2006,
  Author = {F. Wood and S. Goldwater and M. J. Black},
  Booktitle = {Proceedings of the 28th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {1165--1169},
  Title = {A Non-Parametric {B}ayesian Approach to Spike Sorting},
  Year = {2006}
}

inproceedings{Wood-UAI-2006,
  Author = {F. Wood and T. L. Griffiths and Z. Ghahramani},
  Booktitle = {Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence },
  Pages = {536--543},
  Title = {A Non-Parametric {B}ayesian Method for Inferring Hidden Causes},
  Year = {2006}
}

inproceedings{Kim-BioRob-2006,
  Author = {S. P. Kim and F. Wood and M. J. Black},
  Booktitle = {The First IEEE / RAS-EMBS International Conference on Biomedical Robotics and Biomechatronics},
  Pages = {259--299},
  Title = {Statistical Analysis of the Non-stationarity of Neural Population Codes},
  Year = {2006}
}

article{Grollman-JFR-2006,
  Author = {D. H. Grollman and O. C. Jenkins and F. Wood},
  Journal = {Journal of Field Robotics},
  Pages = {1077--1089},
  Title = {Discovering natural kinds of robot sensory experiences in unstructured environments},
  Volume = {23},
  Year = {2006}
}

inproceedings{Wood-NIPS-2005,
  Author = {F. Wood and S. Roth and M. J. Black},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {1527--1544},
  Title = {Modeling neural population spiking activity with {G}ibbs distributions},
  Year = {2005}
}

inproceedings{Wood-EMBS-2005,
  Author = {F. Wood and Prabhat and J. P. Donoghue and M. J. Black},
  Booktitle = {Proceedings of the 27th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {149--152},
  Title = {Inferring Attentional State and Kinematics from Motor Cortical Firing Rates},
  Year = {2005}
}

inproceedings{Wood-NIPS-2005,
  Author = {F. Wood and S. Roth and M. J. Black},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {1527--1544},
  Title = {Modeling neural population spiking activity with {G}ibbs distributions},
  Year = {2005}
}

inproceedings{Wood-EMBS-2005,
  Author = {F. Wood and Prabhat and J. P. Donoghue and M. J. Black},
  Booktitle = {Proceedings of the 27th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {149--152},
  Title = {Inferring Attentional State and Kinematics from Motor Cortical Firing Rates},
  Year = {2005}
}

inproceedings{Grollman-NIPS-2005,
  Author = {D. H. Grollman and O. C. Jenkins and F. Wood},
  Booktitle = {Advances in Neural Information Processing Systems Workshop on Machine Learning Based Robotics in Unstructured Environments},
  Title = {Discovering Natural Kinds of Robot Sensory Experiences in Unstructured Environments},
  Year = {2005}
}

inproceedings{Wood-EMBS-2004,
  Author = {F. Wood and M. Fellows and J. P. Donoghue and M. J. Black},
  Booktitle = {Proceedings of the 27th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {4126--4129},
  Title = {Automatic Spike Sorting for Neural Decoding},
  Year = {2004}
}

inproceedings{Wood-EMBS-2004,
  Author = {F. Wood and M. Fellows and J. P. Donoghue and M. J. Black},
  Booktitle = {Proceedings of the 27th IEEE Conference on Engineering in Medicine and Biological Systems},
  Pages = {4126--4129},
  Title = {Automatic Spike Sorting for Neural Decoding},
  Year = {2004}
}

article{Wood-TBME-2004,
  Author = {F. Wood and M. Fellows and C. Vargas-Irwin and M. J. Black and J. P. Donoghue},
  Journal = {IEEE Transactions in Biomedical Engineering},
  Pages = {912-918},
  Title = {On the Variability of Manual Spike Sorting},
  Volume = {51},
  Year = {2004}
}

article{Wood-IEEE-CompGraphics-1996,
  Author = {F. Wood and D. Brown and B. Amidon and J. Alferness and B. Joseph and R. E. Gillilan and C. Faerman},
  Journal = {IEEE Computer Graphics and Applications},
  Pages = {72--78},
  Title = {Windowing and Telecollaboration for Virtual Reality with Applications to the Study of a Tropical Disease},
  Volume = {16},
  Year = {1996}
}

article{Gillilan-CompGraphics-1995,
  Author = {R. E. Gillilan and F. Wood},
  Journal = {Computer Graphics},
  Pages = {55--58},
  Title = {Visualization, Virtual Reality, and Animation within the Data Flow Model of Computing},
  Volume = {29},
  Year = {1995}
}
