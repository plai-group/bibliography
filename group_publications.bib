@inproceedings{Dhir-IROS-2016,
  author  = "Neil Dhir and Yura Perov and Frank Wood",
  title   = "Nonparametric Bayesian Models for Unsupervised Activity Recognition and Tracking",
  booktitle = "IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2016)",
  year = "2016"
}

@unpublished{he-dataset-embodied-ai-2025,
	title = {PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI},
	author = {Yingchen He and Christian D. Weilbach and Martyna E. Wojciechowska and Yuxuan Zhang and Frank Wood},
	year = {2025},
	arxiv = {2505.12707},
	url = {https://arxiv.org/abs/2505.12707},
	url_pdf = {https://arxiv.org/pdf/2505.12707.pdf},
	abstract = {Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\footnote\{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.\} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.},
}